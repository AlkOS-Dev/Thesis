\documentclass[a4paper,9pt,twoside]{report}

% --------   PREAMBLE PART ----------

% -------- ENCODING & LANGUAGES --------

\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage[polish, english]{babel}


\usepackage{amsmath, amsfonts, amsthm, latexsym}

\usepackage[final]{pdfpages}
\usepackage[backend=biber, style=numeric, sorting=none]{biblatex}
\addbibresource{references.bib}


\usepackage{commath}

\usepackage[hidelinks]{hyperref}

\usepackage[inkscapepath=../output/svg/]{svg}


% ------ MARGINS, INDENTATION, LINESPREAD ------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}


\linespread{1.5}
\allowdisplaybreaks

\usepackage{indentfirst}
\setlength{\parindent}{5mm}


%------ RUNNING HEAD - CHAPTER NAMES, PAGE NUMBERS ETC. -------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[LE,RO]{\thepage} 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}

\renewcommand{\headrulewidth}{0 pt}


\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[LE,RO]{\thepage}
  
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.0pt}
}

%------ code listings -------

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstdefinestyle{cppstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=C++,
    morekeywords={constexpr, nullptr, size_t, uint64_t}
}
\lstset{style=cppstyle}
\renewcommand{\lstlistingname}{C++ Code Snippet}

\lstdefinestyle{nasmstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=[x86masm]Assembler,
    morekeywords={rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8, r9, r10, r11, r12, r13, r14, r15,
                  eax, ebx, ecx, edx, esi, edi, ebp, esp,
                  cr0, cr2, cr3,
                  mov, push, pop, call, ret, int, iretq, jmp, je, jne, jg, jl, cmp, test,
                  add, sub, mul, div, inc, dec, xor, or, and,
                  lidt, lgdt, sti, cli, hlt,
                  section, global, extern, db, dw, dd, dq, resb, resw, resd, resq,
                  macro, endmacro, \%define}
}

\lstdefinestyle{cmakestyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=CMake,
    morekeywords={project, add_executable, add_library, target_link_libraries, set, include_directories,
                  cmake_minimum_required, file, macro, endmacro, foreach, endforeach, if, endif, else,
                  find_package, add_custom_command, add_custom_target, install, alkos_find_sources, alkos_register_userspace_app}
}

% --------- DRAWING -------

\usepackage{tikz}
\usetikzlibrary{fit, backgrounds, shapes, arrows.meta, positioning, calc, chains}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{svg}
\usepackage{graphicx}

% --------- CHAPTER HEADERS -------

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 

    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% --------- TABLE OF CONTENTS SETUP ---------

\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}
  [0pt]
  {}
  {\bfseries \thecontentslabel.\quad}
  {\bfseries}
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% -------- TABLES AD FIGURES NUMBERING --------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}


% ----- DEFINING ENVIRONMENTS FOR THEOREMS, DEFINITIONS ETC. -----

\makeatletter
\newtheoremstyle{definition}
{3ex}
{3ex}
{\upshape}
{}
{\bfseries}
{.}
{.5em}
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% ------- END OF PREAMBLE PART (MOSTLY) ----------





% ---------- USER SETTINGS ---------

\newcommand{\tytul}{Funkcjonalne jądro systemu operacyjnego: AlkOS}
\renewcommand{\title}{From Bare Metal to a Functional Kernel: The AlkOS Operating System}
\newcommand{\type}{Engineer}
\newcommand{\supervisor}{mgr inż. Paweł Sobótka}



\begin{document}
\sloppy
\selectlanguage{english}

\includepdf[pages=-]{titlepage-en}

\null\thispagestyle{empty}\newpage

% ------ PAGE WITH SIGNATURES ------------

%\thispagestyle{empty}\newpage
%\null
%
%\vfill
%
%\begin{center}
%\begin{tabular}[t]{ccc}
%............................................. & \hspace*{100pt} & .............................................\\
%supervisor's signature & \hspace*{100pt} & author's signature
%\end{tabular}
%\end{center}
%


% ---------- ABSTRACT -----------

{  \fontsize{12}{14} \selectfont
\begin{abstract}

\begin{center}
\title
\end{center}

TODO

\end{abstract}
}

\null\thispagestyle{empty}\newpage

%% --------- DECLARATIONS ------------
%
%%
%%	IT IS NECESSARY OT ATTACH FILLED-OUT AUTORSHIP DEECLRATION. SCAN (IN PDF FORMAT) NEEDS TO BE PLACED IN scans FOLDER AND IT SHOULD BE CALLED, FOR EXAMPLE, DECLARATION_OF_AUTORSHIP.PDF. IF THE FILENAME OR FILEPATH IS DIFFERENT, THE FILEPATH IN THE NEXT COMMAND HAS TO BE ADJUSTED ACCORDINGLY.
%%
%%	command attacging the declarations of autorship
%%
%\includepdf[pages=-]{scans/declaration-of-autorship}
%\null\thispagestyle{empty}\newpage
%
%% optional declaration
%%
%%	command attaching the declaataration on granting a license
%%
%\includepdf[pages=-]{scans/declaration-on-granting-a-license}
%%
%%	.tex corresponding to the above PDF files are present in the 3. declarations folder 
%
\null\thispagestyle{empty}\newpage
% ------- TABLE OF CONTENTS -------
\selectlanguage{english}
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\newpage % IF YOU HAVE EVEN QUANTITY OD PAGES OF TOC, THEN REMOVE IT OR ADD \null\newpage FOR DOUBLE BLANK PAGE BEFORE INTRODUCTION


% -------- THE BODY OF THE THESIS ------------

\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11}
\chapter{Introduction}
\markboth{}{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\section{Theoretical Background} 
TODO KTOKOLWIEK

\subsubsection{Interrupts}
\subsubsection{Kernel Tick}
\subsubsection{Exceptions}
\subsubsection{Virtualization}
\subsubsection{Address Space}
\subsubsection{Context Switch}

\section{Scope of the Thesis} 
TODO KTOKOLWIEK
\section{Limitations and Assumptions}

\subsection{Minimal requirements}
\begin{itemize}
  \item avx
  \item osxsave
  \item swapgs
  \item invariant tsc
  \item HPET
  \item APIC
  \item acpi
  \item 64bit LAPIC
  \item mmu
\end{itemize}

\subsection{Known Limitations}
\label{subsec:limitations}

\begin{itemize}
  \item max process name = 128
  \item max processes = 4096
  \item max threads = 8192
\end{itemize}

Functionalities:
\begin{itemize}
  \item aaa
\end{itemize}

\section{Achieved Functionalities} 
TODO KTOKOLWIEK
\section{Work Division -- Code} 
TODO KTOKOLWIEK
\section{Work Division -- Thesis}
TODO KTOKOLWIEK

% ==================================================================

\chapter{Creating an Operating System from Scratch}

This chapter outlines the development sequence of the kernel's most critical components, upon which the majority of the subsequent code depends. The proposed progression is derived from the experience gained and materials consulted during the development process. While this is not the only valid approach to system development, it represents a structured path designed to minimize redundant discovery and facilitate implementation. The discussion focuses exclusively on the monolithic kernel design with memory virtualization, where every module resides within the same address space - the kernel address space.

\section{Host Environment Preparation}

Before proceeding to writing actual kernel code we must first do some preparation on our development tools and development environemnts. Below we can find brief description of needed things.

\subsection{Cross-Compilation Toolchain}
TODO KTOKOLWIEK
\subsection{Building Machinery}
TODO KTOKOLWIEK
\subsection{Emulation}
TODO KTOKOLWIEK

\section{General Design Considerations}
TODO KTOKOLWIEK

\subsection{Intrusive Data Structures}
TODO KTOKOLWIEK

\subsection{Ref-counting}
TODO KTOKOLWIEK

\subsection{Archtiecture Abstraction}
TODO KTOKOLWIEK

\subsection{Directory Layout}
TODO KTOKOLWIEK

\subsection{Including Good Practices}
TODO KTOKOLWIEK

\section{Target Environment}
TODO KTOKOLWIEK

\subsection{Implementation of Libc and Libc++}
TODO KTOKOLWIEK

\subsection{Bootloader}
\label{subsec:theory_bootloader}

The process of bringing a computer from a powered-off state to a fully functional operating system is governed by a rigid chain of physical and logical constraints. At the hardware level, the Central Processing Unit (CPU) functions as a complex state machine. Upon the application of power or a reset signal, the CPU resets its internal registers to default values and sets the Instruction Pointer to a specific, hardcoded physical address known as the \textit{Reset Vector} \cite{IntelManual-Reset}.

\subsubsection{The Memory Paradox and Storage}
A fundamental challenge in this sequence is the source of the initial instructions. The standard Random Access Memory (RAM), which serves as the primary workspace for modern operating systems, is volatile. It requires active electrical flow to maintain its state. When the system is powered off, the state is lost; upon power-up, the memory cells contain random garbage data. Consequently, the CPU cannot fetch valid instructions from standard RAM immediately after a reset.

To resolve this, hardware architects map the Reset Vector address to a non-volatile memory region, typically Flash Memory or Read-Only Memory (ROM), which retains data without power.

\subsubsection{Embedded vs. Complex Architectures}
In simple embedded architectures (e.g., microcontrollers used in automotive braking systems or household appliances), the entire application code is often stored in this non-volatile memory. The memory controller maps this storage directly into the CPU's addressable space. This technique, known as \textbf{Execute In Place (XIP)}, allows the CPU to fetch and execute the developer's code from the very first clock cycle \cite{ARM-CortexM4-Generic-User-Guide}. The developer "owns" the machine from the first nanosecond.

In contrast, more complex architectures (such as ARM-based smartphones or single-board computers like the Raspberry Pi) often store the main operating system on external, complex storage media like SD cards or eMMC chips. The CPU cannot simply memory-map an SD card; it requires a sophisticated software driver to communicate with the storage controller. To bridge this gap, manufacturers embed a tiny, immutable piece of software called the \textbf{BootROM} directly into the silicon. This code initializes the minimal required hardware (often internal SRAM) and loads a secondary bootloader from the external storage into that SRAM, which in turn loads the main software.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    % Standard block style
    block/.style={
        rectangle, 
        draw=black!70, 
        rounded corners=3pt, 
        minimum width=2.2cm, 
        minimum height=1cm, 
        align=center, 
        fill=white,
        font=\small
    },
    % Style for the "Silicon" container
    soc/.style={
        rectangle,
        draw=black!40,
        dashed,
        fill=gray!5,
        rounded corners=5pt,
        inner sep=0.5cm
    },
    % Arrow styles
    arrow/.style={-Latex, thick, color=black!80},
    data/.style={-Latex, thick, dashed, color=black!80},
    % Label style for arrows
    lbl/.style={
        font=\footnotesize\bfseries, 
        fill=white, 
        inner sep=1pt,
        text=black!80
    }
]

% =========================================================
% LEFT SIDE: SIMPLE XIP
% =========================================================

\node (cpu1) [block, fill=blue!10] {CPU};
\node (flash) [block, below=1.5cm of cpu1, fill=orange!10] {NOR Flash\\(Memory Mapped)};
\node (label1) [above=0.1cm of cpu1, font=\bfseries] {Simple (XIP)};

% Arrow
\draw[arrow] (cpu1) -- node[midway, right, font=\footnotesize] {Direct Fetch} (flash);


% =========================================================
% RIGHT SIDE: COMPLEX BOOT
% =========================================================

% We place CPU2 to the right
\node (cpu2) [block, right=6cm of cpu1, fill=blue!10] {CPU};

% Define relative positions for BootROM and SRAM relative to CPU2
\node (bootrom) [block, below left=1.2cm and -0.5cm of cpu2, fill=gray!20] {BootROM\\(Immutable)};
\node (sram) [block, below right=1.2cm and -0.5cm of cpu2, fill=green!10] {Internal\\SRAM};

% Draw the SoC Boundary around them
\begin{scope}[on background layer]
    \node (soc_box) [soc, fit=(cpu2) (bootrom) (sram), label={[anchor=south west, inner sep=5pt]north west:\tiny System on Chip (SoC)}] {};
\end{scope}

% External Storage below the SoC
\node (sdcard) [block, below=1.0cm of soc_box, fill=orange!10] {SD Card / Disk\\(External)};
\node (label2) [above=0.5cm of soc_box, font=\bfseries] {Complex (Bootload)};


% =========================================================
% ARROWS & FLOW (Complex)
% =========================================================

% 1. Power On -> BootROM
\draw[arrow] (cpu2) -- node[lbl, pos=0.6] {1. Init} (bootrom);

% 2. BootROM -> External
\draw[arrow] (bootrom) -- node[lbl, pos=0.4] {2. Load} (sdcard);

% 3. External -> SRAM (Data copy)
\draw[data] (sdcard) -- node[lbl, pos=0.4] {3. Copy} (sram);

% 4. CPU -> SRAM (Execution)
\draw[arrow] (cpu2) -- node[lbl, pos=0.6] {4. Jump} (sram);

\end{tikzpicture}
\caption{Comparison of Boot Architectures}
\label{fig:boot_methods}
\end{figure}

On modern "heavy" machines, such as x86-64 workstations, the initialization process is exponentially more complex. The main CPU is fragile and dependent on a specific environment to function. Before the main cores can execute a single instruction, the hardware requires:
\begin{enumerate}
    \item \textbf{Power Sequencing:} Multiple voltage rails (Vcore, VccSA, VccIO) must be brought up in a specific order with millisecond-precision timing.
    \item \textbf{Clock Stabilization:} Phase-Locked Loops (PLLs) must be tuned and stabilized to generate the gigahertz-range frequencies required by the cores.
    \item \textbf{DRAM Training:} Modern DDR4/DDR5 memory requires a complex calibration process to align signal timing before it becomes usable.
\end{enumerate}
\cite{Intel-Datasheet-Vol1}

To manage this, modern chipsets often include a smaller, dedicated processor (e.g., the Intel Management Engine or AMD Platform Security Processor) that starts before the main CPU. This co-processor initializes the platform hardware to a state where the main CPU can begin execution.

\subsubsection{The Chain of Trust and Abstraction}
\label{subsubsec:chain_of_trust_and_abstraction}
By the time a modern Operating System kernel begins execution, it is likely the fourth or fifth program in the boot chain. The entity responsible for defining the interface between the hardware and the OS is the \textbf{System Firmware} \cite{UEFI-Base-Spec, UEFI-PI-Spec}.

The firmware's responsibility is to abstract the diverse implementations of different motherboards (e.g., how the disk controller is wired) and provide a mechanism to load an OS from a disk into RAM. However, relying solely on firmware is often insufficient for a portable operating system:
\begin{itemize}
    \item \textbf{Inconsistent State:} Different firmware implementations may leave the CPU in varying states (e.g., different interrupt configurations or privilege modes).
    \item \textbf{Feature Limitations:} Firmware is designed to be simple and compatible, often leaving the CPU in a conservative, low-feature mode with caches or advanced vector units disabled.
    \item \textbf{Interface Variance:} The method used to retrieve a memory map or video configuration can vary wildly between hardware generations.
\end{itemize}

To solve this, a \textbf{Third-Party Bootloader} is often utilized. This program acts as a "Normalizer" \cite{Limine-Spec}. It knows how to talk to various firmware types and storage devices. Its job is to abstract away the firmware differences, load the kernel file into memory, and pass control to the OS in a unified, predictable manner.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1cm,
    stage/.style={rectangle, draw, fill=white, text width=4cm, align=center, minimum height=1cm},
    arrow/.style={-Latex, thick}
]

\node (power) [stage, fill=gray!10] {\textbf{Hardware Power-On}\\(Reset Vector)};
\node (firmware) [stage, below=of power] {\textbf{System Firmware}\\(BIOS / UEFI)};
\node (loader) [stage, below=of firmware] {\textbf{Bootloader}\\(GRUB / Limine)};
\node (trampoline) [stage, below=of loader] {\textbf{OS Trampoline}\\(Arch Specific)};
\node (kernel) [stage, below=of trampoline, fill=gray!30] {\textbf{Kernel Main}\\(Arch Agnostic)};

\draw[arrow] (power) -- node[right, font=\footnotesize] {Init Platform} (firmware);
\draw[arrow] (firmware) -- node[right, font=\footnotesize] {Load from Disk} (loader);
\draw[arrow] (loader) -- node[right, font=\footnotesize] {Normalize State} (trampoline);
\draw[arrow] (trampoline) -- node[right, font=\footnotesize] {Enable Features (AVX, Paging)} (kernel);

\end{tikzpicture}
\caption{Typical Boot Chain for x86\_64}
\label{fig:boot_chain}
\end{figure}

\subsubsection{The OS-Level Trampoline}
Even with a standardized bootloader, the kernel cannot assume full control immediately. A generic bootloader cannot know the specific internal requirements of the OS. For instance:
\begin{itemize}
    \item The kernel may typically require memory to be mapped to a specific virtual address range (e.g., the higher half).
    \item Specific hardware features (like Floating Point Units or Virtualization Extensions) are often disabled by default to save power and must be explicitly enabled.
    \item The OS must define its own memory protection structures to enforce its specific security model.
\end{itemize}

Therefore, a robust operating system must implement its own initialization stage, effectively an \textbf{OS-Level Bootloader} or "Trampoline." This architecture-specific code is responsible for taking the machine from the normalized state provided by the external bootloader, enabling the specific CPU features required by the kernel, and establishing the runtime environment before passing control to the architecture-agnostic kernel main function.

\section{Memory Preloading and Discovery}
\label{sec:mem_discovery}
One of the first and most critical responsibilities of a kernel during the bootstrap phase is to establish an authoritative map of the system's physical memory. Unlike user-space applications, which simply request memory from the operating system via system calls (e.g., \texttt{malloc} or \texttt{mmap}), the kernel is the manager responsible for fulfilling those requests. Upon entry, the kernel does not know how much RAM is available, where it is located, or which memory ranges are reserved for hardware mapped I/O (MMIO).

This discovery process is not standardized. It is strictly coupled to the target architecture, the silicon vendor, and the residing firmware. Depending on the platform complexity, the kernel may acquire the memory map through one of three primary mechanisms: static definition, firmware interrogation, or hardware description structures.

\subsection{Static Definition}

On strictly embedded architectures (e.g., ARM Cortex-M or AVR), the physical memory layout is immutable. The location and size of SRAM banks, Flash storage, and peripheral registers are defined by the silicon vendor and do not change. In these environments, runtime discovery is redundant.

The memory map is hardcoded directly into the kernel's source code or linker scripts, matching the specific System-on-Chip (SoC) datasheet \cite{ARM-CortexM4-Generic-User-Guide}. The developer explicitly defines the boundary between kernel code, stack, and heap. As illustrated in Figure \ref{fig:cortex_m4_memory}, the address space is rigid; the kernel assumes ownership of specific addresses immediately upon reset without querying external entities.

In this context, the Operating System does not "discover" memory. The kernel code assumes these addresses are valid from the first instruction. For example, a Cortex-M4 kernel may be hardcoded to expect code at 0x00000000 and RAM at 0x20000000. If the software is flashed onto a different chip variant, it will simply fault; flexibility is sacrificed for minimizing initialization overhead.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    % Main memory block style
    memblock/.style={
        rectangle, 
        draw=black, 
        thick,
        minimum width=4.0cm, 
        align=center, 
        anchor=south,
        outer sep=0pt
    },
    % Left-side exploded box style
    sideblock/.style={
        rectangle,
        draw=black,
        minimum width=3.8cm,
        text width=3.5cm,
        align=center,
        font=\sffamily\scriptsize,
        fill=white
    },
    % Address label style
    addr/.style={
        font=\ttfamily\scriptsize
    }
]

% ==========================================
% 1. DRAW MAIN MEMORY STACK
% ==========================================

% Code
\node[memblock, minimum height=1.5cm] (code) at (0,0) {Code\\ \scriptsize 0.5GB};

% SRAM
\node[memblock, minimum height=1.5cm, above=0cm of code] (sram) {SRAM\\ \scriptsize 0.5GB};

% Peripheral
\node[memblock, minimum height=1.5cm, above=0cm of sram] (periph) {Peripheral\\ \scriptsize 0.5GB};

% External RAM
\node[memblock, minimum height=2.0cm, above=0cm of periph] (extram) {External RAM\\ \scriptsize 1.0GB};

% External Device
\node[memblock, minimum height=2.0cm, above=0cm of extram] (extdev) {External Device\\ \scriptsize 1.0GB};

% PPB
\node[memblock, minimum height=0.6cm, above=0cm of extdev] (ppb) {Private peripheral\\bus \scriptsize 1.0MB};

% Vendor
\node[memblock, minimum height=1.0cm, above=0cm of ppb] (vendor) {Vendor-specific\\memory \scriptsize 511MB};


% ==========================================
% 2. ADDRESS LABELS (RIGHT SIDE)
% ==========================================
\node[addr, anchor=south west] at (vendor.north east) {0xFFFFFFFF};
\node[addr, anchor=south west] at (vendor.south east) {0xE0100000};
\node[addr, anchor=south west] at (ppb.south east)    {0xE0000000};
\node[addr, anchor=south west] at (extdev.south east) {0xA0000000};
\node[addr, anchor=south west] at (extram.south east) {0x60000000};
\node[addr, anchor=south west] at (periph.south east) {0x40000000};
\node[addr, anchor=south west] at (sram.south east)   {0x20000000};
\node[addr, anchor=south west] at (code.south east)   {0x00000000};


% ==========================================
% 3. LEFT SIDE BIT-BANDING (MANUAL PLACEMENT)
% ==========================================

% - SRAM GROUP -
% Define the target point (Base of SRAM)
\coordinate (sram_target) at (sram.south west);

% 1MB Region Box (Level with SRAM base)
\node[sideblock, anchor=east] (sram_region) at (-2.5, 1.2) {1MB Bit band region};
% Address for 1MB Region
\node[addr, anchor=east] at (sram_region.south west) {0x20000000};
\node[addr, anchor=east] at (sram_region.north west) {0x200FFFFF};

% Alias Box (Floated above)
\node[sideblock, anchor=east, minimum height=0.8cm] (sram_alias) at (-2.5, 2.5) {32MB Bit band alias};
% Address for Alias
\node[addr, anchor=east] at (sram_alias.south west) {0x22000000};
\node[addr, anchor=east] at (sram_alias.north west) {0x23FFFFFF};

% Connect lines for SRAM
\draw (sram_alias.north east) -- (sram_target);
\draw (sram_alias.south east) -- (sram_target);
\draw (sram_region.north east) -- (sram_target);
\draw (sram_region.south east) -- (sram_target);


% - PERIPHERAL GROUP -
% Define the target point (Base of Peripheral)
\coordinate (periph_target) at (periph.south west);

% 1MB Region Box (Level with Peripheral base)
\node[sideblock, anchor=east] (periph_region) at (-2.5, 4.2) {1MB Bit band region};
% Address for 1MB Region
\node[addr, anchor=east] at (periph_region.south west) {0x40000000};
\node[addr, anchor=east] at (periph_region.north west) {0x400FFFFF};

% Alias Box (Floated above)
\node[sideblock, anchor=east, minimum height=0.8cm] (periph_alias) at (-2.5, 5.5) {32MB Bit band alias};
% Address for Alias
\node[addr, anchor=east] at (periph_alias.south west) {0x42000000};
\node[addr, anchor=east] at (periph_alias.north west) {0x43FFFFFF};

% Connect lines for Peripheral
\draw (periph_alias.north east) -- (periph_target);
\draw (periph_alias.south east) -- (periph_target);
\draw (periph_region.north east) -- (periph_target);
\draw (periph_region.south east) -- (periph_target);

\end{tikzpicture}
\caption{Cortex-M4 Memory Map with Bit-banding regions (Adapted from \cite{ARM-CortexM4-Generic-User-Guide})}
\label{fig:cortex_m4_memory}
\end{figure}

\subsection{Flattened Device Tree (DTB)}

To allow a single kernel binary to support multiple board configurations without hardcoding, architectures such as ARM64 and RISC-V utilize the \textbf{Flattened Device Tree}. The bootloader passes a pointer to a binary structure (the DTB Blob) which describes the hardware topology, including physical memory ranges and reserved regions.

The DTB format encodes the device tree into a linear, pointerless data structure. It consists of a fixed-size header followed by three variable-sized blocks:
\begin{enumerate}
    \item \textbf{Memory Reservation Block}: Lists physical memory ranges that the kernel must not overwrite (e.g., firmware runtime data).
    \item \textbf{Structure Block}: Describes the device nodes and properties in a linear tree format using token-based tags.
    \item \textbf{Strings Block}: A pool of null-terminated property names referenced by offset.
\end{enumerate}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    block/.style={
        rectangle, 
        draw=black, 
        thick,
        minimum width=5cm, 
        minimum height=0.8cm, 
        align=center, 
        fill=white
    },
    arrow/.style={-Latex, thick, color=black!70}
]

\node[block, fill=gray!10] (header) {\textbf{Header}\\ \scriptsize (Magic, Totalsize, Offsets)};
\node[block, below=0.1cm of header] (reserve) {Memory Reservation Block};
\node[block, below=0.1cm of reserve] (struct) {Structure Block\\ \scriptsize (Nodes \& Properties)};
\node[block, below=0.1cm of struct] (strings) {Strings Block\\ \scriptsize (Property Names)};

% Pointers
\draw[arrow] (header.east) -- ++(0.5,0) |- node[pos=0.7, right, font=\scriptsize] {off\_mem\_rsvmap} (reserve.east);
\draw[arrow] (header.east) -- ++(0.8,0) |- node[pos=0.7, right, font=\scriptsize] {off\_dt\_struct} (struct.east);
\draw[arrow] (header.east) -- ++(1.1,0) |- node[pos=0.7, right, font=\scriptsize] {off\_dt\_strings} (strings.east);

\end{tikzpicture}
\caption{Structure of a Flattened Device Tree Blob}
\label{fig:dtb_structure}
\end{figure}
\todo[inline]{Probably delete this, unless it can be extended or modified to bring more value}

The kernel parses this blob at boot to discover available RAM. Notably, the DTB standard mandates \textbf{Big-Endian} byte ordering. On Little-Endian architectures like x86-64, the kernel must perform byte-swapping when parsing these structures.

\cite{Devicetree-Spec}

\subsection{Firmware Interrogation}

On general-purpose platforms (x86-64), the hardware is modular. The kernel cannot predict the amount of installed RAM or the physical address map. In this scenario, the kernel must query the system firmware directly. This introduces a dependency on the firmware interface:
\begin{itemize}
    \item \textbf{Legacy BIOS:} Requires invoking interrupt vectors (e.g., \texttt{INT 0x15, EAX=0xE820}) to retrieve a list of memory ranges.
    \item \textbf{UEFI:} Requires calling specific boot services (\texttt{GetMemoryMap}) to retrieve descriptors of physical pages and their attributes.
\end{itemize}

\cite{UEFI-Base-Spec}

\subsection{Hardware Abstraction}
How does the kernel handle such diverse set of methods of querying the memory? This is a part of a bigger topic - namely Hardware / Architecture Abstraction - and wil be discussed in more detail in \todo[inline]{add label to hardware abstraction layer / problem}

\subsection{Discovering and Enabling CPU Features}
\label{subsec:cpu_discovery}

Modern Central Processing Units (CPUs) are not monolithic entities with a fixed feature set. Instead, they represent an accumulation of decades of architectural extensions. A generic x86-64 processor guarantees a baseline instruction set (User-level ISA), but specific capabilities regarding vectorization (AVX, AVX-512), cryptography (AES-NI), security (SMEP, SMAP), and performance (PCID, TSC-Deadline) vary significantly between processor generations and manufacturers.
\cite{Intel-AVX}

An operating system kernel cannot blindly execute advanced instructions. Doing so on hardware that lacks support results in an \textit{Invalid Opcode} exception, causing a kernel panic. Therefore, a robust kernel must perform a feature discovery handshake during the early initialization phase.

\subsubsection{Feature Identification}
On the x86 architecture, this discovery is performed via the \texttt{CPUID} instruction. This instruction acts as a query interface where the software loads a leaf index into the \texttt{EAX} register (and optionally a subleaf in \texttt{ECX}) and executes \texttt{CPUID}. The processor returns feature bitmaps and vendor information in the general-purpose registers (\texttt{EAX}, \texttt{EBX}, \texttt{ECX}, \texttt{EDX}).
\cite{Intel-CPUID}

While x86 relies on this dynamic instruction-based discovery, other architectures employ different strategies:
\begin{itemize}
    \item \textbf{ARM64 (AArch64)} utilizes special system registers (e.g., \texttt{ID\_AA64PFR0\_EL1}) that the kernel reads to determine support for floating-point units or cryptographic extensions.
    \item \textbf{RISC-V} typically employs the Device Tree Blob (DTB) or the \texttt{misa} (Machine ISA) Control and Status Register to inform the kernel about supported standard extensions (e.g., Atomics, Floats).
\end{itemize}
\cite{ARM-Arch-Ref, RISCV-Priv-Spec}

\subsubsection{Feature Enablement}
Identifying that a feature exists is often insufficient; the kernel must explicitly enable it. To maintain backward compatibility and minimize power consumption, processors often boot with advanced features disabled.

A prime example is the Floating Point Unit (FPU) and Vector Extensions (SSE/AVX). On x86-64, even if \texttt{CPUID} reports that AVX is supported, attempting to execute a \texttt{VMOVDQA} instruction will fault unless the kernel has:
\begin{enumerate}
    \item Enabled the FPU by clearing the Emulation bit in Control Register \texttt{CR0}.
    \item Enabled SSE by setting the \texttt{OSFXSR} bit in \texttt{CR4}.
    \item Enabled XSAVE/XRSTOR support by setting the \texttt{OSXSAVE} bit in \texttt{CR4}.
    \item Explicitly enabled AVX state saving in the Extended Control Register (\texttt{XCR0}).
\end{enumerate}
\cite{Intel-ControlRegisters}

This enabling phase is critical not only for allowing instruction execution but also for the scheduler. The operating system must know the size of the processor's register state (Context) to correctly save and restore threads during context switches. If AVX-512 is enabled, the context size increases significantly compared to standard SSE, impacting memory usage and context switch latency.

\cite{Intel-XSAVE}

\subsection{Establishment of Basic Communication}

One of the primary objectives when initializing code on a target architecture is to establish an external communication channel. In an emulation environment, this is often achieved by interacting with the emulator's framework (e.g., QEMU utilizes a serial port that can be attached to a Linux shell session). On physical hardware, the developer may need to render fonts on a screen (e.g., using the VGA standard on AMD64 desktop platforms) or implement a basic network stack. The preferred method should support bidirectional communication during the early development stages to facilitate testing and provide input to the kernel. This functionality is primarily required for debugging and testing and as development progresses, it is advisable to disable this communication and associated debug traces via compilation flags. It is crucial to strictly separate debug-only communication from standard output devices, as the former must be disabled in release builds to ensure performance and security.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{res/debug-output.png}
    \caption{QEMU serial-port Communication}
    \label{fig:qemu_comms}
\end{figure}

As shown in Figure \ref{fig:qemu_comms}, alongside typical output to the screen and keyboard input, the kernel transmits text to the QEMU serial port, which is then streamed to the host shell. This approach enables host-side scripts to parse logs, detect bugs or failures, and allow manual inspection of the system state immediately preceding a crash.

\subsection{Enabling Interrupts and Exceptions}
\label{subsec:os-tutorial-interrupts}

Prior to the implementation of memory management (Section \ref{subsec:os_tutorial_mem}), it is essential to establish a basic interrupt handling mechanism. On most platforms, the exception system relies entirely on interrupt mappings. Consequently, without a functional interrupts, the kernel is unable to display debug information on the designated communication device when an error occurs.

Exception handlers should output a descriptive message detailing the failure. This message must include the CPU state, the source of the exception, the instruction pointer where the error occurred, and, if applicable, an error code explaining the cause. An example of a kernel dump following an exception is presented below:

\begin{lstlisting}[caption={Example Kernel Panic on Exception}, label={lst:kernel_panic}]
    [ KERNEL PANIC ] Received exception: 0 (Divide Error (#DE))
    And error: 0
    At instruction address: 0xffffffff8021b856
    rip:                    0xffffffff8021b856
    rflags:                 0x0000000000010246
    rsp:                    0xffffffff8138dc20
    rax:                    0x0000000000000009
    rbx:                    0x000000000006f000
    rcx:                    0x0000000000000000
    rdx:                    0x0000000000000000
    rsi:                    0xffffffff803648f3
    rdi:                    0xffffffff80818c28
    rbp:                    0xffffffff8138dc40
    r8:                     0xffffffff80305d00
    r9:                     0xffffffff80305d59
    r10:                    0xffffffff8138df10
    r11:                    0xffffffff802fdfaf
    r12:                    0x0000000000000010
    r13:                    0x0000000000180000
    r14:                    0x0000000001a91000
    r15:                    0x0000000000000800
    
    RFLAGS:                 0x0000000000010246
\end{lstlisting}

Before initializing Memory Management, exception handling is the primary function required from the interrupt system. At this stage, there is typically no use case for interrupt-driven devices, as the consumers (processes) have not yet been booted, and multicore operations (which require memory for bookkeeping structures) are not active. However, the design of the interrupt subsystem must account for future extensibility and requirements.

\subsubsection{Design Considerations}

It is important to note that low-level, hardware-compliant interrupt handlers differ significantly from standard compiled functions. They often require specific entry and exit instructions, making the runtime swapping of these functions difficult. A robust solution is to create a low-level assembly wrapper that efficiently performs all architecture-dependent operations before invoking a high-level function responsible for the kernel's logic. However, this approach has limitations. The high-level function is typically hardcoded into the low-level handler, preventing runtime reconfiguration.

To address this, a hardware abstraction layer is required to serve as an intermediary between the architecture-specific wrapper and the high-level kernel logic. This abstraction should manage the low-level tables and facilitate the swapping of handlers at runtime. This can be implemented using an array of abstract handlers mapped to hardware interrupts. Furthermore, devices are mapped to hardware interrupts dynamically during runtime based on user configuration and hardware discovery, necessitating the ability to swap drivers and logic dynamically. Without this modularity, the system would require multiple stages of interrupt initialization, multiple architecture-specific handlers, or complex handlers that query the kernel state to determine valid operations (e.g., a page fault handler checking for virtualization support each time is invoked).

It is also critical to design handlers to perform minimal work. This precludes operations such as waiting, sleeping, or extensive tracing. If device handling requires complex logic or a state change, the handler should invoke the scheduler to perform a context switch upon exiting the interrupt. Executing complex logic within the interrupt context blocks other interrupts and may lead to data loss. Consequently, the interrupt abstraction layer must also support task switching.

When implementing this abstraction, three distinct classes of interrupts can be identified:

\begin{itemize}
    \item \textbf{Exceptions} -- Generated by the CPU to indicate specific conditions requiring immediate attention, such as Page Faults, Division by Zero, or invalid instruction operands.
    \item \textbf{Hardware Interrupts} -- Generated by external devices (e.g., Timers, Keyboard, Disk Controller) to communicate efficiently with the kernel without the need for polling.
    \item \textbf{Software Interrupts} -- Initiated by software instructions. For example, on the x86-64 architecture, the instruction \texttt{INT 0x80} triggers an interrupt with the vector number \texttt{0x80}.
\end{itemize}

Architecture-specific details must also be considered. for example, the AMD64 architecture utilizes the legacy PIC \cite{osdev-pic} and the improved, SMP-supporting APIC \cite{osdev-apic}. Finally, to enable Symmetric Multiprocessing (SMP) and utilize multiple cores, the interrupt mechanism serves as the primary method of inter-core communication.

\subsection{Tracing System}

As discussed in Section \ref{subsec:os-tutorial-interrupts}, direct tracing inside interrupt handlers is generally discouraged due to latency concerns. However, in a general context, kernel logs and debug messages must be preserved to a file or terminal. The challenge is that writing to a file or physical device incurs significant latency, while system code must execute as rapidly as possible. To resolve this, the tracing framework must be robust enough to operate in a concurrent environment (handling interrupts and SMP) while decoupling the generation of traces from their output. This can be achieved by allocating a large circular buffer for messages, which is then asynchronously flushed to the output device by a dedicated, low-priority task. Furthermore, to facilitate effective debugging in a multi-threaded and multi-core environment, each log entry should automatically capture essential context metadata, specifically the high-precision timestamp, the current Core ID, and the Process ID.

This goes about the general design, but during initial development system there is no need to have full tracing system, simple print with formatting is enough to proceed.

\subsection{Testing framework}

Once bidirectional communication with an external system is established and tracing capabilities are active, it is advisable to implement a protocol for verifying code directly on the target system. Such a framework facilitates regression and unit testing, enabling the detection of failures with every code modification. Testing must be conducted on the target architecture, as code that functions correctly on a host Linux environment may fail locally due to missing CPU features, implementation differences in system calls, memory management constraints, or discrepancies in generated assembly. The design must isolate test executions to ensure that the side effects of a failed test do not compromise subsequent ones. Ideally, isolation is achieved by rebooting the system into a testing module after each test. While this method introduces latency, it prevents false positives that would otherwise consume significant development time. Consequently, the framework must be designed to yield reliable and repeatable results.

\subsection{Memory Management}
\label{subsec:os_tutorial_mem}
TODO: KRYCZKA

\subsubsection{Physical Memory Management}
\label{subsubsec:physical_memory_management}
\todo[inline]{A ton of info here, needs some refinment, cleanup, to make it flow smoothly}

The Physical Memory Manager (PMM) forms the foundational layer of any operating system's memory subsystem. It is responsible for tracking, allocating, and reclaiming the machine's finite physical RAM-measured in discrete units called \textit{page frames} (typically 4~KiB on x86-64). 
Allocators that support multi-page requests manage contiguous groups of pages. 

\paragraph{Design Objectives}
A PMM must balance three competing objectives:
\begin{enumerate}
    \item \textbf{Throughput and Latency:} Minimize CPU cycles per allocation/deallocation.
    \item \textbf{Fragmentation Minimization:} Prevent the degradation of usable memory into scattered, unusable holes.
    \item \textbf{Hardware Compliance:} Respect physical addressing constraints imposed by devices (e.g., legacy ISA DMA requiring addresses below 16~MiB).
\end{enumerate}

The choice of allocation algorithm fundamentally shapes the PMM's performance profile. Before examining specific algorithms, it is essential to deeply understand the core problem: \textbf{fragmentation}-the inability to reuse memory that is free. This occurs when free regions exist but are too small or poorly positioned to satisfy requests.

\textbf{Internal vs.\ External Fragmentation.} Operating system developers must distinguish between two forms. \textit{Internal fragmentation} arises when allocated regions are larger than requested due to size-class rounding (e.g., a 50-byte request rounded to 64 bytes wastes 14 bytes). \textit{External fragmentation} occurs when free memory is scattered into many small holes, none large enough to satisfy a request, even though total free memory exceeds the request size.

\textbf{The Root Cause: Isolated Deaths.} Fragmentation fundamentally arises from \textit{isolated deaths}-when an object is freed but its neighbors remain allocated, creating an unusable hole. If adjacent objects always died together, their combined space would be reclaimed as one contiguous block. The allocator's core challenge is predicting which objects will die at similar times and placing them contiguously. Since the allocator cannot know future behavior, it must rely on heuristics that exploit regularities in program behavior~\cite{wilson1995survey}.

\textbf{Program Behavior Patterns.} Most real programs do not behave randomly. Studies of allocation traces reveal three dominant patterns~\cite{wilson1995survey}:
\begin{itemize}
    \item \textbf{Ramps:} Monotonic accumulation of long-lived data (e.g., building a parse tree).
    \item \textbf{Peaks:} Bursty allocation of temporary structures that are collectively discarded after a phase (e.g., processing one request).
    \item \textbf{Plateaus:} Rapid initial allocation followed by stable long-term usage.
\end{itemize}
These patterns are exploitable: objects allocated together often die together. Address-ordered allocation (using low addresses first) tends to cluster temporally-related objects, and when a phase ends, contiguous regions become free simultaneously.

\textbf{Placement Policies.} Allocators implement placement policies that determine where to satisfy a request:
\begin{itemize}
    \item \textbf{First Fit:} Use the first sufficiently large free region (address-ordered). Fast, and tends to pack one end of memory.
    \item \textbf{Best Fit:} Use the smallest region that is large enough. Minimizes wasted remainder but may accumulate unusable ``splinters.''
    \item \textbf{Next Fit:} Resume searching from where the last allocation stopped. Scatters allocations and often performs poorly.
\end{itemize}
Empirical studies show that best fit and address-ordered first fit perform similarly well, both significantly outperforming next fit for real workloads~\cite{wilson1995survey}.

\textbf{Strategy, Policy, and Mechanism.} Allocator design involves three levels of abstraction~\cite{wilson1995survey}: a \textbf{strategy} exploits regularities in program behavior (e.g., ``cluster objects that will die together''), a \textbf{policy} determines where to place allocations (e.g., best fit), and a \textbf{mechanism} implements the policy efficiently (e.g., segregated free lists). Understanding this hierarchy clarifies why different allocators can achieve similar fragmentation results-they may implement equivalent policies via different mechanisms.

With this foundation, we examine four allocation mechanisms in order of increasing sophistication. 

\paragraph{Bitmap Allocation}

The bitmap allocator is the most straightforward approach. The entire physical address space is represented as a bit array where each bit corresponds to one page frame-0 indicates free, 1 indicates allocated.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    bit/.style={draw, minimum width=0.5cm, minimum height=0.5cm, anchor=south west},
    freeb/.style={bit, fill=green!20},
    usedb/.style={bit, fill=red!20}
]

% Draw bitmap array
\node[anchor=east] at (-0.2, 0.25) {\textbf{Bitmap:}};
\node[freeb] at (0, 0) {0};
\node[usedb] at (0.5, 0) {1};
\node[usedb] at (1.0, 0) {1};
\node[freeb] at (1.5, 0) {0};
\node[freeb] at (2.0, 0) {0};
\node[usedb] at (2.5, 0) {1};
\node[freeb] at (3.0, 0) {0};
\node[freeb] at (3.5, 0) {0};
\node at (4.2, 0.25) {...};

% Index labels
\node[font=\tiny] at (0.25, -0.3) {0};
\node[font=\tiny] at (0.75, -0.3) {1};
\node[font=\tiny] at (1.25, -0.3) {2};
\node[font=\tiny] at (1.75, -0.3) {3};
\node[font=\tiny] at (2.25, -0.3) {4};
\node[font=\tiny] at (2.75, -0.3) {5};
\node[font=\tiny] at (3.25, -0.3) {6};
\node[font=\tiny] at (3.75, -0.3) {7};

% Physical addresses
\node[anchor=west, font=\scriptsize] at (5, 0.25) {Frame $i$ $\rightarrow$ Address $i \times 4096$};

% Legend
\node[freeb, minimum width=0.4cm, minimum height=0.4cm] at (0, -1) {};
\node[anchor=west, font=\scriptsize] at (0.5, -0.8) {Free};
\node[usedb, minimum width=0.4cm, minimum height=0.4cm] at (2, -1) {};
\node[anchor=west, font=\scriptsize] at (2.5, -0.8) {Allocated};

\end{tikzpicture}
\caption{Bitmap Representation of Physical Memory}
\label{fig:bitmap_allocator}
\end{figure}

To allocate $n$ contiguous pages, the allocator must scan for $n$ consecutive zero bits. This results in $O(N)$ worst-case complexity, where $N$ is the total number of physical pages. For a 16~GiB system with 4~KiB pages, this means scanning over 4 million bits in the worst case-introducing significant latency and non-determinism.

Optimizations include processing entire machine words (64 bits at once using intrinsic instructions like \texttt{\_\_builtin\_ffsll}) and maintaining a ``roaming pointer'' to skip permanently allocated regions at the start of memory. The space overhead is minimal: a 4~GiB address space requires only 128~KiB of bitmap data.

Bitmaps have a subtle advantage for allocation policy: they naturally support \textit{address-ordered search}. Scanning from low addresses finds the first physically lowest block, which tends to pack allocations at one end of memory and cluster temporally-related objects. This implements an address-ordered first fit policy implicitly. Additionally, bitmaps are stored ``off to the side''-separate from the data they describe-which can improve cache locality during the search phase compared to traversing linked structures embedded within memory blocks.

\paragraph{Stack-Based (Free List) Allocation}

While bitmaps excel at contiguous allocation, their $O(N)$ search is expensive for single-page allocations-the most common case. A faster alternative trades contiguity support for speed.

The stack allocator maintains a Last-In-First-Out (LIFO) stack of free page addresses. Since the pages themselves are free, the ``next'' pointer can be stored directly within the page's memory-eliminating external metadata overhead.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    page/.style={draw, minimum width=2.5cm, minimum height=0.8cm, fill=green!10},
    arrow/.style={-Latex, thick}
]

\node[page] (p1) at (0, 0) {Page @ 0x5000};
\node[page] (p2) at (0, -1.2) {Page @ 0x9000};
\node[page] (p3) at (0, -2.4) {Page @ 0x2000};
\node[font=\scriptsize] at (0, -3.4) {...};

\node[anchor=east, font=\bfseries] at (-1.8, 0) {HEAD $\rightarrow$};

\draw[arrow] (p1.south) -- (p2.north);
\draw[arrow] (p2.south) -- (p3.north);

\node[anchor=west, align=left, font=\scriptsize] at (2, -0.6) {Pop: $O(1)$\\Push: $O(1)$};
\node[anchor=west, align=left, font=\scriptsize, text=red!70!black] at (2, -2.0) {No contiguous\\allocation support};

\end{tikzpicture}
\caption{Stack-Based Free List Allocator}
\label{fig:stack_allocator}
\end{figure}

This approach offers strict $O(1)$ performance for single-page operations-no searching is required. The key policy decision is whether to use a LIFO (stack) or FIFO (queue) ordering:
\begin{itemize}
    \item \textbf{LIFO (Last-In-First-Out):} Recently freed pages are reused immediately. This maximizes CPU cache locality-the page data likely remains in cache. However, it mixes allocations from different program phases, potentially scattering objects that should be grouped.
    \item \textbf{FIFO (First-In-First-Out):} Pages ``age'' before reuse, giving the system time to reclaim them efficiently (e.g., for write-back). Recent studies suggest FIFO ordering can reduce fragmentation compared to pure LIFO, performing nearly as well as address ordering~\cite{wilson1995survey}.
\end{itemize}

When all pages are the same size (as in page frame allocation), this structure implements \textit{simple segregated storage}: all blocks in a page-sized ``size class'' are on one list. This eliminates internal fragmentation entirely-every returned page is exactly the requested size. However, the stack cannot efficiently satisfy requests for \textit{contiguous} multi-page allocations (e.g., for large DMA buffers), as the order of pages reflects deallocation history, not physical adjacency.

\paragraph{The Buddy System}

The stack achieves $O(1)$ speed but cannot allocate contiguous ranges efficiently. The Buddy System is the standard algorithm in general-purpose kernels~\cite{knuth1973art}, though it comes with significant trade-offs that must be understood.

The Buddy System is fundamentally a \textit{segregated free list} scheme with a constrained splitting and coalescing discipline. Unlike general segregated fits that can coalesce any adjacent blocks using boundary tags, the Buddy System restricts coalescing to ``buddy pairs''-blocks that together form a power-of-two aligned parent. This restriction enables an elegant $O(1)$ buddy lookup (via XOR) but limits flexibility: adjacent free blocks that are not buddies \textit{cannot be merged}, even if they are physically contiguous~\cite{wilson1995survey}.

Memory is organized into blocks sized as powers of two ($2^k$ pages). The allocator maintains an array of free lists, one per ``order'' $k$:
\begin{itemize}
    \item Order 0: 4~KiB blocks (1 page)
    \item Order 1: 8~KiB blocks (2 pages)
    \item Order 10: 4~MiB blocks (1024 pages)
\end{itemize}

\textbf{The Core Insight: Binary Address Subdivision.} The elegance of the Buddy System lies in how physical addresses naturally subdivide into buddy pairs. Consider a block of order $k$ at address $A$. Its ``buddy'' is the adjacent block of the same size that, together with it, forms a block of order $k+1$. The buddy's address can be computed by flipping bit $k$ (counting from zero) in the address:

\[
\texttt{buddy\_addr} = A \oplus (1 \ll k)
\]

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    memblock/.style={draw, minimum height=0.6cm, anchor=west},
    addr/.style={font=\ttfamily\scriptsize}
]

% Order 2 level (full block)
\node[memblock, minimum width=8cm, fill=gray!15] (o2) at (0, 3) {};
\node[above=0.1cm of o2, font=\bfseries\small] {Order 2 (16 KiB)};
\node[addr, anchor=west] at (0, 2.3) {0b...0\textcolor{red}{0}00};
\node[addr, anchor=east] at (8, 2.3) {0b...1111};

% Order 1 level (two buddies)
\node[memblock, minimum width=4cm, fill=blue!15] (o1a) at (0, 1.2) {};
\node[memblock, minimum width=4cm, fill=blue!25] (o1b) at (4, 1.2) {};
\node[above=0.05cm of o1a, font=\scriptsize] {Buddy A};
\node[above=0.05cm of o1b, font=\scriptsize] {Buddy B};
\node[addr, anchor=west] at (0, 0.5) {0b...\textcolor{red}{0}000};
\node[addr, anchor=west] at (4, 0.5) {0b...\textcolor{red}{1}000};

% Order 0 level (four blocks)
\node[memblock, minimum width=2cm, fill=green!15] (o0a) at (0, -0.5) {};
\node[memblock, minimum width=2cm, fill=green!25] (o0b) at (2, -0.5) {};
\node[memblock, minimum width=2cm, fill=green!15] (o0c) at (4, -0.5) {};
\node[memblock, minimum width=2cm, fill=green!25] (o0d) at (6, -0.5) {};
\node[addr] at (1, -1.2) {\textcolor{red}{0}0};
\node[addr] at (3, -1.2) {\textcolor{red}{0}1};
\node[addr] at (5, -1.2) {\textcolor{red}{1}0};
\node[addr] at (7, -1.2) {\textcolor{red}{1}1};

% Labels
\node[anchor=east] at (-0.3, 3) {Order 2};
\node[anchor=east] at (-0.3, 1.2) {Order 1};
\node[anchor=east] at (-0.3, -0.5) {Order 0};

% Annotation
\node[anchor=west, align=left, font=\scriptsize] at (8.5, 1.2) {Bit $k$ (red) determines\\which half of parent};

\end{tikzpicture}
\caption{Buddy System: Address Bits Determine Buddy Pairs}
\label{fig:buddy_address}
\end{figure}

This property enables $O(1)$ buddy lookup during coalescing. When a block is freed, the allocator computes its buddy address with a single XOR, checks if the buddy is free, and if so, merges them into a larger block. This merge propagates up recursively until a buddy is found to be in use.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    block/.style={draw, minimum height=0.6cm, fill=blue!10},
    freeblock/.style={draw, minimum height=0.6cm, fill=green!20},
    arrow/.style={-Latex, thick, dashed}
]

% Original block
\node[block, minimum width=8cm] (orig) at (0, 2) {Order 3 Block (32 KiB)};

% Split into two Order 2
\node[freeblock, minimum width=4cm] (l2a) at (-2, 0.8) {Order 2 (16 KiB)};
\node[block, minimum width=4cm] (l2b) at (2, 0.8) {Order 2 (16 KiB)};

% Further split left Order 2
\node[freeblock, minimum width=2cm] (l1a) at (-3, -0.4) {Order 1};
\node[block, minimum width=2cm] (l1b) at (-1, -0.4) {Order 1};

% Further split
\node[freeblock, minimum width=1cm] (l0a) at (-3.5, -1.6) {O0};
\node[block, minimum width=1cm, fill=red!20] (l0b) at (-2.5, -1.6) {O0};

\draw[arrow] (orig.south) -- (l2a.north);
\draw[arrow] (orig.south) -- (l2b.north);
\draw[arrow] (l2a.south) -- (l1a.north);
\draw[arrow] (l2a.south) -- (l1b.north);
\draw[arrow] (l1a.south) -- (l0a.north);
\draw[arrow] (l1a.south) -- (l0b.north);

\node[anchor=west, font=\scriptsize] at (4.5, 0) {Split on miss};
\node[anchor=west, font=\scriptsize, text=red!70!black] at (-0.5, -1.6) {Allocated};
\node[anchor=west, font=\scriptsize, text=green!50!black] at (-3, -2.2) {Buddy (free)};

\end{tikzpicture}
\caption{Buddy System: Recursive Splitting}
\label{fig:buddy_split}
\end{figure}

When a request arrives, it is rounded up to the nearest power of two. If no block of that order is available, a larger block is recursively split into two ``buddies.'' On deallocation, if a block's unique buddy is also free, they are merged (coalesced) back into a larger block. This achieves $O(\log k)$ complexity where $k$ is the maximum order-effectively constant time in practice.

\textbf{Internal Fragmentation: The Primary Drawback.} The power-of-two size restriction causes substantial internal fragmentation. A request for 33~KiB requires a 64~KiB block, wasting approximately 48\% of the allocation. Statistical analysis shows \textit{expected} internal fragmentation is approximately 28\%, though this varies with the request size distribution~\cite{wilson1995survey}. In contrast, general segregated fits schemes with finer size classes (e.g., 10--20\% spacing) can reduce internal fragmentation to just a few percent.

\textbf{External Fragmentation: The Coalescing Limitation.} The buddy restriction on coalescing can also cause external fragmentation. Consider two adjacent free blocks that are \textit{not} buddies-they cannot merge, leaving unusable gaps. Studies using both real and synthetic traces show that buddy systems generally exhibit \textit{more} fragmentation than segregated fits with boundary tags, where any adjacent blocks can coalesce~\cite{wilson1995survey}.

Despite these drawbacks, the Buddy System remains popular because: (1) it is conceptually simple; (2) the XOR-based buddy lookup is extremely fast; and (3) for page-frame allocation where the atomic unit is 4~KiB, internal fragmentation only matters for multi-page allocations.

\paragraph{Two-Level Segregated Fit (TLSF)}

The Buddy System demonstrates that segregated free lists with appropriate coalescing can balance speed and contiguity. But its power-of-two constraint wastes substantial memory. Can we achieve $O(1)$ performance with finer-grained size classes and general coalescing?

For real-time systems where deterministic latency is critical, the TLSF allocator answers this question affirmatively, guaranteeing strict $O(1)$ worst-case execution time for both allocation and deallocation, without the internal fragmentation penalty of power-of-two rounding.

\textbf{Segregated Fit Background.} Segregated fit allocators maintain multiple free lists, each holding blocks within a specific size range. Given a request, the allocator maps the size to an index and pops from the corresponding list. The challenge is making this mapping-and finding a non-empty list-fast.

\textbf{The Two-Level Index.} TLSF solves this with a hierarchical bitmap structure:

\begin{enumerate}
    \item \textbf{First-Level Index (FLI):} Partitions sizes by powers of two. Each FLI class $i$ covers sizes in the range $[2^i, 2^{i+1})$. The FLI is determined by: $\texttt{fli} = \lfloor \log_2(\texttt{size}) \rfloor$
    \item \textbf{Second-Level Index (SLI):} Each FLI class is subdivided into $2^J$ linear sub-classes (typically $J=4$ or $J=5$, giving 16 or 32 subdivisions). Within FLI class $i$, the SLI extracts the next $J$ most significant bits after the leading bit.
\end{enumerate}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    flibox/.style={draw, minimum width=0.7cm, minimum height=0.7cm},
    slibox/.style={draw, minimum width=0.5cm, minimum height=0.5cm, fill=blue!10},
    listbox/.style={draw, rounded corners, minimum width=1.5cm, minimum height=0.5cm, fill=green!15},
    arrow/.style={-Latex, thick}
]

% Size input
\node[font=\bfseries] at (-3, 2) {Request: 100 bytes};
\node[font=\scriptsize] at (-3, 1.4) {$\log_2(100) = 6$};
\node[font=\scriptsize] at (-3, 0.9) {FLI = 6 (range 64--127)};

% FLI bitmap
\node[anchor=east, font=\bfseries] at (0, 2) {FLI:};
\node[flibox] (f4) at (0.5, 2) {\tiny 4};
\node[flibox] (f5) at (1.3, 2) {\tiny 5};
\node[flibox, fill=yellow!40] (f6) at (2.1, 2) {\tiny 6};
\node[flibox] (f7) at (2.9, 2) {\tiny 7};
\node[flibox] (f8) at (3.7, 2) {\tiny 8};
\node at (4.5, 2) {...};

% SLI for FLI 6
\node[anchor=east, font=\bfseries] at (0, 0.5) {SLI[6]:};
\node[slibox] (s0) at (0.5, 0.5) {};
\node[slibox] (s1) at (1.1, 0.5) {};
\node[slibox, fill=yellow!40] (s2) at (1.7, 0.5) {1};
\node[slibox] (s3) at (2.3, 0.5) {};

% Labels for SLI
\node[font=\tiny] at (0.5, -0.1) {64-79};
\node[font=\tiny] at (1.1, -0.1) {80-95};
\node[font=\tiny, text=red!70!black] at (1.7, -0.1) {96-111};
\node[font=\tiny] at (2.3, -0.1) {112-127};

% Free list
\draw[arrow] (s2.south) -- ++(0, -0.6) -- ++(1.5, 0);
\node[listbox] (list) at (4.5, -0.8) {Free: 104 B};

% Mapping annotation
\draw[arrow, dashed, gray] (f6.south) -- (s2.north);

\end{tikzpicture}
\caption{TLSF: Size-to-Index Mapping (J=2 for illustration)}
\label{fig:tlsf_mapping}
\end{figure}

\textbf{Constant-Time Lookup.} Both the FLI and each SLI are represented as bitmaps (typically 32 or 64 bits). When the target list is empty, the allocator must find the next larger non-empty list. This is accomplished with a single hardware \texttt{ffs} (find first set) instruction on the bitmap-a constant-time operation. No iteration is required.

\textbf{Allocation Algorithm:}
\begin{enumerate}
    \item Map requested size to $(fli, sli)$ indices.
    \item If the corresponding SLI bit is set, pop a block from that list.
    \item If not, use \texttt{ffs} on the SLI bitmap to find the next non-empty sub-class. If the entire SLI is empty, use \texttt{ffs} on the FLI to find the next populated size class.
    \item Split the found block if necessary; insert the remainder back into the appropriate list.
\end{enumerate}

\textbf{Deallocation and Coalescing.} Each allocated block carries a small header (and optionally a footer) containing its size and status. On free, TLSF checks the \textit{physical neighbors} (not buddies-any adjacent block) using these boundary tags. If a neighbor is free, the blocks are merged. Unlike the Buddy System, TLSF can coalesce any adjacent free blocks regardless of their sizes, reducing external fragmentation.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    block/.style={draw, minimum height=1cm, anchor=west},
    header/.style={draw, minimum height=0.3cm, fill=gray!30, anchor=west},
    freeblk/.style={block, fill=green!15},
    usedblk/.style={block, fill=red!15}
]

% Memory layout
\node[header, minimum width=0.8cm] (h1) at (0, 0) {\tiny H};
\node[freeblk, minimum width=2cm] (b1) at (0.8, 0) {Free (64B)};
\node[header, minimum width=0.8cm] (h2) at (2.8, 0) {\tiny H};
\node[usedblk, minimum width=3cm] (b2) at (3.6, 0) {Used (96B)};
\node[header, minimum width=0.8cm] (h3) at (6.6, 0) {\tiny H};
\node[freeblk, minimum width=2cm] (b3) at (7.4, 0) {Free (48B)};

% Arrows for neighbor check
\draw[-Latex, thick, blue] (b2.north west) to[out=120, in=60] node[above, font=\scriptsize] {check prev} (b1.north east);
\draw[-Latex, thick, blue] (b2.north east) to[out=60, in=120] node[above, font=\scriptsize] {check next} (b3.north west);

\node[anchor=north, font=\scriptsize] at (4.5, -0.8) {Headers store size $\rightarrow$ locate neighbors in $O(1)$};

\end{tikzpicture}
\caption{TLSF: Boundary Tags Enable Arbitrary Coalescing}
\label{fig:tlsf_coalesce}
\end{figure}

TLSF achieves ``good fit'' allocation (blocks closely matching request size) without power-of-two rounding. Measured fragmentation is typically under 15\%, compared to up to 50\% for the Buddy System. The strict $O(1)$ bounds and low fragmentation make TLSF the standard choice for embedded and real-time kernels~\cite{masmano2004tlsf}.

\textbf{Why Segregated Fits Work Well.} TLSF exemplifies segregated fit allocators, whose effectiveness is often underappreciated. By partitioning blocks into size classes, segregated fits \textit{implicitly} implement a good-fit policy: searching the list for size $s$ excludes blocks vastly larger or smaller, making most candidates reasonable fits. This policy emerges from the indexing structure rather than explicit search-a crucial insight for achieving both speed and low fragmentation~\cite{wilson1995survey}.

\textbf{Deferred Coalescing.} TLSF performs immediate coalescing, but many allocators \textit{defer} coalescing for performance. The rationale: if a block of size $s$ is freed and then size $s$ is requested again, immediate coalescing would have merged it into a larger block, requiring the larger block to be re-split. Deferred systems maintain ``quick lists'' of recently freed blocks by size class, only coalescing during memory pressure. This trades slightly higher peak memory usage for significantly faster allocation in bursty workloads~\cite{wilson1995survey}.

Having examined the core allocation algorithms, we now consider two orthogonal concerns: hardware-imposed addressing constraints, and the layered allocation architecture used in production kernels.

\paragraph{Comparative Summary}

\begin{table}[htbp]
\centering
\begin{tabular}{|l|c|c|c|l|}
\hline
\textbf{Algorithm} & \textbf{Alloc} & \textbf{Free} & \textbf{Internal Frag.} & \textbf{Use Case} \\
\hline
Bitmap & $O(N)$ & $O(1)$ & Low & Early boot, embedded \\
Stack/Free List & $O(1)$ & $O(1)$ & None & Slab backends, hot cache \\
Buddy System & $O(\log k)$ & $O(\log k)$ & High (power-of-2) & General purpose (Linux) \\
TLSF & $O(1)$ & $O(1)$ & Low & Real-time systems \\
\hline
\end{tabular}
\caption{Comparison of Physical Memory Allocation Algorithms}
\label{tab:pmm_comparison}
\end{table}

\paragraph{Addressing Constraints and Memory Partitioning}

The algorithms above assume a uniform pool of memory. In practice, physical memory is not always uniformly addressable by all system components. Hardware constraints force the PMM to consider \textit{which} physical frames can satisfy a given allocation:

\begin{itemize}
    \item \textbf{DMA Limitations:} Legacy devices (e.g., ISA bus peripherals) can only address the bottom 16~MiB of physical memory (24-bit addressing). More modern but still constrained devices may be limited to the 32-bit boundary (4~GiB). Allocations for DMA buffers must come from these restricted regions.
    \item \textbf{NUMA Topology:} On multi-socket systems, physical memory is partitioned across NUMA nodes. Allocating from a remote node incurs latency penalties, so the PMM should prefer local memory when possible.
    \item \textbf{Kernel Direct Mapping:} On 32-bit systems, the kernel's virtual address space is limited. Physical memory beyond a certain threshold (often $\sim$896~MiB) cannot be permanently mapped and requires special handling.
\end{itemize}

A common solution is to partition the physical address space into disjoint \textbf{regions} or \textbf{zones}, each managed by a separate allocator instance. Allocation requests carry metadata (flags or capabilities) indicating which regions are acceptable. The allocator attempts the preferred region first and falls back to others only if permitted. Critically, a request for DMA-capable memory \textit{cannot} fall back to high memory-the hardware simply cannot address those frames.

\paragraph{Advanced Topics and Further Reading}

The allocators described above handle the fundamental problem of page frame allocation. Real-world kernels typically layer additional mechanisms on top of the PMM for efficiency and scalability.

\textbf{Object Caching: The Slab Allocator.} For kernel objects of fixed type (inodes, task structures, network buffers), the cost of \textit{constructing} an object often exceeds the cost of allocating memory for it. Construction includes initializing mutexes, condition variables, and reference counts. The \textit{slab allocator}~\cite{bonwick1994slab} addresses this by caching \textit{constructed} objects.

A slab is a contiguous region of memory (typically one or more pages) pre-divided into fixed-size object slots. When an object is freed, it is returned to its slab \textit{without destruction}-the mutex remains initialized, the reference count at zero. On the next allocation, the object is returned immediately, avoiding constructor overhead. Measurements on SunOS showed allocation dropping from 33~microseconds to 5.7~microseconds-an 83\% reduction, mostly from avoiding reconstruction~\cite{bonwick1994slab}.

\textbf{Multiprocessor Scalability: Per-CPU Caches and Hoard.} On SMP systems, a single allocator lock becomes a contention bottleneck. Two solutions dominate:
\begin{itemize}
    \item \textbf{Per-CPU Caches:} Each CPU maintains a small local cache of free objects. Allocations are satisfied from the local cache without locking; only when the cache empties does the CPU acquire a global lock to refill. This amortizes lock overhead across many allocations.
    \item \textbf{Hoard Architecture:} Hoard~\cite{berger2000hoard} uses per-processor heaps with a global heap fallback. Memory is organized into \textit{superblocks} (contiguous chunks of same-sized objects). When a per-processor heap is empty, it requests a superblock from the global heap; when a superblock becomes mostly empty, it is returned to the global heap for rebalancing. A key invariant bounds \textit{blowup}: memory consumption is at most a constant factor times the maximum live data across all processors, preventing runaway fragmentation from producer-consumer patterns where one thread allocates and another frees.
\end{itemize}

For an in-depth treatment of allocation algorithms, fragmentation theory, and experimental methodology, the comprehensive survey by Wilson et al.~\cite{wilson1995survey} remains an essential reference.
\subsubsection{Virtual Memory Management}
TODO: KRYCZKA
\subsubsection{Virtual Address Space}
TODO: KRYCZKA
\subsubsection{Kernel Space Allocation API}
TODO: KRYCZKA

\subsection{Discovering External Devices and System Capabilities}

Upon the successful initialization of the fully functional memory management subsystem, the kernel can proceed to discover external devices and implement subsystems for capabilities such as storage, input, and networking. It is not necessary to support an exhaustive list of peripherals to achieve a minimal working kernel. Therefore, this section covers only the absolute minimum set of devices required to support the scheduler and execute user-space programs.

\subsubsection{Discovery and Abstraction (Drivers)}

Device discovery varies significantly between architectures, necessitating platform-specific implementations. For instance, on most desktop AMD64 computers, the ACPI subsystem \cite{ACPI-spec} is required to enumerate core components (such as CPUs, APICs, and non-PCI devices), followed by PCI discovery \cite{osdev-pci} to detect devices operating on PCI lanes. Architecture-independent kernel must provide some unified way allowing the archtiecture to perform the discovery at a proper moment. Furthermore it must provide well defined abstraction for various device types, which must be strictly followed by architecture implementation. Example driver interface may look like this:

\begin{lstlisting}[caption={Event Clock Driver Structure}, label={lst:eventClockDriver-tutorial}]
    struct alignas(arch::kCacheLineSizeBytes) EventClockRegistryEntry : data_structures::RegistryEntry {
        /* Clock numbers */
        u64 min_next_event_time_ns;  // Minimum time for the next event in nanoseconds
    
        /* Clock specific data */
        EventClockFlags flags;     // Features of the event clock, e.g., core-local
        CoreMask supported_cores;  // Cores that support this event clock
    
        /* infra data */
        u64 next_event_time_ns;  // Time for the next event in nanoseconds
        EventClockState state;   // Current state of the clock
    
        /* Driver data */
        void *own_data;  // Pointer to the clock's own data, used for callback
    
        /* callbacks */
        struct callbacks {
            // Callback to set next event time
            u32 (*next_event)(EventClockRegistryEntry *, u64);  
            // Callback to set clock state
            u32 (*set_oneshot)(EventClockRegistryEntry *);
            // Callback to set clock state
            u32 (*set_periodic)(EventClockRegistryEntry *);     
            void (*on_entry)(EventClockRegistryEntry *);        // optional
            void (*on_exit)(EventClockRegistryEntry *);         // optional
        } cbs;
    };
\end{lstlisting}

This structure must define crucial operations for the specific device type alongside data describing its functionality, allowing the kernel to select the most appropriate driver either through user configuration or automatic selection algorithms.

\subsubsection{Core Local Storage}

Implementing an SMP kernel necessitates an infrastructure for accessing core-local data early in the boot process to facilitate future scalability and prevent extensive refactoring. This initialization must occur after memory setup and CPU topology discovery, as structures must be allocated for each core individually. In advanced development stages, numerous per-core structures are required to eliminate locking overhead and enhance performance in critical sections. Furthermore, data strictly correlated with the execution context, such as the currently running thread, must be easily accessible for the core we are runnig on. Consequently, the architecture must provide a mechanism, such as a dedicated register or instruction, to access these unique core-specific areas. On the x86-64 architecture, this is typically implemented using the \texttt{GS} or \texttt{FS} segment registers. Since the full 64-bit base address cannot be loaded into these segment selectors directly, the kernel must utilize Model Specific Registers (specifically \texttt{IA32\_GS\_BASE} or \texttt{IA32\_KERNEL\_GS\_BASE}) to map the per-core structure to the logical address space. The \texttt{SWAPGS}\cite{osdev-gs} instruction is then employed during interrupt entry and exit to switch between user-space and kernel-space thread-local storage transparently.

\begin{lstlisting}[caption={Example Core Local Usage}, label={lst:core-local}]
    void cdecl_UpdateTcbOnSyscallEntry()
    {
        const auto thread = hardware::GetCoreLocalTcb();
        const u64 t       = TimingModule::Get().GetSystemTime().ReadLifeTimeNs();
        thread->user_time_ns += t - thread->timestamp;
        thread->timestamp = t;
        thread->num_syscalls++;
    };
\end{lstlisting}

\subsubsection{Timing Design}

Before discussing timing devices, a fundamental design decision must be made regarding the kernel type: \texttt{tickless} or \texttt{ticking}. Ticking kernels rely on a global constant defining the frequency of periodic timing interrupts. These values typically operate with millisecond granularity, as higher frequencies would degrade system performance. In contrast, tickless kernels do not define periodic interrupts. Instead, they schedule the next timing event based on immediate requirements, such as the sleep schedule or the time slice of the currently running process. It is important to note that ticking kernels also require higher granularity interrupts if support for \texttt{nanosleep} is needed \ref{subsec:linux-timing}.

\subsubsection{Clocks}

The first category of devices required is a simple clock capable of tracking the time elapsed since system startup, primarily for gathering statistics and maintaining timekeeping. These are essential for placing tasks on a timeline and scheduling interrupts for high-precision sleep operations. Without such capabilities, waking tasks would require iterating over all sleeping entities to decrement remaining time, resulting in $O(n)$ complexity. Since interrupt handlers must execute rapidly, tasks should be managed in a high-performance timeline based priority queue, such as a Red-Black tree, which guarantees stability and $O(\log n)$ removal time. On AMD64, the TSC clock \cite{IntelManual-TSC} is suitable for measuring system uptime (sufficient for the scheduler), while the RTC is used for wall-clock time displayed to the user. If multiple timers are available, the kernel should select the one offering the best performance-accuracy ratio.

\subsubsection{Event Clocks}

At this stage, the approach to timing event logic must be established, depending on the target architecture and the capabilities exposed to users. 

The first consideration is sleep granularity. A standard precision of approximately one millisecond is achievable on nearly every architecture using periodic interrupts. However, implementing high-precision nanosecond or microsecond sleep requires timer devices capable of fast, reprogrammable one-shot interrupts (e.g., the LAPIC Timer on AMD64), as handling periodic interrupts at microsecond intervals is technically infeasible.

Another critical factor is the support for multiple cores. In an SMP environment, it is preferable to utilize timing devices that reside locally on the cores and operate independently, thereby maximizing performance. While most modern architectures provide such devices (e.g., LAPIC Timer on AMD64), some older or embedded SMP platforms may lack them. In such cases, the kernel must utilize a global timer and employ interrupt-driven inter-core communication to simulate local interrupts.

Consequently, a suitable timing device must be selected. When supporting multiple architectures, the kernel typically implements drivers for all available options and dynamically selects the appropriate one based on device capabilities.

An example of device availability schemes by architecture is as follows:
\begin{itemize}
\item SMP fast local one-shot timer $\rightarrow$ core local interrupt system + nanosleep mechanism available.
\item SMP local periodic-only timer $\rightarrow$ core local interrupt system + low precision msleep only available.
\item SMP non-local one-shot timer $\rightarrow$ owner core of timer interrupts propagates the interrupt to target cores + nanosleep.
\item SMP non-local periodic-only timer $\rightarrow$ owner core of timer interrupts propagates the interrupt to target cores + low precision msleep only available.
\end{itemize}

Most modern architectures support both local and high-precision timers. For example, on AMD64 with SMP, the presence of local LAPIC Timers \cite{IntelManual-APIC} (high precision, one-shot) is guaranteed, significantly simplifying the implementation.

\subsubsection{Keyboard}

Lastly we need some way to provide input to the kernel on the real machine not only emulated environment. For that we will need some keyboard handling. First easier way is to utilize some simple ports like PS-2 \cite{osdev-ps21}\cite{osdev-ps22} if are available on our device. Otherwise we will probably have to implement some usb drivers what will be much more difficult \cite{usb20-spec}\cite{osdev-usb}.

\subsection{File Systems}
TODO: ADAM

\subsection{Screen Drawing}
TODO: KRYCZKA

\subsection{Scheduling}

The fundamental unit for resource management is the process. This object represents the execution environment for the threads operating within it. Consequently, it must act as a container for all resources shared between these threads. At a minimum, a process structure must manage:

\begin{itemize}
    \item \textbf{Page Tables} -- Architecture-specific structures describing the mapping between virtual and physical addresses.
    \item \textbf{Mapping Metadata} -- Structures enabling the tracking and allocation of virtual address ranges.
    \item \textbf{File Descriptor Table} -- Descriptors facilitating access to files and I/O streams.
\end{itemize}

While additional resources may be required to implement features such as Inter-Process Communication (IPC), pipes, or synchronization primitives, the three elements listed above constitute the bare minimum for the process concept. A straightforward implementation involves defining separate structures for \texttt{Thread} and \texttt{Process} objects, although some designs merge these into a single task structure (as discussed in Section \ref{subsec:linux-task}).
At this stage, it is prudent to prepare data structures that allow for efficient querying of processes, facilitating operations such as termination, joining, or IPC. It is important to note that the creation and destruction of processes are computationally expensive operations. They require building page tables, mapping the kernel address space, and allocating memory for bookkeeping structures. To mitigate latency, some of this workload, particularly during destruction, can be deferred to dedicated kernel worker threads, allowing the system to return control to the caller more rapidly.

\noindent The minimal stuct may look like this:
\begin{lstlisting}[caption={Process Structure}, label={lst:processStruct}]
    struct Process {
        /* Process resources */
        data_structures::DoubleLinkedList<VMemArea *> area_list;
        PPtr<void> page_table_root;
        Mem::VPtr<Fs::FdTable> fd_table;
    };
\end{lstlisting}

In a production environment, this structure would naturally include additional fields for management purposes, such as synchronization primitives, statistics, configuration flags, and a state enumeration for lifecycle management.

\subsubsection{Thread}

The second primary component of the scheduling subsystem is the thread. The thread represents the fundamental unit of execution - the actual code consuming CPU time and utilizing resources from its parent process. To effectively restore, start, and switch execution, the system must track the thread's state, which includes CPU registers, the instruction pointer, and architecture-specific flags. All of this state information can be compressed into a single field: the thread's stack pointer.

This approach simplifies the architecture significantly and enhances performance. Since the stack is heavily utilized, it is typically cached, eliminating the need to allocate separate memory regions for saving context. Consequently, all context-switching operations are unified to use the stack as the storage medium for the thread's state. When saving context, registers are pushed onto the stack. When initializing a thread, the stack is artificially populated with boot-up values.

\noindent A concise initial structure can be defined as follows:
\begin{lstlisting}[caption={Thread Structure First Iteration}, label={lst:os-tutorial-thread-v0}]
struct Thread {
    Process* owner;

    void *user_stack;
    void *user_stack_bottom;
};
\end{lstlisting}

To manage the thread lifecycle, it is necessary to introduce a state enumeration. This field allows the scheduler to determine the appropriate action for a given thread. For instance, a waiting thread must be removed from the run queue, while a blocked thread must be removed from the specific wait queue it is blocked on.

\noindent The structure is updated to reflect these requirements:
\begin{lstlisting}[caption={Thread Structure Second Iteration}, label={lst:os-tutorial-thread-v1}]
enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kBlocked,
    kTerminated,
};

struct Thread {
    Process* owner;
    ThreadState state;

    void *user_stack;
    void *user_stack_bottom;
};
\end{lstlisting}

A critical design consideration regarding context switching is the location where the state is saved. Saving the execution state directly onto the user stack is inherently insecure. A malicious program could manipulate the stack pointer to overwrite critical kernel structures, or the kernel could unintentionally leak sensitive data (such as cryptographic keys) onto the user stack during a switch. Furthermore, to allow kernel code to be preempted or to block (i.e., perform a context switch while inside the kernel), the kernel state must be preserved. This necessitates a separate, protected kernel stack for each thread. Relying on a single kernel stack per core is insufficient for a preemptible kernel.

\noindent The final minimal structure is presented below:
\begin{lstlisting}[caption={Thread Structure Third Iteration}, label={lst:os-tutorial-thread-v2}]
enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kBlocked,
    kTerminated,
};

struct Thread {
    Process* owner;
    ThreadState state;

    void *kernel_stack;
    void *kernel_stack_bottom;
    void *user_stack;
    void *user_stack_bottom;
};
\end{lstlisting}

\subsubsection{Context Creation}
\label{subsec:context-init}

With the context storage mechanism defined, the next step is context initialization. This requires a function responsible for fabricating a context frame on the thread's stack, adhering to a predefined schema. This schema must mirror the layout of registers and CPU flags expected by the context restoration logic. To create a new context, the stack is populated with initial values: zeros for general-purpose registers (or specific function arguments, depending on the ABI), the address of the entry point function, and architecture-specific default flags.

\subsubsection{Context Conversion}

Before a context switch can occur, an initial context must exist. This implies the creation of the first process and the preparation of all associated structures described previously, utilizing the context initialization logic \ref{subsec:context-init}. Several approaches exist to achieve this; for example, the kernel may spawn an initial user-space program (init) responsible for booting other programs, or it may create a kernel-space process to perform further system initialization.

\subsubsection{Context Switch}

Once multiple tasks are established, the system can perform context switching. Assuming the entire execution state resides on the stack, the context switch operation is relatively straightforward. The sequence of operations is as follows:

\begin{itemize}
    \item Push the current thread's context (registers and flags) onto its stack.
    \item Perform any necessary bookkeeping or state updates.
    \item Switch the stack pointer to the target thread's stack.
    \item Restore the state (registers and flags) from the new stack.
\end{itemize}

\noindent Based on AMD64 assembly, the implementation resembles the following:
\begin{lstlisting}[style=nasmstyle, caption={Assembly ISR wrapper}, label={lst:isr_asm}]
; c_decl
; void ContextSwitch(Thread* thread)
;   RDI = thread
ContextSwitch:
    sub rsp, _context_switch_stack_space_ext     ; Allocate space for saving registers.
    push_all_regs                                ; Save the state

    ; ... Some other booking, state changes -> depends on supported features
.done:
    mov rsp, [rdi+Thread.kernel_stack]   ; Change the stack

    pop_all_regs            ; Restore registers of NEW thread's stack
    add rsp, _all_reg_size  ; Deallocate register save space.
    iretq                   ; Load next thread's RIP from its stack
\end{lstlisting}

\subsubsection{Scheduler}
TODO: JAKUB

\subsubsection{Sleep}
TODO: JAKUB

\subsubsection{Blocking - Wait Queues}
TODO: JAKUB

\subsection{User Space}
TODO: JAKUB

\subsubsection{Syscalls}
TODO: ADAM
\subsubsection{Libc System Headers}
TODO: ADAM
\subsubsection{Conversion to User Space}
TODO: JAKUB

\section{Host Environment -- Working OS}

\subsection{Rootfs}
TODO: ADAM
\subsection{Compiling Userspace Programs}
TODO: ADAM

% ==================================================================

\chapter{Analysis of Existing Solutions}

\section{Linux}

The Linux operating system family is one of the most widely used globally, alongside Windows. The Linux kernel relies on a monolithic architecture. This implies that the kernel has a broad range of responsibilities, while user space contains applications that rely entirely on system capabilities. In practice, all code responsible for memory management, scheduling, interrupt handling, and device drivers resides within the kernel address space. This approach presents various advantages and disadvantages.

\noindent Advantages:
\begin{itemize}
    \item \textbf{Zero-copy and memory efficiency} -- As all components reside in a single address space, data does not need to be copied between distinct address spaces. Passing a pointer is sufficient to transfer data between modules.
    \item \textbf{Minimized context switching} -- Within the single kernel address space, invoking functions from different modules is a direct function call, avoiding the overhead of context switching required by microkernels to access driver services.
    \item \textbf{Simplified source management} -- A single binary structure allows for a unified build scheme. Developing numerous separately compiled services can easily lead to inconsistencies in behavior or data models.
    \item \textbf{Ease of implementation} -- This architecture is often easier to implement for small to mid-sized projects.
\end{itemize}

\noindent Disadvantages:
\begin{itemize}
    \item \textbf{Stability} -- A bug in a single driver, even for an unused device, can compromise the stability of the entire system.
    \item \textbf{Security} -- Vulnerabilities in drivers can potentially expose the kernel address space to malicious user-space execution.
    \item \textbf{Scalability challenges} -- For large-scale projects, strict standards are necessary. Without them, the code base can easily become unmaintainable due to complex webs of references and dependencies.
\end{itemize}

To enhance extensibility and avoid the need for recompilation when adding drivers, Linux employs Loadable Kernel Modules (LKMs) \cite{linux_lkm}. These allow the kernel functionality to be extended dynamically via well-defined interfaces during runtime.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tikzpicture}
            \tikzstyle{every node}=[font=\large]
            \draw [fill={rgb,255:red,242; green,242; blue,242}, thick] (0,0) rectangle (14, 8);
            \node [anchor=north west, font=\bfseries\Large] at (0.3, 7.7) {Kernel Address Space};
            \draw [fill=white, thick, rounded corners=3.0] (0.5, 4.5) rectangle (13.5, 6.5);
            \node [font=\Large, align=center] at (7, 5.5) {Base Linux Kernel Components};
            \tikzstyle{mod}=[draw, fill=white, thick, rounded corners=3.0, minimum width=2cm, minimum height=1.3cm, font=\small]
    
            \node[mod] at (1.5, 3.2) {vfat};
            \node[mod] at (3.7, 3.2) {fat};
            \node[mod] at (5.9, 3.2) {realtek};
            \node[mod] at (8.1, 3.2) {pcspkr};
            \node[mod] at (10.3, 3.2) {kvm-intel};
            \node[mod] at (12.5, 3.2) {kvm};
    
            \node[mod] at (1.5, 1.5) {nvme};
            \node[mod] at (3.7, 1.5) {nvidia};
            \node[mod] at (5.9, 1.5) {rfkill};
            \node[mod] at (8.1, 1.5) {nf\_tables};
            \node[mod] at (10.3, 1.5) {snd};
            \node[mod] at (12.5, 1.5) {\Huge \dots};
    
        \end{tikzpicture}
    }
    \caption{Linux Monolithic Architecture with Loadable Kernel Modules}
    \label{fig:linux_lkm}
\end{figure}

\subsection{Timing}
\label{subsec:linux-timing}

Historically, Linux utilized a strictly periodic tick (defined by hardware), configuring hardware timers to generate interrupts at a fixed frequency (e.g., 100 Hz or 1000 Hz). While this design was straightforward, it imposed limitations on sleep granularity and power efficiency. To address these issues, Linux introduced High Resolution Timers (\texttt{hrtimers}), which allow the kernel to schedule interrupts with nanosecond precision based on immediate needs rather than a fixed cadence. Despite the capabilities of high-resolution timers, the concept of a periodic tick is maintained within the kernel for performance reasons and architectural legacy, by simulating the tick with high precision framework.

This hight precision framework has one major drawback - it can get slow for big number of events as it based on priority queue with $O(\log n)$complexities. Relying exclusively on high-precision mechanisms for every timing event would introduce unacceptable overhead in scenarios involving a massive number of active timers. To mitigate this, Linux maintains the \texttt{jiffies} counter, which increments at the frequency of the system tick. This counter drives the Timer Wheel mechanism \cite{linux_timer_wheel}, a highly efficient algorithm designed for managing low-precision timeouts. The timer wheel offers $O(1)$ complexity for insertion and expiration, making it ideal for subsystems that require the management of thousands or even hundreds of thousands of concurrent events where nanosecond precision is unnecessary. A prime example is the networking stack, which must track timeouts for tens of thousands of open TCP connections. Utilizing high-resolution timers for such a volume would be computationally impossible. The \texttt{jiffies}-based timer wheel allows the system to handle these massive quantities of events with minimal CPU overhead. Consequently, standard API calls such as \texttt{msleep} continue to rely on this coarse-grained, high-performance infrastructure.

\subsection{The Process and Thread Model}
\label{subsec:linux-task}

Unlike many other operating systems that distinguish strictly between processes (containers of resources) and threads (units of execution), Linux treats them almost identically. The core data structure is the \texttt{task\_struct} \cite{linux_task}.
\begin{itemize}
    \item A \textbf{Process} is a \texttt{task\_struct} with a unique memory map and file descriptor table.
    \item A \textbf{Thread} is simply a \texttt{task\_struct} created via the \texttt{clone()} system call with flags such as \texttt{CLONE\_VM} and \texttt{CLONE\_FILES}, causing it to share the address space and resources with its parent.
\end{itemize}

\subsection{Scheduler}

Linux implements several scheduling policies \cite{linux_policies}\cite{linux_cfs} operating on task priorities. There are two major classes: \textbf{Real Time}, which operates on priorities 1-99, and \textbf{Fair}, which operates on priority 0. Tasks with higher priorities preempt those with lower priorities. Linux provides six policy classes:

\begin{itemize}
    \item \texttt{SCHED\_DEADLINE} -- Takes precedence over any other policy and provides real-time capabilities.
    \item \texttt{SCHED\_FIFO} -- A real-time policy where a task runs until it yields control or is preempted by a higher-priority task.
    \item \texttt{SCHED\_RR} (Round Robin) -- A real-time policy where each task is assigned a time slice. Upon exhaustion, the task is moved to the end of the queue.
    \item \texttt{SCHED\_OTHER} -- The standard policy for the majority of non-real-time tasks.
    \item \texttt{SCHED\_BATCH} -- Designed for non-interactive tasks that run for longer periods without context switches, tolerating longer scheduling latencies.
    \item \texttt{SCHED\_IDLE} -- Used for very low priority tasks that run only when the system is otherwise idle.
\end{itemize}

From version 2.6.23 \cite{linux_cfs} up to 6.6 \cite{linux_eevdf}, the \texttt{CFS} (Completely Fair Scheduler) was the default scheduler for the \textbf{Fair} class. Starting with version 6.6, the \texttt{EEVDF} (Earliest Eligible Virtual Deadline First) scheduler began replacing CFS.

\subsubsection{Completely Fair Scheduler}
The Linux documentation summarizes the design of the \textbf{CFS} as follows:

\begin{quote}
80\% of CFS's design can be summed up in a single sentence: CFS basically models an “ideal, precise multi-tasking CPU” on real hardware.

“Ideal multi-tasking CPU” is a (non-existent :-)) CPU that has 100\% physical power and which can run each task at precise equal speed, in parallel, each at 1/nr\_running speed. For example: if there are 2 tasks running, then it runs each at 50\% physical power - i.e., actually in parallel.
    
On real hardware, we can run only a single task at once, so we have to introduce the concept of “virtual runtime.” The virtual runtime of a task specifies when its next timeslice would start execution on the ideal multi-tasking CPU described above. In practice, the virtual runtime of a task is its actual runtime normalized to the total number of running tasks.
\cite{linux_cfs}
\end{quote}

The implementation uses a red-black tree sorted by virtual runtime, representing the execution timeline. The next task selected is the leftmost node in the tree (the task with the least spent execution time) and runs until next another taks becomes the leftmost + some granularity to prevent over-scheduling.

\subsubsection{Earliest Eligible Virtual Deadline First Scheduler}

This algorithm is becoming the new standard for Linux scheduling, addressing shortcomings of the previous \textbf{CFS} implementation. The new approach functions similarly to CFS but operates on deadlines rather than accumulated runtime \cite{linux_eevdf}. This modification allows for better prioritization of latency-sensitive tasks.

\subsection{Memory Management}
\label{subsec:linux_mem}

Linux features centralized memory management with various APIs, including \texttt{kmalloc}, \texttt{kzalloc}, \texttt{vmalloc}, and \texttt{kvmalloc} \cite{linux_mem_interfaces}.

\subsubsection{Physical Memory}
As an architecture-independent kernel, Linux abstracts hardware details. Physical memory is partitioned into zones, each serving a distinct purpose \cite{linux_mem_phys}:

\begin{itemize}
    \item \texttt{ZONE\_DMA} -- Memory suitable for DMA (Direct Memory Access) by devices that cannot access the full addressable range.
    \item \texttt{ZONE\_NORMAL} -- Standard memory directly accessible by the kernel.
    \item \texttt{ZONE\_MOVABLE} -- Similar to \texttt{ZONE\_NORMAL}, but the content of pages in this zone can be migrated to different physical frames (preserving virtual address).
    \item \texttt{ZONE\_DEVICE} -- Memory reserved for specific hardware, such as GPUs, mapped to device drivers.
    \item \texttt{ZONE\_HIGHMEM} -- Memory not covered by permanent kernel mappings, used only by some 32-bit architectures.
\end{itemize}

To enhance multi-core performance, Linux employs a two-step strategy \cite{linux_mem_phys}. It utilizes a global buddy allocator \cite{buddy_allocator} alongside Per-CPU Pagesets (PCP). Allocations are first attempted from the local PCP and if that fails, the system falls back to the global allocator with full synchronization.

\subsubsection{Virtual Memory}

Each process possesses its own Page Table, responsible for mapping virtual addresses to physical ones. Physical frames are acquired from the Physical Memory Manager \ref{subsec:linux_mem}. Virtual address space management for each process utilizes red-black trees to efficiently locate free areas. The kernel address space is mapped into every process. Additionally, Linux implements demand paging, a lazy allocation approach where physical pages are assigned only when the process actually accesses the memory, utilizing the page-fault mechanism.

\subsection{System Interface}

Linux is a Unix-like operating system kernel. The primary interface for system communication is provided via system calls, wrapped by the GNU C Library (glibc).

\subsubsection{Standards}

Linux adheres to the \textbf{POSIX} standard, implementing functions such as \texttt{fork()} and \texttt{exec()}, as well as the concept of \texttt{pthread}.

\subsubsection{Fork Mechanism}

The \texttt{fork()} mechanism is rooted in historical design decisions. While copying the entire parent process state appears inefficient and can introduce performance issues, Linux implements various optimizations, such as Copy-On-Write (\texttt{COW}), to mitigate overhead. This is particularly relevant given that \texttt{exec()} is frequently called immediately after \texttt{fork()}, discarding the copied state. The advantages and disadvantages of this mechanism include:

\noindent Advantages:
\begin{itemize}
    \item Simplified implementation of pipes and pipelining.
    \item Automatic state propagation, eliminating the need to manually specify files or streams in function call.
\end{itemize}

\noindent Disadvantages:
\begin{itemize}
    \item Complex implementation requiring multiple optimizations to remain efficient.
    \item Requires the duplication of process structures, which is inherently complex.
    \item Potential performance bottlenecks in edge cases, such as with large page tables.
    \item Requires careful resource management by the programmer to release resources before execution.
    \item Conceptually difficult for beginners to grasp.
    \item Not portable to architectures lacking an MMU.
\end{itemize}

\section{Minix 3}

Minix 3 is a microkernel-based operating system designed with a strong emphasis on high reliability. Unlike monolithic kernel architectures, Minix 3 relocates the majority of operating system components - including device drivers, file systems, memory management mechanisms, and scheduling services - into user-space processes. As a result, the kernel's responsibilities are strictly minimized to functions such as message passing, interrupt handling, and low-level scheduling. Currently, the system supports two hardware architectures: x86 and ARM.

This architectural approach provides several notable advantages. By isolating critical system services in user space, Minix 3 significantly improves fault tolerance, as failures in individual components do not lead to the compromise of the entire system. Moreover, these components can be restarted independently, enabling efficient recovery from faults instead of system-wide crashes. The resulting modularity also simplifies system maintenance and evolution, since services may be updated, replaced, or extended without requiring recompilation of the kernel.

\begin{figure}[htbp]
  \centering
  \resizebox{1\textwidth}{!}{%
    \begin{tikzpicture}
    \tikzstyle{every node}=[font=\large]

    % - Background Layers -
    % Kernel Mode Background
    \draw [fill={rgb,255:red,242; green,242; blue,242}] (2.5,10.75) rectangle (17.25,8.25);
    % User Mode Background
    \draw [fill={rgb,255:red,242; green,242; blue,242}] (2.5,18.5) rectangle (17.25,11);
    
    % Horizontal Separators
    \draw (2.5,16) -- (17.25,16);
    \draw (2.5,13.5) -- (17.25,13.5);

    % - User Processes Layer (Layer 4) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,18) rectangle node {\large User Processes} (7,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,18) rectangle node {\large Shell} (9.5,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,18) rectangle node {\large Make} (12,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,18) rectangle node {\large User} (14.5,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,18) rectangle node {\LARGE ...} (17,16.5);

    % - Server Processes Layer (Layer 3) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,15.5) rectangle node {\large Server Processes} (7,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,15.5) rectangle node {\large File} (9.5,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,15.5) rectangle node {\large PM} (12,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,15.5) rectangle node {\large Sched} (14.5,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,15.5) rectangle node {\LARGE ...} (17,14);

    % - Device Driver Layer (Layer 2) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,13) rectangle node {\large Device Processes} (7,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,13) rectangle node {\large Disk} (9.5,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,13) rectangle node {\large TTY} (12,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,13) rectangle node {\large Net} (14.5,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,13) rectangle node {\LARGE ...} (17,11.5);

    % - Kernel Layer (Layer 1) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,10.25) rectangle node {\large Kernel} (7.25,8.75);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.75,10.25) rectangle node {\large Clock Task} (12,8.75);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.5,10.25) rectangle node {\large System Task} (17,8.75);
    
    % - Labels and Brackets -
    % User Mode bracket
    \draw [thick] (2.0,11) -- (2.0,18.5);
    \draw [thick] (2.0,11) -- (2.2,11);
    \draw [thick] (2.0,18.5) -- (2.2,18.5);
    \node[left, align=center] at (1.8,14.75) {\large \textbf{User}\\\large \textbf{Mode}};
    
    % Kernel Mode bracket
    \draw [thick] (2.0,8.25) -- (2.0,10.75);
    \draw [thick] (2.0,8.25) -- (2.2,8.25);
    \draw [thick] (2.0,10.75) -- (2.2,10.75);
    \node[left, align=center] at (1.8,9.5) {\large \textbf{Kernel}\\\large \textbf{Mode}};
    
    % - Right Side Layer Labels -
    \node[right] at (17.5, 17.25) {\large \textbf{Layer 4}};
    \node[right] at (17.5, 14.75) {\large \textbf{Layer 3}};
    \node[right] at (17.5, 12.25) {\large \textbf{Layer 2}};
    \node[right] at (17.5, 9.5)   {\large \textbf{Layer 1}};
    
    \end{tikzpicture}
  }%
\caption{The Minix 3 Microkernel Architecture}
\label{fig:minix_architecture}
\end{figure}

\subsection{Scheduler}
Adhering to the microkernel philosophy, Minix 3 separates scheduling policy from the low-level context switching mechanism. While the kernel remains responsible for the mechanics of context switching and interrupt handling, scheduling decisions are delegated to a dedicated user-space server, referred to as the \texttt{Sched} server.

The scheduler employs a multi-level priority round-robin algorithm \cite{minix_sched}. The system maintains sixteen priority queues organized hierarchically. Kernel tasks occupy the highest priority levels, followed by device drivers, system servers, and user applications. The lowest priority queue is reserved exclusively for idle tasks. At any scheduling decision point, the scheduler selects the next runnable process from the highest non-empty priority queue.

Every process is assigned a specific time quantum, representing the maximum CPU time allowed before preemption occurs. Upon the exhaustion of a process's time quantum, the kernel notifies the scheduler server. To penalize CPU-bound tasks and maintain system responsiveness, the scheduler lowers the priority of such processes, moving them to a lower queue.

To prevent indefinite starvation of low-priority processes, a \texttt{balance\_queues} function executes periodically at five-second intervals \cite{minix_sched_bq}. This function re-evaluates process states and promotes tasks that have not received CPU time for an extended period, thereby ensuring that interactive applications remain responsive.

\subsection{Timing}
The timing subsystem in Minix 3 is responsible for maintaining system time, coordinating scheduling intervals, and handling alarms and timers. The core timing functionality is encapsulated within the Clock Task, a kernel-level process that manages time-related operations.

\subsubsection{Hardware Abstraction}
Minix 3 provides an abstraction layer over platform-specific timing hardware. On x86 systems, this includes the legacy Programmable Interval Timer (PIT) \cite{osdev-pit}, while ARM-based platforms rely on board-specific timer implementations \cite{minix_arch_clock}. During system initialization, the kernel configures the selected hardware timer to generate periodic interrupts. If not specified in the kernel environment, the default interrupt frequency is architecture-dependent: 60 Hz on x86 and 1000 Hz on ARM systems.

\subsubsection{Timers and Alarms}
The kernel maintains a \texttt{clock\_timers} queue to manage synchronous timers and alarms \cite{minix_kernel_clock}. Upon receipt of a hardware timer interrupt, the \texttt{timer\_int\_handler} routine is executed. If the expiration time of the next scheduled timer in the \texttt{clock\_timers} queue has been reached, the kernel executes the associated callback function. 

In the case of system alarms, this callback is the \texttt{cause\_alarm} function \cite{minix_do_setalarm}. This function sends a notification message from the Clock Task to the requesting process (e.g., the Process Manager or Scheduler), informing it that the requested time interval has elapsed. This mechanism allows user-space servers to perform periodic tasks, such as the scheduler's queue balancing.

\subsubsection{Real-Time Clock (RTC)}
Persistent wall-clock time is provided by a dedicated user-space driver, \texttt{readclock}. This driver interfaces directly with the CMOS Real-Time Clock (RTC) to retrieve the current date and time. During system initialization, this information is communicated to the Process Manager to establish the system's initial time reference \cite{minix_readclock}.

\subsection{Memory Management}
Memory management in Minix 3 is primarily handled by the user-space Virtual Memory (VM) server. The kernel retains control over the hardware Memory Management Unit (MMU) and address space switching but delegates higher-level memory policies to the VM server.

\subsubsection{VM Server}
The VM server is responsible for memory region allocation, page table administration, and page fault handling. It implements a region-based memory model in which a process address space is divided into contiguous regions with defined access permissions (read, write, and execute). To efficiently manage free and allocated memory regions, the VM server uses \textbf{AVL trees} \cite{minix_vm}.

\subsubsection{Allocation and Paging}
Internal memory allocation for kernel data structures and VM metadata is performed using a slab allocator \cite{slab_allocation}. Minix 3 supports demand paging: when a process attempts to access an unmapped virtual address, a page fault is raised. The kernel intercepts this exception and forwards the fault information to the VM server, which resolves it by mapping an appropriate physical frame or retrieving the required page from secondary storage \cite{shenoy_lecture}.

\subsubsection{Memory Grants}
Due to the strict isolation of address spaces in the microkernel architecture, processes cannot directly access the memory of others. To facilitate the exchange of large data structures without violating protection boundaries, Minix 3 provides a capability-based mechanism known as \texttt{Memory Grants}, accessed via the \texttt{safecopy} API \cite{minix_safecopy}.

A process (the grantor) dynamically generates a grant capability that explicitly permits a specific peer process (the grantee) to read from or write to a designated memory range. The grantee utilizes this grant ID to request a data transfer from the System Task. The kernel validates the grant permissions before performing the copy operation between the disjoint address spaces. This mechanism ensures that drivers and servers can operate on client buffers while preventing unauthorized access to arbitrary memory locations.

\subsection{System Interface}
Although Minix 3 conforms to the POSIX standard, its underlying system interface differs substantially from monolithic operating systems due to its message-based microkernel architecture.

\subsubsection{IPC Primitives}
Inter-process communication (IPC) is built upon four fundamental primitives provided by the kernel. For each operation, the kernel mediates communication by copying message contents directly from the sender's address space to that of the receiver.
\begin{itemize}
    \item \texttt{send(dest, \&message)}: Sends a message to the specified destination. The sender blocks if the destination is not ready to receive.
    \item \texttt{receive(source, \&message)}: Blocks the calling process until a message is received from the specified source.
    \item \texttt{sendrec(src\_dest, \&message)}: A combined operation that sends a message and then blocks until a reply is received from the same process.
    \item \texttt{notify(dest)}: A non-blocking signaling mechanism used to inform a destination of an event without transmitting a data payload.
\end{itemize}

\subsubsection{Communication Policies}
IPC in Minix 3 is governed by these strict policies:
\begin{enumerate}
    \item \textbf{Access Control:} Processes may exchange messages only with explicitly authorized peers.
    \item \textbf{Hierarchical Communication:} Message flow generally follows the system's layered architecture (Layer 4 $\rightarrow$ Layer 3 $\rightarrow$ Layer 2).
    \item \textbf{User-Space Isolation:} User processes are prohibited from communicating directly with one another or with the kernel.
\end{enumerate}

\subsubsection{System Call Implementation}
In Minix 3, system calls are not implemented as direct kernel invocations. Instead, standard library functions (e.g., \texttt{read}, \texttt{fork}) serve as wrappers that construct a message containing the call arguments and transmit it to the appropriate server using the \texttt{sendrec} primitive \cite{minix_ipc}. If the server needs to perform a privileged operation, it forwards the request to the System Task on behalf of the calling process, as servers possess higher privileges than user processes.

\subsubsection{Server Delegation}
System calls are dispatched to specialized servers according to their functionality \cite{minix_callnr}:
\begin{itemize}
    \item \textbf{Process Manager (PM):} Responsible for process lifecycle operations such as \texttt{fork}, \texttt{exec}, \texttt{exit}, and \texttt{wait}.
    \item \textbf{Virtual File System (VFS):} Handles file-related operations including \texttt{open}, \texttt{read}, \texttt{write}, and \texttt{stat}.
\end{itemize}
The kernel acts solely as a transport mechanism and remains agnostic to the semantics of the system call. After processing a request, the server returns a reply message to the calling process, thereby unblocking it and delivering the result.


% ==================================================================

\chapter{Supporting Multiple Hardware Platforms} 
TODO KTOKOLWIEK

\section{HAL -- Hardware Abstraction Layer} 
TODO KTOKOLWIEK

\section{x86-64 Support} 
TODO KTOKOLWIEK
\subsection{Technical Debt}
TODO KTOKOLWIEK
\subsection{Common Pitfalls}
TODO KTOKOLWIEK


% ==================================================================

\chapter{Our Implementation} 
TODO KTOKOLWIEK

\section{Bootloader}

Before the architecture-agnostic kernel can commence execution, the underlying hardware must be brought to a known, deterministic state. As discussed in Section \ref{subsec:theory_bootloader}, the initialization protocols differ significantly depending on the specific hardware architecture and the firmware interface.

For the AlkOS kernel on the x86-64 architecture, we rely on the \textbf{Multiboot2} specification \cite{Multiboot2-Spec}. This allows us to utilize a battle-tested external bootloader, such as GRUB, to handle the intricacies of storage drivers, filesystem parsing, and memory map retrieval. GRUB loads our kernel binary into memory and transfers control to our entry point.

However, the state provided by Multiboot2 is insufficient for the immediate execution of our C++ kernel. Upon entry, the system is in \textbf{32-bit Protected Mode}, paging is disabled, interrupts are disabled, and the stack pointer is undefined. Furthermore, the kernel is loaded at a low physical address, whereas AlkOS is designed as a \textbf{higher-half kernel}: its code and data are linked to virtual addresses in the upper portion of the 64-bit address space (\texttt{0xFFFFFFFE00000000}).

To bridge this gap, we implemented a two-stage internal bootloader: \textbf{Loader32} and \textbf{Loader64}. Each stage exists as a separate binary with distinct compilation requirements, as illustrated in Figure \ref{fig:boot_flow}.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=1.8cm,
        stage/.style={draw, fill=gray!15, rounded corners=4pt, minimum width=3.2cm, minimum height=1.2cm, font=\small\bfseries, align=center},
        arrow/.style={-Latex, thick},
        note/.style={font=\scriptsize, align=center, text width=3cm}
    ]
    
    % Stages
    \node[stage] (grub) {GRUB\\Bootloader};
    \node[stage, right=of grub] (loader32) {Loader32\\(32-bit ELF)};
    \node[stage, right=of loader32] (loader64) {Loader64\\(64-bit PIE)};
    \node[stage, right=of loader64] (kernel) {AlkOS Kernel\\(Higher-Half)};
    
    % Arrows with labels
    \draw[arrow] (grub) -- (loader32) node[midway, above, font=\scriptsize] {Protected Mode};
    \draw[arrow] (loader32) -- (loader64) node[midway, above, font=\scriptsize] {Long Mode};
    \draw[arrow] (loader64) -- (kernel) node[midway, above, font=\scriptsize] {Virtual Memory};
    
    % Notes below
    \node[note, below=0.6cm of grub] {Multiboot2\\entry point};
    \node[note, below=0.6cm of loader32] {32-bit instructions\\Enable Long Mode};
    \node[note, below=0.6cm of loader64] {Position-independent\\Load kernel to higher-half};
    \node[note, below=0.6cm of kernel] {Linked at\\0xFFFFFFFE00000000};
    
    \end{tikzpicture}
    }
    \caption{AlkOS Boot Flow: Three-Stage Transition from GRUB to Higher-Half Kernel}
    \label{fig:boot_flow}
\end{figure}

\subsection{Why Two Internal Loaders?}

The separation into Loader32 and Loader64 is not arbitrary, it stems from fundamental constraints of the x86-64 architecture and our kernel design:

\begin{enumerate}
    \item \textbf{Loader32 contains 32-bit instructions}: The transition from Protected Mode to Long Mode requires executing 32-bit code. Instructions such as setting control register bits, loading the GDT, and performing the mode switch \textit{cannot} be encoded in 64-bit instruction format. Thus, Loader32 is compiled as a 32-bit ELF binary (\texttt{elf32-i386}).
    
    \item \textbf{Loader64 is Position-Independent Code (PIC)}: The kernel is linked at a fixed higher-half virtual address (\texttt{0xFFFFFFFE00000000}). However, Loader64 must execute \textit{before} virtual memory is fully configured. GRUB loads Loader64 elf as a Multiboot2 module. This elf must still be loaded in the first place in memory that is big enought to hold it, meaning at an \textit{unknown physical address}. To function correctly regardless of where it is loaded, Loader64 is compiled as a Position-Independent Executable (PIE) using \texttt{-fPIE -pie} flags, employing RIP-relative addressing for all memory accesses.
    
    \item \textbf{The kernel ignores its physical location}: Once virtual memory is established with the higher-half mapping, the kernel code executes using virtual addresses. It is unaware of, and unaffected by, its underlying physical memory location.
\end{enumerate}

\subsection{Loader32: 32-bit to 64-bit Transition}

Loader32 is the true entry point of AlkOS, receiving control directly from GRUB. Its primary responsibility is to transition the CPU from 32-bit Protected Mode to 64-bit Long Mode, enabling the 4-level paging required by the x86-64 architecture \cite{Intel-ControlRegisters}.

\subsubsection{Entry Point}

The Multiboot2 specification defines the machine state upon entry: \texttt{EAX} contains the magic number \texttt{0x36d76289}, and \texttt{EBX} points to the Multiboot information structure. The entry point establishes a stack and delegates to C++ code:

\begin{lstlisting}[style=nasmstyle, caption={Loader32 Entry Point (entry.nasm)}, label={lst:loader32_entry}]
bits 32
; GRUB leaves us in 32-bit Protected Mode
; EAX = Magic number, EBX = Multiboot info pointer

section .text
global Entry
Entry:
    mov esp, stack_top      ; Establish stack
    mov ebp, esp
    and esp, 0xFFFFFFF0     ; Align to 16 bytes (System V ABI)
    
    push ebx                ; Arg2: Multiboot info pointer
    push eax                ; Arg1: Magic number
    call MainLoader32       ; Delegate to C++ code
\end{lstlisting}

\subsubsection{CPU Feature Detection}

Before attempting the Long Mode transition, Loader32 verifies that the CPU supports the required features. This is accomplished through the CPUID instruction \cite{Intel-CPUID}:

\begin{lstlisting}[style=nasmstyle, caption={Long Mode Detection (checks.nasm)}, label={lst:check_longmode}]
EXTENDED_FUNCTIONS_THRESHOLD equ 0x80000000
LONG_MODE_BIT equ 1 << 29

CheckLongMode:
    ; Check if extended CPUID functions are available
    mov eax, EXTENDED_FUNCTIONS_THRESHOLD
    cpuid
    cmp eax, EXTENDED_FUNCTIONS_THRESHOLD + 1
    jl .no_long_mode
    
    ; Query extended processor features (EAX=80000001h)
    mov eax, EXTENDED_FUNCTIONS_THRESHOLD + 1
    cpuid
    test edx, LONG_MODE_BIT   ; Check EDX bit 29
    jz .no_long_mode
    
    mov eax, 0                ; Success
    ret
.no_long_mode:
    mov eax, 1                ; Failure
    ret
\end{lstlisting}

\subsubsection{Memory Management Initialization}

Before enabling paging, Loader32 initializes a temporary bootstrap Physical Memory Manager (PMM) and Virtual Memory Manager (VMM). The memory map obtained from the Multiboot2 information structure is parsed to identify available physical memory regions. Loader32 then creates an \textbf{identity mapping} of the first 10 gigabytes of physical memory using 1GB pages:

\begin{lstlisting}[caption={Identity Mapping Setup (main.cpp)}, label={lst:loader32_mmap}]
MemoryManagers SetupMemoryManagement(MultibootInfo &multiboot_info) {
    auto mmap_tag = multiboot_info.FindTag<TagMmap>();
    const MemoryMap mmap(*mmap_tag);
    
    // Create Physical Memory Manager from Multiboot memory map
    auto pmm = PhysicalMemoryManager::Create(mmap, lowest_safe_addr, ...);
    auto vmm = new VirtualMemoryManager(*pmm);
    
    // Identity map first 10GB using 1GB page granularity
    vmm.Map<PageSizeTag::k1Gb>(
        0,                                   // Virtual address
        0,                                   // Physical address  
        10 * PageSize<PageSizeTag::k1Gb>(),  // 10GB
        kPresentBit | kWriteBit | kGlobalBit
    );
    
    return {*pmm, *vmm};
}
\end{lstlisting}

The identity mapping ensures that code continues to execute correctly after paging is enabled, the instruction pointer remains valid because virtual addresses 0--10GB map directly to the same physical addresses.

\subsubsection{Long Mode Enablement}

The transition to Long Mode follows the sequence defined in the Intel Software Developer's Manual \cite{Intel-ControlRegisters}:

\begin{lstlisting}[style=nasmstyle, caption={Enabling Long Mode and Paging (enables.nasm)}, label={lst:enable_longmode}]
LONG_MODE_BIT equ 1 << 8
EFER_MSR      equ 0xC0000080
PAGING_BIT    equ 1 << 31
PAE_BIT       equ 1 << 5

EnableLongMode:
    ; Set LME (Long Mode Enable) in EFER MSR
    mov ecx, EFER_MSR
    rdmsr
    or eax, LONG_MODE_BIT
    wrmsr
    ret

EnablePaging:
    ; Enable PAE (Physical Address Extension) in CR4
    mov eax, cr4
    or eax, PAE_BIT
    mov cr4, eax
    
    ; Load PML4 table address into CR3
    mov eax, [ebp + 8]    ; pml4_table parameter
    mov cr3, eax
    
    ; Enable paging in CR0
    mov eax, cr0
    or eax, PAGING_BIT
    mov cr0, eax
    ret
\end{lstlisting}

\subsubsection{Loading Loader64 as an ELF Module}

A critical responsibility of Loader32 is loading the next stage. Loader64 is provided to GRUB as a separate Multiboot2 module. Loader32 locates this module, parses it as an ELF64 executable, allocates memory for its segments, and performs dynamic relocation:

\begin{lstlisting}[caption={Loading Loader64 Module (main.cpp)}, label={lst:load_loader64}]
static u64 LoadNextStageModule(MultibootInfo &multiboot_info, 
                               MemoryManagers mem_managers) {
    auto &pmm = mem_managers.pmm;
    
    // Find the loader64 module by command line string
    auto next_module = multiboot_info.FindTag<TagModule>([](TagModule *tag) {
        return strcmp(tag->cmdline, "loader64") == 0;
    });
    
    // Parse ELF and determine required memory
    auto elf = PhysicalPtr<byte>(next_module->mod_start);
    u64 virt_start, virt_end;
    Elf64::GetProgramBounds(elf.ValuePtr(), virt_start, virt_end);
    
    // Allocate contiguous physical memory for the module
    u64 module_size = virt_end - virt_start;
    auto module_dest = pmm.AllocContiguous32(module_size);
    
    // Load and relocate the PIE executable
    auto entry_point = Elf64::Load(elf.ValuePtr(), module_dest);
    Elf64::Relocate(elf.ValuePtr(), module_dest);
    
    return entry_point;
}
\end{lstlisting}

\subsubsection{64-bit GDT and Mode Transition}

Before jumping to 64-bit code, a 64-bit Global Descriptor Table must be loaded. The GDT defines the code and data segments for Long Mode:

\begin{lstlisting}[style=nasmstyle, caption={64-bit GDT Definition (gdt.nasm)}, label={lst:gdt64}]
section .data
align 16
GDT64:
    .Null: dq 0
    .Code: equ $ - GDT64
        dw 0x0000         ; Limit (unused in 64-bit)
        dw 0x0000         ; Base (unused in 64-bit)
        db 0x00
        db 0x9A           ; Access: Present, Code, Readable
        db 0x20           ; Flags: Long mode
        db 0x00
    .Data: equ $ - GDT64
        dw 0x0000
        dw 0x0000
        db 0x00
        db 0x92           ; Access: Present, Data, Writable
        db 0x00
        db 0x00
.End:
    .Pointer:
        dw .End - GDT64 - 1
        dq GDT64
\end{lstlisting}

The final transition performs a far jump to flush the CPU pipeline and begin executing 64-bit code:

\begin{lstlisting}[style=nasmstyle, caption={Transition to 64-bit Mode (transition\_64.nasm)}, label={lst:transition64}]
    ; Save kernel entry address and loader data pointer
    mov eax, [ebp+8]            ; entry_high
    mov [k_ptr+4], eax
    mov eax, [ebp+12]           ; entry_low  
    mov [k_ptr], eax
    mov eax, [ebp+16]           ; loader_data
    mov [loader_data_ptr], eax
    
    ; Load 64-bit GDT
    lgdt [GDT64.Pointer]
    
    ; Set data segments
    mov ax, GDT64.Data
    mov ss, ax
    mov ds, ax
    mov es, ax
    
    ; Far jump to 64-bit code segment
    jmp GDT64.Code:jmp_elf

[bits 64]
jmp_elf:
    ; Now executing in 64-bit Long Mode
    mov rdi, [loader_data_ptr]  ; Pass transition data
    mov rax, [k_ptr]
    jmp rax                     ; Jump to Loader64
\end{lstlisting}

\subsection{Loader64: Position-Independent Higher-Half Setup}

Loader64 executes in 64-bit Long Mode but faces a unique challenge: it is loaded at an unknown physical address by GRUB, yet must set up virtual memory and load the kernel at a fixed higher-half address.

\subsubsection{Position-Independent Compilation}

Loader64 is compiled as a Position-Independent Executable (PIE). The CMake configuration specifies:

\begin{lstlisting}[language=cmake, caption={Loader64 CMake Configuration}, label={lst:loader64_cmake}]
set_target_properties(alkos.boot.loader.64 PROPERTIES 
    POSITION_INDEPENDENT_CODE ON
)
target_link_options(alkos.boot.loader.64 PRIVATE
    -fPIE -pie
    "LINKER:-z,norelro"
)
\end{lstlisting}

This ensures all code uses RIP-relative addressing. The entry point demonstrates this:

\begin{lstlisting}[style=nasmstyle, caption={Loader64 Entry Point with RIP-Relative Addressing (entry.nasm)}, label={lst:loader64_entry}]
section .text
bits 64
Entry:
    ; RIP-relative addressing for position independence
    lea rsp, [rel stack_top]    ; Load stack using RIP-relative
    mov rbp, rsp
    
    mov r10d, edi               ; Save transition data pointer
    
    ; Load GDT using RIP-relative address
    lea rax, [rel GDT64.Pointer]
    lgdt [rax]
    
    ; Set up data segments
    mov ax, [rel GDT64_DATA_SELECTOR]
    mov ss, ax
    mov ds, ax
    mov es, ax
    
    mov edi, r10d
    call MainLoader64
\end{lstlisting}

\subsubsection{State Restoration from Loader32}

Loader32 passes its PMM and VMM state to Loader64 via a \texttt{TransitionData} structure. Loader64 deserializes this state to continue memory management:

\begin{lstlisting}[caption={TransitionData Deserialization (main.cpp)}, label={lst:transition_data}]
static auto InitializeLoaderEnvironment(const TransitionData *transition_data) {
    // Reconstruct PMM from serialized state
    PhysicalMemoryManager *pmm = 
        new PhysicalMemoryManager(transition_data->pmm_state);
    
    // Reconstruct VMM with existing page tables
    VirtualMemoryManager *vmm = 
        new VirtualMemoryManager(*pmm, transition_data->vmm_state);
    
    // Reconstruct Multiboot info accessor
    MultibootInfo mb_info(transition_data->multiboot_info_addr);
    
    return {MemoryManagers{*pmm, *vmm}, mb_info};
}
\end{lstlisting}

\subsubsection{Kernel Loading at Higher-Half Address}

The kernel ELF is linked to execute at virtual address \texttt{0xFFFFFFFE00000000}. Loader64 allocates virtual memory at this address and loads the kernel segments:

\begin{lstlisting}[caption={Higher-Half Kernel Loading (main.cpp)}, label={lst:kernel_load}]
// Kernel virtual address (upper 33 bits set)
static constexpr u64 kKernelVirtualAddressStart = 0xFFFFFFFE00000000ULL;

static u64 LoadKernelIntoMemory(MultibootInfo &multiboot_info,
                                const KernelModuleInfo &kernel_info,
                                MemoryManagers mem_managers) {
    auto &vmm = mem_managers.vmm;
    
    // Allocate virtual memory at the higher-half address
    vmm.Alloc(kKernelVirtualAddressStart, 
              kernel_info.size,
              kPresentBit | kWriteBit | kGlobalBit);
    
    // Load ELF segments into the allocated virtual memory
    byte *module_start = reinterpret_cast<byte *>(kernel_info.tag->mod_start);
    auto entry_point = Elf64::Load(module_start);
    
    return entry_point;
}
\end{lstlisting}

\subsubsection{Direct Memory Mapping}

To allow the kernel to access physical memory directly, Loader64 creates a \textbf{direct memory mapping}, a large region of virtual address space that maps linearly to physical memory:

\begin{lstlisting}[caption={Direct Memory Mapping Setup (main.cpp)}, label={lst:direct_map}]
static constexpr u64 kDirectMemMapAddrStart = 0xFFFF800000000000ULL;
static constexpr u64 kDirectMemMapSizeGb = 512;

static void EstablishDirectMemMapping(MemoryManagers &mms) {
    auto &vmm = mms.vmm;
    
    // Map 512GB of physical memory to higher-half virtual addresses
    vmm.Map<PageSizeTag::k1Gb>(
        kDirectMemMapAddrStart,                        // Virtual base
        0,                                             // Physical base
        kDirectMemMapSizeGb * PageSize<PageSizeTag::k1Gb>(),
        kPresentBit | kWriteBit | kGlobalBit
    );
}
\end{lstlisting}

This mapping allows the kernel to convert any physical address \texttt{P} to the virtual address \texttt{0xFFFF800000000000 + P}.

\subsubsection{Kernel Arguments Preparation}

Before transferring control to the kernel, Loader64 collects essential system information into a \texttt{KernelArguments} structure:

\begin{lstlisting}[caption={Kernel Arguments Preparation (main.cpp)}, label={lst:kernel_args}]
struct PACK alignas(64) KernelArguments {
    /// Kernel Mem Layout
    u64 kernel_start_addr;
    u64 kernel_end_addr;

    /// VMem
    u64 pml_4_table_phys_addr;

    /// Memory Bitmap
    u64 mem_info_bitmap_addr;
    u64 mem_info_total_pages;

    /// Framebuffer
    u64 fb_addr;
    u32 fb_width;
    u32 fb_height;
    u32 fb_pitch;
    u32 fb_bpp;  // Bits per pixel

    // RGB Format
    u8 fb_red_pos;
    u8 fb_red_mask;
    u8 fb_green_pos;
    u8 fb_green_mask;
    u8 fb_blue_pos;
    u8 fb_blue_mask;

    /// Ramdisk
    u64 ramdisk_start;
    u64 ramdisk_end;

    /// ACPI
    u64 acpi_rsdp_phys_addr;

    /// Multiboot
    u64 multiboot_info_addr;
    u64 multiboot_header_start_addr;
    u64 multiboot_header_end_addr;
};
}
\end{lstlisting}

The final transition to the kernel is a simple jump:

\begin{lstlisting}[style=nasmstyle, caption={Kernel Entry (transition.nasm)}, label={lst:enter_kernel}]
; void EnterKernel(u64 kernel_entry, KernelArguments *args)
;                  [rdi]            [rsi]
EnterKernel:
    xchg rdi, rsi       ; Swap: RDI = args, RSI = entry
    jmp rsi             ; Jump to kernel entry point
\end{lstlisting}

\subsection{Memory Layout}

Figure \ref{fig:memory_layout} illustrates the virtual memory layout established by the bootloader before kernel execution begins.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        memblock/.style={draw, minimum width=8cm, minimum height=0.9cm, font=\small\ttfamily},
        addrblock/.style={font=\tiny\ttfamily, anchor=east},
        scale=0.95
    ]
    
    % Virtual address space
    \node[font=\bfseries] at (2, 6.5) {Virtual Address Space};
    
    % Memory regions
    \node[memblock, fill=red!20] (kernel) at (2, 5.5) {Kernel Code \& Data};
    \node[addrblock] at (-2.3, 5.5) {0xFFFFFFFE00000000};
    
    \node[memblock, fill=blue!20] (direct) at (2, 4.3) {Direct Physical Memory Mapping (512GB)};
    \node[addrblock] at (-2.3, 4.3) {0xFFFF800000000000};
    
    \node[memblock, fill=gray!10, minimum height=2cm] (unused) at (2, 2.6) {Unused Virtual Address Space};
    
    \node[memblock, fill=green!20] (identity) at (2, 0.9) {Identity Mapped (10GB)};
    \node[addrblock] at (-2.3, 0.9) {0x0000000000000000};
    
    % Arrows to physical memory
    \node[font=\bfseries] at (9, 6.5) {Physical Memory};
    
    \node[memblock, fill=yellow!20, minimum width=4cm] (phys) at (10, 3) {Physical RAM};
    
    % Mapping arrows
    \draw[-Latex, thick, dashed] (kernel.east) -- ++(1,0) |- (phys.west);
    \draw[-Latex, thick] (direct.east) -- ++(0.5,0) |- (phys.170);
    \draw[-Latex, thick] (identity.east) -- ++(0.5,0) |- (phys.190);
    
    \end{tikzpicture}
    \caption{Virtual Memory Layout After Bootloader Initialization}
    \label{fig:memory_layout}
\end{figure}

\subsection{Architecture Initialization}

After the kernel receives control, the first architecture-specific function executed is \texttt{arch::ArchInit}. This function enables additional CPU features that were not safe to enable during the bootloader phase:

\begin{lstlisting}[caption={Architecture Initialization (init.cpp)}, label={lst:arch_init}]
namespace arch {
void ArchInit(const RawBootArguments &) {
    BlockHardwareInterrupts();
    
    ...
    
    // Enable CPU features (sequence is important)
    EnableOSXSave();    // Enable XSAVE/XRSTOR for context switching
    EnableSSE();        // Enable SSE extensions
    EnableAVX();        // Enable AVX if supported
    EnableNXE();        // Enable No-Execute bit for security
    
    ...
}
}
\end{lstlisting}


\section{Memory Management}
\label{sec:memory}
TODO KRYCZKA

\section{Interrupts}
\label{sec:interrupts}

Interrupt handling is a critical component of any operating system. Without interrupts, the system would rely on manually polling devices or checking states, which is inefficient and wasteful of resources. Furthermore, interrupts enable the system to be significantly more responsive. For example, when a keyboard key is pressed, the operating system may need to immediately switch execution to the thread responsible for consuming the input, rather than waiting for the current task to finish. More broadly, the OS utilizes the interrupt mechanism to perform context switches. If a task exceeds its allocated execution time, timing devices such as the LAPIC Timer trigger an interrupt, allowing the scheduler to preempt the current task and grant CPU time to others.

\subsection{x86-64 Interrupts}

\subsubsection{Interrupt Handling}
On the x86-64 architecture, every interrupt is assigned a unique number (vector) which maps directly to an entry in the \textbf{Interrupt Descriptor Table} (IDT) \cite{IntelManual-Interrupts}. This table contains instructions for the CPU on how to react to specific interrupts. The entry layout is as follows:

\begin{lstlisting}[caption={IDT Entry Layout}, label={lst:idtEntry}]
  enum class IdtPrivilegeLevel : u8 { kRing0 = 0, kRing1 = 1, kRing2 = 2, kRing3 = 3 };

  struct PACK IdtEntryFlags {
      IdtGateType type : 4;
      u8 zero : 1;
      IdtPrivilegeLevel dpl : 2;
      u8 present : 1;
  };
  struct PACK IdtEntry {
      u16 isr_low;    // The lower 16 bits of the ISR's address
      u16 kernel_cs;  // The GDT segment selector that the CPU
                      //  will load into CS before calling the ISR
      u8 ist;         // The IST in the TSS that the CPU will load into RSP
      IdtEntryFlags attributes;  // Type and attributes
      u16 isr_mid;  // The higher 16 bits of the lower 32 bits of the ISR's address
      u32 isr_high; // The higher 32 bits of the ISR's address
      u32 reserved; // Set to zero
  };
\end{lstlisting}

The most critical component of the IDT entry is the address of the function to be invoked (split into \texttt{isr\_low}, \texttt{isr\_mid}, and \texttt{isr\_high}). Another important field is \texttt{kernel\_cs}, which specifies the code segment \cite{IntelManual-Segments} loaded before executing the handler. In x86-64, there are four privilege levels (Rings). We assume the kernel always operates in Ring 0 (the most privileged level). Therefore, the Ring 0 kernel code segment is always written to this field. Conversely, the \texttt{IdtPrivilegeLevel dpl} field specifies the minimal privilege level required to trigger the interrupt via software, which is essential for implementing system calls (syscalls) invoked from userspace.

\subsubsection{Interrupt Service Routines}

Functions handling interrupts differ significantly from standard functions generated by the compiler. When the CPU calls an interrupt handler, it first switches the stack pointer to the kernel stack if a privilege level change occurs (e.g., Ring 3 to Ring 0). This transition is managed via the Task State Segment (TSS) mechanism (for details, refer to \cite{osdev-tss}). The CPU then changes the code segment as specified in the \textbf{IDT entry} (Listing \ref{lst:idtEntry}). Subsequently, it pushes the state of the interrupted procedure onto the stack.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=0cm,
      start chain=going below,
      stacknode/.style={
          draw, 
          minimum width=5cm, 
          minimum height=1cm, 
          outer sep=0pt, 
          font=\ttfamily
      },
      labelnode/.style={
          minimum height=1cm,
          font=\footnotesize\sffamily,
          anchor=east
      }
  ]
  
  \node (highmem) at (0, 1) {Higher address (High Memory)};
  \draw[->] (highmem) -- (0, 0.2);

  \node [stacknode, on chain, fill=gray!10] (ss) {Stack Segment (SS)};
  \node [stacknode, on chain, fill=gray!10] (rsp) {Stack Pointer (RSP)};
  \node [stacknode, on chain, fill=gray!10] (rflags) {RFLAGS};
  \node [stacknode, on chain, fill=gray!10] (cs) {Code Segment (CS)};
  \node [stacknode, on chain, fill=gray!10] (rip) {Instruction Pointer (RIP)};
  \node [stacknode, on chain, fill=gray!10] (error) {Error Code (optional)};
  \draw[<-, thick, red] (error.east) -- ++(1.5,0) node[right, text=red] {Current RSP};

  \node [below=0.5cm of error] (lowmem) {Lower address (Low Memory)};
  \draw[->] (lowmem.north) -- (error.south);

  \end{tikzpicture}
  \caption{Interrupt Service Routine stack layout on entry}
  \label{fig:stackframe}
\end{figure}

The layout illustrated in Figure \ref{fig:stackframe} is known as the \textbf{Interrupt Frame}. This structure is fundamental to the kernel architecture, serving as the basis for context switching, context conversion, and jumping to userspace (Ring 3). A key distinction is that an \textbf{ISR} must return using the special instruction \textbf{IRETQ} \cite{IntelManual-Interrupts}, which reverses the actions described above, including restoring the privilege level and code segment. 

A significant challenge with standard compiler-generated functions is that they may modify the stack frame (prologue/epilogue) in ways that interfere with the hardware-defined layout. To maintain full control and prevent stack corruption, we implement assembly wrappers. These wrappers perform the necessary architecture-specific actions before invoking the architecture-agnostic interrupt handling code defined in C++.

\begin{lstlisting}[style=nasmstyle, caption={Assembly ISR wrapper}, label={lst:isr_asm}]
%macro context_switch_if_needed 0
  cmp rax, 0
  je .done                         ; Omit context switch if there is no need

  mov r13, rax                     ; save next TCB

  mov rdi, r13
  mov rsi, rsp
  call cdecl_ContextSwitchOnInterrupt

  mov rsp, [r13+Thread.kernel_stack]   ; Change the stack
.done:

  load_user_gs_if_needed

  pop_all_regs                    ; Restore registers.
  add rsp, _all_reg_size          ; Deallocate register save space.
  add rsp, 8                      ; Pop error code.
  iretq
%endmacro

; Macro for hardware or software interrupts.
; Calls a handler with the signature 'void handler(u16 lirq, void* frame)'.
%macro interrupt_wrapper 3 ; %1: Logical IRQ, %2: idt idx %3: C handler function
isr_wrapper_%+%2:
    push 0                          ; Push a dummy error code for unification.
    sub rsp, _all_reg_size          ; Allocate space for saving registers.
    push_all_regs                   ; Save registers.

    load_kernel_gs_if_needed

    cld                         ; Clear direction flag for string operations.
    mov rdi, %1                 ; Arg1: mapped lirq number.
    mov rsi, rsp                ; Arg2: pointer to stack frame.
    call %3                     ; Call the specific ISR handler.

    context_switch_if_needed
%endmacro
\end{lstlisting}

\subsubsection{Synchronization}

Since an interrupt can potentially trigger a context switch, synchronization is crucial. This applies to both kernel-space threads at any moment of their lifetime and userspace programs executing system calls. It would be catastrophic if a timer interrupt forced a context switch while the kernel was in the middle of updating scheduler structures or memory tables. Therefore, even on a single-core system, synchronization must be enforced. This is achieved by disabling hardware interrupts (using \textbf{CLI} and \textbf{STI} instructions on x86-64) during critical sections.

\subsection{Unified Interrupt Frame}
\label{subsec:unifiedFrame}

To simplify interrupt handling, we introduced a unified frame structure based on the hardware Interrupt Frame (Figure \ref{fig:stackframe}). Since the state of a thread must be preserved before a context switch, all general-purpose registers must be saved. To achieve this efficiently, these registers are pushed onto the stack by the assembly wrapper, creating the \textbf{Unified Interrupt Frame}.

The x86-64 architecture introduces an inconsistency in the stack layout depending on the interrupt source. Certain exceptions, such as Page Faults (vector 14) or General Protection Faults (vector 13), automatically push an error code onto the stack by the CPU. Hardware interrupts and other exceptions do not. To use a single, unified C++ structure for all interrupt handling (\texttt{IsrErrorStackFrame}), the assembly entry wrappers must normalize the stack. For vectors that do not produce a hardware error code, the wrapper explicitly pushes a dummy value (typically 0) before saving the general-purpose registers (as seen in Listing \ref{lst:isr_asm}). This ensures that the stack pointer is always aligned correctly and points to a uniform structure when the C++ handler is invoked.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=0cm,
      start chain=going below,
      stacknode/.style={
          draw, 
          minimum width=5cm, 
          minimum height=1cm, 
          outer sep=0pt, 
          font=\ttfamily
      },
      labelnode/.style={
          minimum height=1cm,
          font=\footnotesize\sffamily,
          anchor=east
      }
  ]
  
  \node (highmem) at (0, 1) {Higher address (High Memory)};
  \draw[->] (highmem) -- (0, 0.2);

  \node [stacknode, on chain, fill=gray!10] (ss) {Stack Segment (SS)};
  \node [stacknode, on chain, fill=gray!10] (old_rsp) {Old Stack Pointer (RSP)};
  \node [stacknode, on chain, fill=gray!10] (rflags) {RFLAGS};
  \node [stacknode, on chain, fill=gray!10] (cs) {Code Segment (CS)};
  \node [stacknode, on chain, fill=gray!10] (rip) {Instruction Pointer (RIP)};
  \node [stacknode, on chain, fill=gray!10] (error) {Error Code (or Dummy)};
  \node [stacknode, on chain, fill=gray!10] (genregs) {General Registers (RDI, RSI, ...)};
  \node [stacknode, on chain, fill=gray!10] (rax) {Last Saved Register};
  
  \draw[<-, thick, red] (rax.east) -- ++(1.5,0) node[right, text=red] {RSP (New Stack Pointer)};

  \node [below=0.5cm of rax] (lowmem) {Lower address (Low Memory)};
  \draw[->] (lowmem.north) -- (rax.south);

  \end{tikzpicture}
  \caption{Unified Interrupt Frame (IsrErrorStackFrame)}
  \label{fig:unifiedstackframe}
\end{figure}

\subsection{Context Switch}

As previously mentioned, the OS utilizes the interrupt mechanism to perform task switching. The interrupt mechanism handles most of the necessary context-switching operations automatically:
\begin{itemize}
\item Manages ring permissions (automatically swapping code and stack segments).
\item Automatically swaps the stack from user stack to kernel stack (using the TSS).
\item Restores \textbf{RFLAGS} (which includes the \textbf{interrupt flag} responsible for enabling/disabling hardware interrupts).
\item Jumps back to the code address pointed to by \textbf{RIP}.
\end{itemize}

To switch contexts, we must ensure the current thread's state is preserved in a \textbf{Unified Interrupt Frame} (Figure \ref{fig:unifiedstackframe}) on its kernel stack. Additionally, a valid frame must exist for the target thread. If a thread has run previously, it will have saved its frame naturally during its last preemption. However, for a new thread that has never executed, this frame must be constructed manually. 

We assume that all threads begin execution in kernel space before eventually jumping to user space. Since all interrupt handling occurs within the kernel, the interrupt frame always resides on the kernel stack. Therefore, we can initialize the kernel stack of the new thread with a fabricated frame before the context switch. The initialization procedure is as follows:

\begin{lstlisting}[caption={Thread stack initialization}, label={lst:threadStack}]
void InitializeThreadStack(void **stack, const Sched::Task &task)
{
    /* NOTE: Thread entry always starts in Kernel Code */
    auto stack_top = static_cast<byte *>(*stack) - sizeof(IsrErrorStackFrame);
    auto frame     = reinterpret_cast<IsrErrorStackFrame *>(stack_top);

    memset(stack_top, 0, sizeof(IsrErrorStackFrame));

    /* Initialize IsrErrorStackFrame (Hardware Part + Error Code) */
    frame->isr_stack_frame.rip    = reinterpret_cast<u64>(task.func);
    frame->error_code             = 0; // Dummy error code
    frame->isr_stack_frame.cs     = static_cast<u64>(cpu::GDT::kKernelCodeSelector);
    frame->isr_stack_frame.rflags = kInitialRFlags;
    frame->isr_stack_frame.rsp    = reinterpret_cast<u64>(*stack);
    frame->isr_stack_frame.ss     = static_cast<u64>(cpu::GDT::kKernelDataSelector);

    /* Initialize function arguments (Software Part / Registers) */
    if (task.args_count > 0) {
        frame->registers.rdi = task.args[0];
    }

    // ... other args

    if (task.args_count > 5) {
        frame->registers.r9 = task.args[5];
    }

    /* Save adjusted stack address */
    *stack = reinterpret_cast<void *>(stack_top);
}
\end{lstlisting}

With both stack frames prepared, the context switch logic is straightforward: swap the \textbf{RSP} (current stack pointer) to the kernel stack of the target thread and execute the \textbf{IRETQ} instruction. This instruction restores the state and resumes execution, as demonstrated in the \textbf{context\_switch\_if\_needed} macro (Listing \ref{lst:isr_asm}).

\subsection{Jumping to Userspace}

Transitioning to userspace is performed similarly to a thread context switch. We construct an artificial interrupt frame, but in this case, the code segment and stack segment within the frame are set to the user space selectors, and the stack pointer is set to the user space stack. The implementation is shown below:

\begin{lstlisting}[caption={Userspace Jump C++ code}, label={lst:userSpaceJump}]
extern "C" void cdecl_JumpToUserSpaceEntry(void *addr, IsrStackFrame *frame)
{
    ASSERT_NOT_NULL(addr);
    ASSERT_NOT_NULL(frame);
    ASSERT_NOT_NULL(hardware::GetCoreLocalTcb());

    auto thread          = hardware::GetCoreLocalTcb();
    thread->kernel_stack = thread->kernel_stack_bottom; // reset kernel stack

    frame->rip    = reinterpret_cast<u64>(addr);
    frame->cs     = static_cast<u64>(cpu::GDT::kUserCodeSelector);
    frame->rflags = static_cast<u64>(kInitialRFlags);
    frame->rsp    = reinterpret_cast<u64>(thread->user_stack_bottom);
    frame->ss     = static_cast<u64>(cpu::GDT::kUserDataSelector);

    const u64 t            = TimingModule::Get().GetSystemTime().ReadLifeTimeNs();
    thread->kernel_time_ns = t - thread->timestamp;
    thread->timestamp      = t;

    SetThreadGs(thread);
    __asm__ volatile("swapgs" ::: "memory");
}
\end{lstlisting}

\begin{lstlisting}[style=nasmstyle, caption={Userspace Jump NASM code}, label={lst:userSpaceJumpNasm}]
; c_decl
; void JumpToUserSpace(void (*func)(), void* arg)
;   RDI = func
;   RSI = arg
; Note: Caller is responsible for ensuring proper environment before calling (disabling IRQs)
JumpToUserSpace:
    sub rsp, _jump_userspace_stack_space
    push rsi
    ; aligned properly

    mov rsi, rsp
    add rsi, 8
    call cdecl_JumpToUserSpaceEntry

    xor rax, rax
    mov rax, _user_data_selector
    mov ds, ax
    mov es, ax
    mov fs, ax
    mov gs, ax

    pop rsi
    mov rdi, rsi ; prepare void* arg for func if needed
    iretq
\end{lstlisting}

\subsection{Interrupts Hardware Abstraction}

To maintain architectural independence, the Interrupt Table is abstracted. Upon any interrupt, control is passed to the \textbf{Logical Interrupt Table (LIT)}, which is responsible for executing the necessary actions common to all interrupts. The architecture-specific code establishes the mapping between the hardware interrupts and the logical interrupt table, from that point forward, interrupt management is handled entirely by the LIT. Responsibilities of the LIT include:

\begin{itemize}
\item Managing interrupt handlers, allowing the kernel to modify interrupt responses dynamically.
\item Tracking interrupt nesting levels.
\item Collecting statistics, such as interrupt counts per thread, kernel time, and user space time.
\item Interacting with hardware interrupt drivers such as the \textbf{PIC} or \textbf{APIC}.
\item Masking (blocking) individual interrupts.
\end{itemize}

The LIT handling procedures may return a pointer to the next thread scheduled for execution. If such a pointer is returned, the hardware wrapper performs the context switch upon exit. This mechanism enables the system to react rapidly to state changes or preempt the current thread via a timer interrupt.
\section{Timing}
\label{sec:timing}

Prior to the implementation of the Scheduler, and alongside functional memory management and interrupt handling, it was necessary to establish the infrastructure and drivers for timing mechanisms. This includes devices capable of measuring the system's uptime, referred to as \textbf{Clocks}. Additionally, the system requires devices capable of generating interrupts at specific intervals. These are essential for preempting the currently executing thread if it fails to yield the CPU voluntarily, thereby ensuring fair scheduling. Such devices are referred to as \textbf{Event Clocks}. To unify the timing subsystem, all devices and mechanisms rely on nanoseconds as the fundamental unit of time abstraction.

\subsection{Infrastructure}

The architecture-specific code is responsible for detecting available hardware and registering it within the central timing infrastructure, specifically the \textbf{ClockRegistry} and \textbf{EventClockRegistry}. Subsequently, architecture-agnostic components, such as the \textbf{ACPI} subsystem, may register additional clocks by parsing system description tables. Finally, the architecture-defined functions \texttt{hal::PickSystemClockSource()} and \texttt{hal::PickSystemEventClockSource()} are invoked during the initialization of the timing module to select the optimal sources for each purpose. The infrastructure allows the scheduler to implement either a tick-based or a tickless strategy.

\subsection{Clocks}

Clock Drivers are defined by the following structure:

\begin{lstlisting}[caption={Clock Driver Structure}, label={lst:clockDriver}]
struct alignas(arch::kCacheLineSizeBytes) ClockRegistryEntry : data_structures::RegistryEntry {
  /* Clock numbers */
  u64 frequency_kHz; // Frequency in kHz
  u64 ns_uncertainty_margin_per_sec;  
    // Uncertainty margin in femtoseconds per second
  u64 clock_numerator; // For conversion to nanoseconds, this is the numerator
  u64 clock_denominator; // For conversion to nanoseconds, this is the denominator

  /* Callbacks */
  u64 (*read)(ClockRegistryEntry *);
  bool (*enable_device)(ClockRegistryEntry *);
  bool (*disable_device)(ClockRegistryEntry *);
  void (*stop_counter)(ClockRegistryEntry *);
  void (*resume_counter)(ClockRegistryEntry *);

  /* Own data */
  void *own_data;
};
\end{lstlisting}

For the x86-64 architecture, clock selection is prioritized based on availability and precision in the following order: TSC > HPET > RTC > PIT. Currently, support is implemented only for the TSC and HPET drivers.

\subsubsection{TSC}
The Time Stamp Counter (TSC) is the optimal clock source due to its high precision and core-locality. Accessing the TSC involves reading a CPU register, which requires only a few cycles, unlike external clocks that may require hundreds. However, the TSC has historical limitations. On older processors, the counter's frequency was tied to the core frequency. Consequently, frequency scaling (throttling) caused the time measurement to drift, rendering it unreliable. Modern Intel processors introduced the Invariant TSC, which ensures a constant frequency regardless of the core's power state. Another limitation is that on certain CPUs, the TSC frequency is not explicitly known and must be measured against a known reference. For this purpose, the system utilizes the HPET, which serves as the minimal hardware requirement for reliable calibration (see \cite{IntelManual-TSC}).

\subsubsection{HPET}
The High Precision Event Timer (HPET) also offers high precision and is generally more stable than the TSC on older hardware. However, it is an external device mapped via Memory-Mapped I/O (MMIO). As a result, accessing the HPET is significantly slower than reading the TSC, potentially taking up to 1000 cycles per read. Due to this performance overhead, the HPET is designated as the secondary choice. It is primarily utilized for calibrating other clocks rather than for frequent timekeeping operations.

\subsection{Event Clocks}

Event Clock Drivers are defined by the following structure:

\begin{lstlisting}[caption={Event Clock Driver Structure}, label={lst:eventClockDriver}]
struct PACK EventClockFlags {
    bool IsCoreLocal : 1;
    u32 padding : 31;
};
static_assert(sizeof(EventClockFlags) == sizeof(u32));

enum class EventClockState : u8 {
    kDisabled = 0,  // Clock is disabled
    kPeriodic,      // Clock is in periodic mode
    kOneshot,       // Clock is in oneshot mode
    kOneshotIdle,   // Clock is in oneshot mode but no event is scheduled
    klast,
};

struct alignas(arch::kCacheLineSizeBytes) EventClockRegistryEntry : data_structures::RegistryEntry {
    /* Clock numbers */
    u64 min_next_event_time_ns;  // Minimum time for the next event in nanoseconds

    /* Clock specific data */
    EventClockFlags flags;     // Features of the event clock, e.g., core-local
    CoreMask supported_cores;  // Cores that support this event clock

    /* infra data */
    u64 next_event_time_ns;  // Time for the next event in nanoseconds
    EventClockState state;   // Current state of the clock

    /* Driver data */
    void *own_data;  // Pointer to the clock's own data, used for callback

    /* callbacks */
    struct callbacks {
        // Callback to set next event time
        u32 (*next_event)(EventClockRegistryEntry *, u64);  
        // Callback to set clock state
        u32 (*set_oneshot)(EventClockRegistryEntry *);
        // Callback to set clock state
        u32 (*set_periodic)(EventClockRegistryEntry *);     
        void (*on_entry)(EventClockRegistryEntry *);        // optional
        void (*on_exit)(EventClockRegistryEntry *);         // optional
    } cbs;
};
\end{lstlisting}

For x86-64, the selection of event clocks is based on the following priority order: LAPIC Timer > HPET > PIT. Currently, only the LAPIC Timer is supported as it provides the core-local interrupt capabilities required for efficient scheduling.

\subsubsection{LAPIC Timer}
Similar to the TSC, the LAPIC Timer is local to the processor core, ensuring extremely low-latency access to its registers. A significant advantage of this architecture is that each core possesses an independent timer, which eliminates the need for shared resource management or synchronization between cores. However, like the TSC, the LAPIC Timer operates at a frequency derived from the CPU bus or core frequency, which is not standard across various CPUs. Consequently, it requires calibration against a reference clock, for which the HPET is utilized.

\section{File System}
\label{section:fs}

The file system is a fundamental part of any operating system, providing mechanisms to store, access, and manage data on storage devices. We have designed a unified abstraction layer known as the Virtual File System (VFS). The VFS abstracts the underlying implementation details of specific file systems, providing the kernel and user space with an API for operations such as reading, writing, creating, moving, and deleting files and directories.

\subsection{VFS}

The VFS enables the operating system to interact with various file system types (e.g., FAT12, FAT16, FAT32) through a uniform interface, eliminating the need for the kernel to understand the specific implementation details. This modularity is achieved through a combination of compile-time enforced interfaces (Concepts) and a runtime dispatch mechanism.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=1.5cm,
      block/.style={
        rectangle, draw, fill=gray!10,
        text width=5cm, text centered,
        rounded corners, minimum height=1cm
      },
      component/.style={
        rectangle, draw, fill=gray!10,
        text width=5cm, text centered,
        minimum height=1cm
      },
      interface/.style={
        rectangle, draw, dashed, fill=gray!10,
        text width=5cm, text centered,
        minimum height=1cm
      },
      storage/.style={
        cylinder, draw, fill=gray!10,
        shape aspect=0.5,
        text width=4.5cm, text centered,
        minimum height=1cm
      },
      arrow/.style={-Latex, thick}
  ]

  \node (user_process) [block] {User Process};
  \node (fd_manager) [block, below=of user_process] {File Descriptor Manager};
  \node (vfs_module) [block, below=of fd_manager] {VFS Module};
  \node (fs_interface) [interface, below=of vfs_module] {Filesystem Interface};
  \node (fs_driver) [component, below=of fs_interface] {File System Driver \\ (FAT12 / FAT16 / FAT32)};
  \node (vfs_io) [interface, below=of fs_driver] {VFS I/O Interface};
  \node (storage_device) [storage, below=of vfs_io] {Storage Device \\ (RAM, Disk, Network)};

  \draw [arrow] (user_process) -- (fd_manager) node [midway, right] {Syscalls};
  \draw [arrow] (fd_manager) -- (vfs_module) node [midway, right] {File Operations};
  \draw [arrow] (vfs_module) -- (fs_interface) node [midway, right] {Dispatch};
  \draw [arrow] (fs_interface) -- (fs_driver) node [midway, right] {Implements};
  \draw [arrow] (fs_driver) -- (vfs_io) node [midway, right] {Uses};
  \draw [arrow] (vfs_io) -- (storage_device) node [midway, right] {I/O Operations};

  \end{tikzpicture}
  \caption{High-level VFS Architecture}
  \label{fig:vfs_architecture}
\end{figure}

As illustrated in Figure \ref{fig:vfs_architecture}, the VFS architecture is composed of four primary layers:

\begin{itemize}
\item \textbf{VFS Module}: The central orchestrator for path resolution and operation delegation.
\item \textbf{Filesystem Interface}: A struct containing function pointers that abstract specific driver operations.
\item \textbf{Filesystem Driver}: The concrete implementation of a specific file system format.
\item \textbf{VFS I/O Interface}: An abstraction for block-level data access.
\end{itemize}

The following subsections describe each of these components in detail.

\subsubsection{VFS Module}

The VFS module serves as the entry point for all file operations and is responsible for managing file system mounts. It exposes the internal kernel API for operations such as opening, reading, and writing files. The module's primary responsibility is to translate these requests into calls to the appropriate filesystem instance based on the provided file path and the currently registered mount points.

When a VFS operation (e.g., \texttt{CreateFile}) is invoked, the module executes the following sequence:
\begin{enumerate}
    \item \textbf{Find Mount Point}: The system utilizes a \textbf{crit-bit tree} \cite{critbit} to efficiently locate the longest prefix match for the given path among all registered mount points. This step identifies the specific filesystem driver responsible for handling the operation.
    \item \textbf{Check Permissions}: The module validates the operation against the mount point options (e.g., ensuring write operations are not attempted on read-only mounts).
    \item \textbf{Path Translation}: The absolute system path is translated into a path relative to the mount point root.
    \item \textbf{Delegation}: The operation is delegated to the corresponding method of the specific filesystem driver.
\end{enumerate}

\begin{lstlisting}[caption={VfsModule CreateFile Implementation}, label={lst:vfsModuleCreateFile}]
Result<> internal::VfsModule::CreateFile(const Path &path)
{
    auto mount_result = FindMountPoint(path);
    RET_UNEXPECTED_IF_ERR(mount_result);

    MountPoint *mount = mount_result.value();

    RET_UNEXPECTED_IF(mount->options.read_only, VfsError::kReadOnly);

    Path relative_path = GetRelativePath_(path, mount->path);
    return mount->fs.CreateFile(relative_path);
}
\end{lstlisting}

\subsubsection{Filesystem Interface}

The \texttt{vfs::Filesystem} struct acts as a uniform interface that all concrete filesystem drivers must expose. It contains function pointers for various file and directory manipulations, alongside metadata about the filesystem. Each function pointer accepts a \texttt{void* ctx} argument, allowing the generic VFS layer to pass the concrete driver instance.

\begin{lstlisting}[caption={vfs::Filesystem Structure}, label={lst:vfsFilesystem}]
struct Filesystem {
    struct Operations {
        // File operations
        Result<> (*create_file)(void *ctx, const Path &path);
        Result<size_t> (*read_file)(
            void *ctx, const Path &path, void *buffer, size_t size, size_t offset
        );
        Result<size_t> (*write_file)(
            void *ctx, const Path &path, const void *buffer, size_t size, size_t offset
        );
        Result<> (*delete_file)(void *ctx, const Path &path);
        // ... (additional operations omitted for brevity)
    };

    struct Info {
        Type type;
        const char *name;  // e.g., "FAT32", "FAT16"
    };
    
    Filesystem() = delete;
    explicit Filesystem(void *context, const Operations &operations, const Info &info)
    : context_(context), ops_(operations), info_(info) {}

private:
    void *context_;  // Pointer to the concrete driver instance
    Operations ops_;
    Info info_;
};
\end{lstlisting}

\subsubsection{Filesystem Driver}

Each filesystem driver implements the logic required for a specific filesystem type (e.g., FAT12, FAT16, FAT32). The driver interprets the raw data structures on the storage device and performs the requested operations. For instance, the FAT32 driver handles the manipulation of the File Allocation Table, directory entries, and cluster chains according to the FAT32 specification (for details, see \cite{fat-spec}).

To implement these drivers efficiently, we employ the Curiously Recurring Template Pattern (CRTP) \cite{crtp}. Each driver class inherits from a templated base class that provides common functionality, while the derived class implements format-specific details. This approach enables static polymorphism, reducing the runtime overhead typically associated with virtual function calls.

\begin{lstlisting}[caption={FAT Driver CRTP Base Class}, label={lst:fatCrtp}]
template <template <typename> typename T, typename IO>
class Fat
{
    using ImplT  = T<IO>;
    using Traits = FatTraits<T, IO>;

protected:
    // ... common FAT structures, validation, and operations

public:
    NODISCARD Filesystem GetFilesystem()
    {
        return Filesystem(
            this, // 'this' pointer is passed as the context
            Filesystem::Operations{
                .create_file = &Fat::CreateFileCallback_,
                // ...
            },
            Filesystem::Info{
                .type = ImplT::kFsType,
                .name = ImplT::kFsName,
            }
        );
    }

private:
    FAST_CALL Result<> CreateFileCallback_(void *ctx, const Path &path)
    {
        return static_cast<Fat *>(ctx)->CreateFile(path);
    }
    // ...
};
\end{lstlisting}

Each \texttt{Fat} derivative implements its specific logic (e.g., Getting FAT entries differs between FAT12 and FAT32/16) and provides a static \texttt{IsValid(IO \&io)} method to probe whether a given I/O device contains a valid instance of that filesystem.

\begin{lstlisting}[caption={Fat12 GetFATEntry Implementation}, label={lst:fat12GetFatEntry}]
    NODISCARD FORCE_INLINE_F ClusterNumT GetFATEntry_(ClusterNumT cluster) const
    {
        ASSERT_LT(
            cluster, BaseT::cluster_count_ + BaseT::kFirstClusterNumber,
            "Cluster number out of range"
        );
        const size_t fat_offset = cluster + (cluster / 2);
        const size_t sector_number =
            BaseT::fat_region_.start + (fat_offset / boot_sector_.fat.bytes_per_sector);
        const size_t sector_offset = fat_offset % boot_sector_.fat.bytes_per_sector;

        // Load 2 sectors if entry spans two sectors
        size_t count =
            (sector_offset == static_cast<size_t>(boot_sector_.fat.bytes_per_sector - 1)) ? 2 : 1;
        auto range = BaseT::io_.ReadRange({sector_number, count});
        if ((cluster % 2) == 0) {  // Even cluster
            return internal::get<ClusterNumT>(range, sector_offset) & kClusterMask;
        } else {
            return internal::get<ClusterNumT>(range, sector_offset) >> 4;
        }
    }
\end{lstlisting}

\subsubsection{VFS Interfaces and Concepts}

To enforce architectural compliance at compile time, we utilize C++20 Concepts to define strict contracts for both filesystem drivers and low-level storage operations.

The \texttt{VFSInterface} concept (Listing \ref{lst:vfsInterfaceConcept}) mandates that any compliant driver class must provide a constructor accepting an I/O backend, a static method to validate the filesystem signature on the storage medium, and a method to retrieve the runtime function table.

\begin{lstlisting}[caption={VFSInterface Concept Definition}, label={lst:vfsInterfaceConcept}]
template <template <typename> typename T, typename IO>
concept VFSInterface = VFSIO<IO> and requires(T<IO> fs, IO io) {
    T<IO>(io);
    { T<IO>::IsValid(io) } -> std::same_as<bool>;
    { fs.GetFilesystem() } -> std::same_as<Filesystem>;
};
\end{lstlisting}

The VFS I/O interface abstracts low-level data access. The \texttt{VFSIO} concept (Listing \ref{lst:vfsioConcept}) establishes a contract for block-based input and output. By adhering to this concept, filesystem drivers remain decoupled from the underlying hardware, enabling seamless interaction with diverse storage backends - such as RAM disks, physical partitions, or network storage - without requiring implementation changes.

\begin{lstlisting}[caption={VFSIO Concept Definition}, label={lst:vfsioConcept}]
template <typename IO>
concept VFSIO =
    requires(IO io, size_t offset, io::SectorRange range, std::span<const byte> data, size_t size) {
        { io.ReadRange(range) } -> std::same_as<std::span<byte>>;
        { io.ReadSector(offset) } -> std::same_as<std::span<byte>>;
        { io.WriteRange(range, data) } -> std::same_as<void>;
        { io.WriteSector(offset, data) } -> std::same_as<void>;
        { io.GetSectorSize() } -> std::same_as<size_t>;
    };
\end{lstlisting}

\subsection{File Descriptors}

The file descriptor system provides a hierarchical structure to handle file I/O operations initiated by user processes. This design tracks open files, maintains current read/write offsets, and enforces access rights.

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{res/fd-diagram.svg}
  \caption{File Descriptor Design}
  \label{fig:fd_architecture}
\end{figure}

As depicted in Figure \ref{fig:fd_architecture}, the system comprises three main tables:

\subsubsection{FdTable (Per-Process)}
Each process maintains its own \texttt{FdTable}, which is an array mapping integer file descriptors to \texttt{RefPtr<OpenFileEntry>} objects.

\begin{lstlisting}[caption={FdTable Allocation}, label={lst:fdTableAllocate}]
FdResult<fd_t> FdTable::Allocate(data_structures::RefPtr<OpenFileEntry> global_entry)
{
    std::lock_guard lock(lock_);
    for (size_t i = 0; i < kMaxFdsPerProcess; ++i) {
        if (entries_[i] == nullptr) {
            entries_[i] = std::move(global_entry);
            ++count_;
            return static_cast<fd_t>(i);
        }
    }
    return std::unexpected(FdError::kFdTableFull);
}
\end{lstlisting}

\subsubsection{OpenFileTable (Global)}

The \texttt{OpenFileTable} is a system-wide pool of \texttt{OpenFileEntry} objects. Each entry represents a unique "open" instance of a resource (file or pipe), maintaining dynamic state such as the current read/write offset, access flags (e.g., Read/Write), and a handle to the underlying resource. Multiple file descriptors from different processes can point to the same \texttt{OpenFileEntry}, allowing them to share the cursor position.

\begin{lstlisting}[caption={OpenFileTable Creation}, label={lst:openFileTableOpenFile}]
FdResult<data_structures::RefPtr<OpenFileEntry>> OpenFileTable::OpenFile(File *file, OpenMode flags)
{
    RET_UNEXPECTED_IF(file == nullptr, FdError::kInvalidArgument);

    std::lock_guard lock(lock_);

    const size_t idx = entries_.Allocate();
    RET_UNEXPECTED_IF(idx == std::numeric_limits<size_t>::max(), FdError::kIoError);

    OpenFileEntry *entry = entries_.Get(idx);
    ASSERT_NOT_NULL(entry);

    new (entry) OpenFileEntry();
    entry->pool_idx_ = idx;

    entry->handle    = FileHandle::Wrap(file);
    entry->flags     = static_cast<u32>(flags);
    entry->offset    = 0;
    entry->is_append = HasMode(flags, OpenMode::kAppend);
    ++count_;

    return data_structures::RefPtr(entry);
}
\end{lstlisting}

The \texttt{FileHandle} utilizes a \texttt{NonOwningTaggedPtr} to hold either a pointer to a \texttt{File} object or a \texttt{Pipe} instance, enabling the \texttt{OpenFileEntry} to manage both cases seamlessly.

\subsubsection{FileTable (Global)}
The \texttt{FileTable} manages unique \texttt{File} objects system-wide. Each \texttt{File} object acts as the in-memory representation of a physical file on the filesystem, storing static attributes such as the path and file size. This ensures that a file on the VFS has a single representation in memory, regardless of how many times it has been opened.

\begin{lstlisting}[caption={FileTable GetOrCreate}, label={lst:fileTableGetOrCreate}]
FdResult<data_structures::RefPtr<File>> FileTable::GetOrCreate(const vfs::Path &path)
{
    RET_UNEXPECTED_IF(path.IsEmpty(), FdError::kInvalidArgument);

    File *existing = Find(path);
    if (existing != nullptr) {
        return data_structures::RefPtr(existing, false);
    }

    const size_t idx = files_.Allocate();
    RET_UNEXPECTED_IF(idx == std::numeric_limits<size_t>::max(), FdError::kIoError);

    File *file = files_.Get(idx);
    ASSERT_NOT_NULL(file);

    new (file) File();
    file->pool_idx_ = idx;

    file->size = 0;
    file->mode = 0;
    file->path = path;
    ++count_;

    return data_structures::RefPtr(file);
}
\end{lstlisting}

\subsubsection{FdManager}
The \texttt{FdManager} integrates these three tables and exposes file descriptor operations (e.g., \texttt{Open}, \texttt{Close}, \texttt{Read}). When a process requests to open a file, the \texttt{FdManager} coordinates the interaction: it validates existence via the VFS module, retrieves the \texttt{File} object, creates a new entry in the \texttt{OpenFileTable}, and finally assigns a file descriptor in the process's \texttt{FdTable}.

\begin{lstlisting}[caption={FdManager Open Implementation}, label={lst:fdManagerOpen}]
FdResult<fd_t> FdManager::Open(const vfs::Path &path, OpenMode flags)
{
    // Check if file exists in VFS
    auto exists_result = VfsModule::Get().FileExists(path);
    RET_UNEXPECTED_IF(!exists_result, FdError::kIoError);
    RET_UNEXPECTED_IF(!*exists_result, FdError::kNotFound));

    auto file_result = file_table_.GetOrCreate(path);
    RET_UNEXPECTED_IF_ERR(file_result);

    auto open_result = open_file_table_.OpenFile(file_result->Get(), flags);
    RET_UNEXPECTED_IF_ERR(open_result);

    FdTable *fd_table = GetCurrentProcessFdTable();
    RET_UNEXPECTED_IF(fd_table == nullptr, FdError::kIoError);

    auto fd_result = fd_table->Allocate(std::move(*open_result));
    RET_UNEXPECTED_IF_ERR(fd_result);

    return *fd_result;
}
\end{lstlisting}

\subsubsection{Resource Lifecycle Management}

To ensure efficient memory usage and prevent resource leaks, the life cycle of \texttt{OpenFileEntry} and \texttt{File} objects is managed through intrusive reference counting. Both structures inherit from a \texttt{RefCounted} base class and are managed via smart pointers (\texttt{RefPtr} and \texttt{TaggedPointer}).

When a process closes a file descriptor (or terminates), the reference to the corresponding \texttt{OpenFileEntry} in the process's \texttt{FdTable} is released. If the reference count drops to zero - indicating that no other file descriptors (across any process) refer to this open stream - the \texttt{OpenFileEntry} is automatically deallocated and removed from the global \texttt{OpenFileTable}.

Consequently, the destruction of an \texttt{OpenFileEntry} releases its hold on the underlying \texttt{File} object. Similarly, if the reference count of the \texttt{File} object reaches zero, it implies that no active streams are reading from or writing to that specific file. The system then automatically reclaims the associated memory from the \texttt{FileTable}. This cascading deallocation mechanism ensures that kernel memory is only consumed for files that are actively in use by at least one process.

\section{Scheduling}

The scheduling module is the final core component of the operating system, relying heavily on and utilizing the previously described modules. Although the current implementation targets a single-core architecture for simplicity, the design is extensible to multi-core systems, as discussed later in this document.

\subsection{Process Structure}

The process structure is defined as follows:

\begin{lstlisting}[caption={Process Structure}, label={lst:processStruct}]
struct PACK Pid {
    u16 id;
    u64 count : 48;

    bool operator==(const Pid &other) const = default;
};

struct PACK ProcessFlags {
    bool KernelSpaceOnly : 1;
    bool PreserveFloats : 1;
};
static_assert(sizeof(ProcessFlags) == 1);

enum class ProcessState : u64 {
    kReady = 0,
    kWaitingForJoin,
    kTerminated,
    kLast,
};
static_assert(sizeof(ProcessState) == sizeof(u64));

struct Process : hal::Process {
    static constexpr size_t kMaxNameLength = vfs::kMaxComponentSize;

    /* Management */
    char name[kMaxNameLength];
    Pid pid;
    ProcessFlags flags;
    Thread *threads;
    u64 live_threads;
    u64 threads_to_clean;
    ProcessState state;
    WaitQueue<Thread, 3> *wait_queue;
    int status;

    /* Process resources */
    Mem::VPtr<Mem::AddressSpace> address_space;

    /* File descriptor table */
    Mem::VPtr<Fs::FdTable> fd_table;

    /* Standard I/O pipes (owned by process) */
    IO::Pipe<Fs::kStdioBufferSize> stdin_pipe;
    IO::Pipe<Fs::kStdioBufferSize> stdout_pipe;
    IO::Pipe<Fs::kStdioBufferSize> stderr_pipe;
};
\end{lstlisting}

\subsubsection{PID}
\label{subsubsec:pid}

As mentioned in the limitations section \ref{subsec:limitations}, the system imposes a hard limit on the maximum number of existing processes. This constraint allows for optimized process lookup via direct indexing. The \textbf{PID} (Process Identifier) is composed of a reservable identifier and an atomic counter, ensuring that each PID remains unique throughout the entire lifetime of the system.

\subsubsection{Process Flags}

\textbf{KernelSpaceOnly} -- enables the creation of kernel-only threads and processes. These processes do not require a separate user-space address space or user-space stack. This minimization of resource consumption improves performance; additionally, context switching between kernel-only threads does not necessitate switching the address space.

\textbf{PreserveFloats} -- indicates that all threads within the process save and restore floating-point registers during context switches by default. Disabling this flag allows the system to omit these potentially expensive operations for threads that do not utilize floating-point arithmetic.

\subsubsection{Process State}

The process state has less significance than the Thread State (described later) and is primarily utilized for resource management when waiting for a process to terminate. It ensures that process resources are not deallocated more than once.

\subsubsection{Address Space}

The address space is a fundamental component of the process structure. It defines all information regarding virtual memory, specifically establishing the translation between the virtual and physical layers. Each process operates within its own virtual address space, facilitating simpler memory management regarding fragmentation, performance, and security. Refer to Memory Management \ref{sec:memory} for further details.

\subsubsection{File Descriptor Table}

This table contains all file descriptors opened by threads executing within the process. Refer to the File System section \ref{section:fs} for more information.

\subsubsection{Pipes}

Pipes handle the buffered I/O of the process, primarily used for standard input and output communication with the user.

\subsubsection{Name}

The name of the process typically corresponds to the name of the executable. Unlike the unique PID, the process name is not required to be unique.

\subsubsection{Threads}

This field points to the first element of a doubly linked list of threads running in this environment. It is used primarily for operations such as termination (kill/exit).

\subsubsection{Live Threads and Threads to Clean}

These counters are used for synchronization with the \textbf{ProcessRipper}. Refer to Kernel Workers \ref{subsubsec:kworkers} for more details.
 
\subsubsection{Status}

The exit status of the process, which is passed to any thread waiting for the process to complete.

\subsubsection{Wait Queue}

The \textbf{Wait Queue} stores threads that are waiting for this process to finish. All threads in this queue are woken up upon process termination.

\subsection{Kernel Address Space}

The entire operating system shares a single Kernel Address Space. Every kernel process operates within this space. Additionally, the kernel address space is mapped into every user-space process's address space. This design choice enhances performance and simplifies architecture, as user threads frequently invoke system calls. Switching address spaces on every system call would incur significant performance overhead and complicate the logic required for handling syscalls.

\subsection{Thread Structure}

The thread structure is defined as follows:

\begin{lstlisting}[caption={Thread Structure}, label={lst:threadStruct}]
enum class UserPriority : u8 { kLow = 0, kMediumLow, kMedium, kMediumHigh, kHigh, kLast };

struct PACK Tid {
    u16 id;
    u64 count : 48;

    bool operator==(const Tid &other) const = default;
};

struct PACK ThreadFlags {
    SchedulingPolicy policy : 8;
    u8 priority : 8;
    UserPriority user_priority : 3;
    bool preserve_floats : 1;
    bool detached : 1;
    u64 padding : 43;
};
static_assert(sizeof(ThreadFlags) == 8);

enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kSleeping,
    kBlockedOnWaitQueue,
    kWaitingForJoin,
    kTerminated,
    kLast,
};
static_assert(sizeof(ThreadState) == sizeof(u64));

static constexpr int kSchedulingIntrusiveLevel  = 0;
static constexpr int kSleepingIntrusiveLevel    = 1;
static constexpr int kProcessListIntrusiveLevel = 2;
static constexpr int kWaitQueueIntrusiveLevel   = 3;

struct Thread : data_structures::IntrusiveRbNode<Thread, u64, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveRbNode<Thread, u64, kSleepingIntrusiveLevel>,
                data_structures::IntrusiveListNode<Thread, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveListNode<Thread, kSleepingIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kWaitQueueIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kProcessListIntrusiveLevel> {
    /* Management */
    Tid tid;
    Pid owner;
    ThreadFlags flags;
    ThreadState state;
    void *retval;
    WaitQueue<Thread, kWaitQueueIntrusiveLevel> *wait_queue;

    /* Thread resources */
    void *kernel_stack;
    void *kernel_stack_bottom;
    void *user_stack;
    void *user_stack_bottom;

    /* Statistics */
    u64 kernel_time_ns;
    u64 user_time_ns;
    u64 timestamp;
    u64 timestamp_execution_start_ns;
    u64 num_interrupts;
    u64 num_syscalls;
    u64 num_context_switches;

    /* Arch */
    hal::Thread arch_data;

    NODISCARD u64 CalculateCpuTime();
};
\end{lstlisting}

\subsubsection{TID}

Similar to the PID \ref{subsubsec:pid}, the system limits the number of available threads. This property is utilized to create unique \textbf{TIDs} (Thread Identifiers), allowing for rapid lookups.

\subsubsection{Owner's PID}

The PID of the process that owns the thread.

\subsubsection{Thread Flags}

A collection of bitfields that determine how the scheduler manages the thread:

\textbf{SchedulingPolicy policy} -- defines the scheduling policy under which the thread currently operates. Refer to Policies \ref{subsec:policies} for more details.

\textbf{u8 priority} -- defines the internal priority of the thread within its assigned policy. This value is managed by the kernel.

\textbf{UserPriority user\_priority} -- may be specified by User Space to inform the scheduler of the relative importance of threads.

\textbf{bool preserve\_floats} -- specifies whether the floating-point state is preserved during context switches.

\textbf{bool detached} -- specifies whether the scheduler should automatically clean up the thread upon termination, without waiting for a join operation.

\subsubsection{Thread State}

Describes the current status of the thread. This state tracking allows the kernel to monitor each thread individually and verify system integrity, as only specific state transitions are permitted. The thread state transitions graph is defined as follows:

\newgeometry{top=1cm, bottom=1.5cm, left=1cm, right=1cm} 

\begin{figure}[p]
    \centering
    \includesvg[height=0.95\textheight, width=\textwidth, keepaspectratio]{res/thread-states.svg}
    \caption{Thread State Transitions Graph}
    \label{fig:thread-states}
\end{figure}

\restoregeometry

\subsubsection{Stacks}

Two distinct stacks are required for each thread: one for user space and one for kernel space. A separate kernel stack is essential for security and stability. Furthermore, sharing a single kernel stack is inefficient because context switches may occur within kernel code, such as during thread joining or other synchronization events.

\subsubsection{Statistics}

Statistics tracking allows for the analysis of execution specifics for each thread. This data is used to dynamically adjust the scheduling module and to debug or test the scheduler.

\subsubsection{Wait Queue}

Allows other threads to block and wait for a specific thread to finish execution (Thread joining).

\subsection{Kernel Workers}
\label{subsubsec:kworkers}

Before initializing the scheduling module, the operating system creates three essential kernel workers:

\textbf{Trace Dumper} -- responsible for dumping kernel traces to the terminal or storage files, ensuring debuggability and stability. This task cannot be performed directly by the kernel code during critical execution paths, as writing to devices or files is too slow for system calls or interrupt handlers. The buffers are dumped periodically every 20ms.

\textbf{Thread Ripper} -- responsible for the deallocation of thread resources, including descriptors and stacks.

\textbf{Process Ripper} -- responsible for the deallocation of process resources, including the address space.

\subsection{Intrusive Data Structures}

Since threads frequently migrate between different linked lists and data structures, allocating and freeing list nodes for every operation introduces significant overhead. To address this, the system employs intrusive data structures. Instead of allocating a node that contains a pointer to the object, the node structure is embedded directly within the object itself. Operations on the lists are performed by simply modifying the fields of the object.

\begin{lstlisting}[caption={Intrusive Nodes}, label={lst:intrusiveNodes}]
template <class T, int kIntrusiveLevel>
struct IntrusiveListNode {
    T *next;
};

template <class T, int kIntrusiveLevel>
struct IntrusiveDoubleListNode {
    T *next;
    T *prev;
};

template <class T, class KeyT, int kIntrusiveLevel>
struct IntrusiveRbNode {
    enum class Color : u8 {
        kBlack = 0,
        kRed   = 1,
    };

    T *parent;
    union {
        struct {
            T *left;
            T *right;
        };
        T *child[2];
    };

    Color color;
    KeyT key;
};
\end{lstlisting}

The Thread data structure \ref{lst:threadStruct} demonstrates the usage of such nodes. This approach offers multiple benefits, including:
\begin{itemize}
    \item No dynamic allocation or deallocation during insert or pop operations.
    \item Improved memory access patterns, the object is accessed directly rather than via a pointer from a separate node.
    \item Verification of whether an object belongs to a list in \textbf{O(1)} time, given only the object itself.
    \item Removal of the object from the list in \textbf{O(1)} time.
    \item Removal of the object from the list without requiring access to the list head structure.
\end{itemize}

\subsection{Meta-Scheduler}

The scheduler architecture follows the \textbf{meta-scheduler} design pattern, which is common in modern operating systems. This approach relies on abstracting scheduling logic into Policies. Policies are primarily responsible for selecting the next task from the set of tasks they manage and reacting to thread behavior (e.g., punishing or rewarding threads with CPU time based on workload characteristics). The scheduler itself is responsible for all other operations, including:
\begin{itemize}
    \item Managing thread states.
    \item Transitioning threads to sleep.
    \item Waking up threads.
    \item Managing idle time.
    \item Configuring next timing events via the timing infrastructure.
    \item Blocking threads on wait queues.
    \item Releasing threads from wait queues.
\end{itemize}
A hierarchy exists between policies, ensuring that threads from higher-priority policies are selected before threads from lower-priority ones.

\subsection{Timing Model}

The system implements a tickless kernel architecture. Instead of relying on a periodic interrupt (the \textbf{Kernel Tick}) at a fixed frequency, the scheduler calculates precisely when the next timing event must occur. This approach improves efficiency and precision.

\subsection{Policies}

The following scheduling policies have been implemented:

\textbf{Round Robin Scheduling Policy} -- simple policy that iterates through a linked list from front to back. Threads are ordered based on their arrival time in the policy.

\textbf{Priority Queue Scheduling Policy} -- policy that orders threads based on kernel-assigned priorities. Priorities are capped at a range of 0-64 to enable the use of an \textbf{O(1)} priority queue, known as a \textbf{Bitmap Priority Queue}. This structure maintains an array of linked lists representing individual priorities. Additionally, a bitmask indicates the presence of tasks at specific priority levels. Lookup operations utilize bitwise instructions (counting leading/trailing zeros) to efficiently find the minimum or maximum priority value (\textbf{O(1)}).

\textbf{Multi-Level Feedback Queue (MLFQ) Scheduling Policy} -- policy that segregates threads into distinct queues according to their workload characteristics. The policy penalizes CPU-bound threads by demoting them to lower priorities, whereas I/O-bound threads are elevated to higher priorities, thereby favoring interactive performance. To prevent the starvation of low-priority tasks, a periodic reset mechanism promotes all threads to the highest priority queue every 100ms. Within each queue, threads are managed using \textbf{Red-Black trees}, sorted by a key that combines user-defined priority and aggregated CPU burst times (the amount of time a thread spends executing on the CPU before it either completes, requires I/O operations, or is interrupted by the operating system). This ensures that each thread in the given queue receives a fair share of CPU time.

\noindent These policies establish a hierarchy defined by the following enumeration:

\begin{lstlisting}[caption={Scheduling Policy Hierarchy}, label={lst:policyHierarchy}]
// PO > P1 > .. > P4
enum SchedulingPolicy {
    kUberTask_PQ_P0 = 0,
    kDrivers_PQ_P1,
    kUrgentTasks_PQ_P2,
    kNormalTasks_MLFQ_P3,
    kBackgroundTasks_RR_P4,
    kLast,
};
\end{lstlisting}

Each policy is designated for a specific group of tasks:

\textbf{Uber Tasks} -- Tasks that must be executed as quickly as possible, such as emergency recovery actions or critical kernel state preservation (e.g., saving state before shutdown or terminating processes during out-of-memory conditions).

\textbf{Drivers} -- Driver tasks that require immediate execution to prevent device blocking and ensure smooth system operation (e.g., audio drivers or network interfaces).

\textbf{Urgent Tasks} -- Tasks that are less critical than drivers but more important than standard user tasks. These may include privileged user-space tasks that consume data from drivers.

\textbf{Normal Tasks} -- The most common group of tasks, comprising standard user-space programs such as graphical interfaces and data processing applications. Basic kernel workers, including those listed in the kernel workers section \ref{subsubsec:kworkers}, also belong to this category.

\textbf{Background Tasks} -- Tasks that are not time-critical and should only be executed when the system is otherwise idle. Examples include update checks or non-urgent cleanup operations.

\subsubsection{Policies Abstraction}
\label{subsec:policies}

The policy abstraction is defined as follows:

\begin{lstlisting}[caption={Policy Abstraction}, label={lst:policyStruct}]
struct Policy {
    struct {
        Thread *(*pick_next_task)(void *);

        void (*add_task)(void *, Thread *);
        void (*remove_task)(void *, Thread *);

        u64 (*get_preempt_time)(void *, Thread *);
        bool (*is_first_higher_priority)(void *, Thread *, Thread *);
        bool (*validate_flags)(void *, const ThreadFlags *);

        void (*on_thread_yield)(void *, Thread *);
        void (*on_periodic_update)(void *, u64 current_time_ns);
    } cbs;
    void *self;
};
\end{lstlisting}

As shown, the primary operation is \textbf{pick\_next\_task}, alongside standard \textbf{add\_task} and \textbf{remove\_task} operations. Additionally, helper functions are provided for the scheduler and for statistics gathering.

\subsection{Sleeping}

To provide high-precision sleeping, the system relies on one-shot, precise timing events. Upon receiving a timer interrupt, the scheduler determines the timestamp for the next event based on the system state, which includes the current thread preemption time and the wake-up times of sleeping threads. Consequently, the scheduler must inspect the sleeping queue to process expired events. To ensure high performance and stable time complexity, an Intrusive Red-Black Tree was selected as the underlying data structure for the \textbf{Sleeping Queue}. The critical operations are \textbf{Insert}, \textbf{ExtractMin}, and \textbf{Remove}, all of which must have predictable execution times. The requirement for efficient arbitrary removal excluded binary heaps, while the need for stability excluded heaps with amortized complexity bounds. The Red-Black Tree satisfied all requirements and was already implemented for other system components, making it the optimal choice.

% ==================================================================

\chapter{Results}
TODO KTOKOLWIEK

\section{User's Manual}

This section describes the usage of the AlkOS build environment and documents the public interfaces available to user-space applications. It outlines the build and execution workflow, the integration of user programs, and the system call interface provided by the operating system.

\subsection{Building and Running the OS}

AlkOS provides a command-line utility script, \texttt{alkos\_cli.bash}, located in the \texttt{scripts/} directory. This script automates the setup of the development environment, the compilation of the kernel, and the execution of the operating system within an emulator. The build infrastructure is based on CMake and requires a Linux host system.

Automated dependency installation is officially supported for Arch Linux and Ubuntu distributions. Nevertheless, the build process is expected to function on other Linux distributions, provided that the required dependencies are installed manually. Reference installation dependencies for supported environments can be found in the \texttt{scripts/env/} directory.

The build and execution workflow consists of three sequential stages:

\begin{enumerate}
    \item \textbf{Environment Initialization:}
    Prior to compilation, a custom cross-compilation toolchain and all required system dependencies must be installed. This can be achieved by executing:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --install all
    \end{verbatim}
    Alternatively, to install only the cross-compiler toolchain, the following command may be used:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --install toolchain
    \end{verbatim}

    \item \textbf{Configuration:}
    During the configuration phase, build configuration and kernel feature flags are generated and stored in the \texttt{config/} directory. The default configuration can be created using:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --configure
    \end{verbatim}
    For customized configurations, the following script is provided:
    \begin{verbatim}
    ./scripts/config/configure.bash <platform> <build_type> [options...]
    \end{verbatim}
    Here, \texttt{<platform>} denotes the target architecture (e.g., \texttt{x86\_64}), \texttt{<build\_type>} specifies either \texttt{Debug} or \texttt{Release}. A complete list of options can be obtained using the \texttt{\text{-}\text{-}help} flag.

    \item \textbf{Compilation and Execution:}
    In the final stage, the kernel and all registered user-space applications are compiled, the root filesystem image and bootable ISO are generated, and the operating system is launched using the QEMU emulator. This process can be initiated with the command:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --run
    \end{verbatim}
\end{enumerate}

In the event of errors occurring at any stage of the process, the scripts may be executed with the \texttt{\text{-}\text{-}verbose} flag, which enables detailed logging output to facilitate debugging.

\subsection{Developing Userspace Applications}

AlkOS supports user-space applications written in C and C++. These applications are integrated into the operating system at build time and included directly in the generated root filesystem.

\subsubsection{Project Structure}

User-space programs reside in the \texttt{userspace/programs/} directory. Each application is placed in its own subdirectory and must include all relevant source files along with a \texttt{CMakeLists.txt} configuration file. A minimal example of a user application is shown in Listing \ref{lst:minimalUserAppStructure}.

\begin{lstlisting}[caption={Minimal User Application Structure}, label={lst:minimalUserAppStructure}]
// userspace/programs/my_app/main.cpp
#include <stdio.h>

extern "C" int main() {
    printf("Hello from AlkOS User Space!\n");
    return 0;
}
\end{lstlisting}

\subsubsection{Build System Integration}

To include a user-space application in the build process, it must be registered with the CMake-based build system. AlkOS provides the helper macro \texttt{alkos\_register\_userspace\_app}, which automatically configures compiler flags and links the application against the custom C and C++ standard libraries (\texttt{libc} and \texttt{libc++}). An example registration is shown in Listing \ref{lst:userAppCMakeRegistration}.

{
    \renewcommand{\lstlistingname}{CMake Snippet}
    \begin{lstlisting}[style=cmakestyle, caption={User Application Registration}, label={lst:userAppCMakeRegistration}]
    # userspace/programs/my_app/CMakeLists.txt
    alkos_find_sources(MY_APP_SOURCES)
    alkos_register_userspace_app(my_app "${MY_APP_SOURCES}")
    \end{lstlisting}
}

After registration, the application is automatically compiled, linked, and placed in the \texttt{/bin} directory of the root filesystem during the next build.

\subsection{C Standard Library}

Instead of relying on an external standard library implementation, AlkOS provides a custom C standard library tailored to its kernel interface.

\subsubsection{Standard C Support}

The library includes implementations of commonly used standard headers such as \texttt{<stdio.h>}, \texttt{<stdlib.h>}, and \texttt{<string.h>}. This enables the compilation of conventional C programs with minimal adaptation. Standard file operations, including \texttt{fopen}, \texttt{fread}, and \texttt{fwrite}, are fully supported and internally routed through the VFS.

\subsubsection{AlkOS-Specific Extensions}

Certain low-level system interactions are beyond the scope of the standard C library. To expose kernel-specific functionality, AlkOS provides additional interfaces via the \texttt{<alkos/calls.h>} header. These interfaces enable access to hardware abstraction, threading primitives, and system control mechanisms. The most important extensions include:

\begin{itemize}
    \item \textbf{Input and Output Subsystem (\texttt{alkos/sys/video.h}, \texttt{alkos/sys/input.h}):}
    The \texttt{GetVideoBufferInfo} function maps the framebuffer into the process address space, allowing direct graphical output. Keyboard input is accessed via \texttt{GetKeyState}, which enables polling of individual keys.

    \item \textbf{Process Management, Threading, and Timing (\texttt{alkos/sys/thread.h}, \texttt{alkos/sys/time.h}):}
    Thread lifecycle management is provided through \texttt{ThreadCreate}, \texttt{ThreadJoin}, and \texttt{ThreadDetach}. High-precision timing services are available via \texttt{NanoSleep} and \texttt{NanoSleepUntil}. Process-level control is facilitated by \texttt{Exec} and \texttt{ProcExit}.

    \item \textbf{Power Management (\texttt{alkos/sys/power.h}, \texttt{alkos/sys/proc.h}):}
    System power states can be controlled using the \texttt{Shutdown} and \texttt{Reboot} interfaces, which invoke the underlying ACPI mechanisms.

    \item \textbf{Extended Filesystem Operations (\texttt{alkos/sys/fs/fs.h}):}
    While standard I/O functions support file access, directory enumeration and metadata queries require OS-specific calls. The \texttt{ReadDirectory} function enables directory traversal, and \texttt{FileInfo} provides information about filesystem objects.
\end{itemize}

\subsection{Syscalls}

The system call interface represents the controlled boundary between user-space applications executing in Ring 3 and the kernel executing in Ring 0. On the \texttt{x86\_64} architecture, AlkOS utilizes the \texttt{int 0x80} software interrupt mechanism to invoke system calls.

\subsubsection{Calling Convention}

The calling convention is largely inspired by the System V AMD64 ABI, with modifications to accommodate system call semantics. The register usage is summarized in Table \ref{tab:syscall_abi}.

\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Register} & \textbf{Purpose} \\ \hline
\texttt{RAX} & System call number (input) / return value (output) \\ \hline
\texttt{RDI} & Argument 1 \\ \hline
\texttt{RSI} & Argument 2 \\ \hline
\texttt{RDX} & Argument 3 \\ \hline
\texttt{R10} & Argument 4 \\ \hline
\texttt{R8} & Argument 5 \\ \hline
\texttt{R9} & Argument 6 \\ \hline
\end{tabular}
\caption{System Call Register Mapping on x86\_64}
\label{tab:syscall_abi}
\end{table}

\subsubsection{Provided System Services}

The available system calls are grouped into logical categories:

\paragraph{Process and Thread Management}
\begin{itemize}
    \item \texttt{exec(path)}: Loads and executes a program.
    \item \texttt{proc\_exit(status)} and \texttt{proc\_abort()}: Terminates the calling process.
    \item \texttt{kill(pid)} and \texttt{wait(pid)}: Manage inter-process control and synchronization.
    \item \texttt{thread\_create}, \texttt{thread\_exit}, \texttt{thread\_join}, \texttt{thread\_detach}: Thread management primitives.
    \item \texttt{get\_heap\_start()}: Returns the base address of the process heap.
\end{itemize}

\paragraph{File I/O and Filesystem Management}
\begin{itemize}
    \item \texttt{open}, \texttt{close}, \texttt{read}, \texttt{write}, \texttt{seek}: File descriptor operations.
    \item \texttt{dup(fd)} and \texttt{dup\_to(fd, newfd)}: File descriptor duplication.
    \item \texttt{read\_directory} and \texttt{file\_info}: Filesystem inspection.
    \item \texttt{create\_directory}, \texttt{delete\_file}, \texttt{move\_file}: Filesystem modification operations.
\end{itemize}

\paragraph{Graphics and Input}
\begin{itemize}
    \item \texttt{create\_graphic\_session(info)}: Initializes a graphical context.
    \item \texttt{blit()}: Requests composition of the application backbuffer.
    \item \texttt{get\_key\_state(vk)}: Retrieves the state of a virtual key.
\end{itemize}

\paragraph{Time and Synchronization}
\begin{itemize}
    \item \texttt{nanosleep(ns)} and \texttt{nanosleep\_until(sys\_ns)}: Suspends execution for a defined duration.
    \item \texttt{get\_clock\_value}: Returns the current value of a specified clock.
    \item \texttt{get\_timezone}: Retrieves the system time zone configuration.
\end{itemize}

\paragraph{System Control and Debugging}
\begin{itemize}
    \item \texttt{power(action)}: Controls system power states.
    \item \texttt{panic(msg)}: Terminates execution and records a diagnostic message.
\end{itemize}


\section{Example Programs}
TODO ADAM

% TODO: extension
% \section{Performance Analysis}
% \subsection{KMalloc Performance}
% \subsection{KFree Performance}
% \subsection{Context Switch Performance}
% \subsection{Syscall Performance}
% \subsection{Scheduler Tests}


\chapter{Future Work}
TODO KTOKOLWIEK

\chapter{Conclusion}
TODO KTOKOLWIEK

% ----------- BIBLIOGRAPHY ---------

\printbibliography[heading=bibintoc]

\pagenumbering{gobble}
\thispagestyle{empty}



% --------- LIST OF SYMBOLS AND ABBREVIATIONS ------
\chapter*{List of symbols and abbreviations}

\begin{tabular}{cl}
API & Application Programming Interface \\
APIC & Advanced Programmable Interrupt Controller \\
CRTP & Curiously Recurring Template Pattern \\
IDT & Interrupt Descriptor Table \\
IPC & Inter-Process Communication \\
ISR & Interrupt Service Routine \\
LAPIC & Local Advanced Programmable Interrupt Controller \\
LIT & Logical Interrupt Table \\
MLFQ & Multi-Level Feedback Queue \\
MMIO & Memory-Mapped I/O \\
MMU & Memory Management Unit \\
OS & Operating System \\
PIC & Programmable Interrupt Controller \\
PID & Process IDentifier \\
PIT & Programmable Interval Timer \\
RTC & Real-Time Clock \\
TID & Thread IDentifier \\
TSC & TimeStamp Counter \\
TSS & Task State Segment \\
VFS & Virtual File System \\
CFS & Completly Fair Scheduler \\
EEVDF & Earliest Eligible Virtual Deadline First \\
DMA & Direct Memory Access \\
PCP & Per-CPU Pagessets \\
COW & Copy On Write \\
SMP & Symmetric MultiProcessing \\
ACPI & Advanced Configuration and Power Interface \\
PCI & Peripheral Component Interconnect \\
USB & Universal Serial Bus \\
IPC & Inter Process Communication \\

\end{tabular}
\\
\thispagestyle{empty}


% ----------  LIST OF FIGURES ------------
\listoffigures
\thispagestyle{empty}

% -----------  LIST OF TABLES ------------
\renewcommand{\listtablename}{List of Tables}
\listoftables
\thispagestyle{empty}

\end{document}

% ==================================================================
% TODOS:
% - wytlumaczyc na starcie co zakladamy etc i dlaczego np segmenty
