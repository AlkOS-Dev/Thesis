\documentclass[a4paper,10pt,twoside]{report}

% --------   PREAMBLE PART ----------

% -------- ENCODING & LANGUAGES --------

\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage[polish, english]{babel}


\usepackage{amsmath, amsfonts, amsthm, latexsym}

\usepackage[final]{pdfpages}
\usepackage{csquotes}
\usepackage[backend=biber, style=numeric, sorting=none]{biblatex}
\addbibresource{references.bib}


\usepackage{commath}

\usepackage[hidelinks]{hyperref}

% ------ MARGINS, INDENTATION, LINESPREAD ------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}


\linespread{1.5}
\allowdisplaybreaks

\usepackage{indentfirst}
\setlength{\parindent}{5mm}


%------ RUNNING HEAD - CHAPTER NAMES, PAGE NUMBERS ETC. -------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[LE,RO]{\thepage} 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}

\renewcommand{\headrulewidth}{0 pt}


\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[LE,RO]{\thepage}
  
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.0pt}
}

%------ code listings -------

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstdefinestyle{cppstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=C++,
    morekeywords={constexpr, nullptr, size_t, uint64_t}
}

\lstdefinestyle{nasmstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=[x86masm]Assembler,
    morekeywords={rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8, r9, r10, r11, r12, r13, r14, r15,
                  eax, ebx, ecx, edx, esi, edi, ebp, esp,
                  cr0, cr2, cr3,
                  mov, push, pop, call, ret, int, iretq, jmp, je, jne, jg, jl, cmp, test,
                  add, sub, mul, div, inc, dec, xor, or, and,
                  lidt, lgdt, sti, cli, hlt,
                  section, global, extern, db, dw, dd, dq, resb, resw, resd, resq,
                  macro, endmacro, \%define}
}

\lstdefinestyle{cmakestyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=CMake,
    morekeywords={project, add_executable, add_library, target_link_libraries, set, include_directories,
                  cmake_minimum_required, file, macro, endmacro, foreach, endforeach, if, endif, else,
                  find_package, add_custom_command, add_custom_target, install, alkos_find_sources, alkos_register_userspace_app}
}

\lstnewenvironment{cppcode}[1][]
  {\renewcommand{\lstlistingname}{C++ Code Snippet}\lstset{style=cppstyle, #1}}
  {}

\lstnewenvironment{nasmcode}[1][]
  {\renewcommand{\lstlistingname}{Assembly Code Snippet}\lstset{style=nasmstyle, #1}}
  {}

\lstnewenvironment{cmakecode}[1][]
  {\renewcommand{\lstlistingname}{CMake Code Snippet}\lstset{style=cmakestyle, #1}}
  {}

% --------- DRAWING -------

\usepackage{tikz}
\usetikzlibrary{fit, backgrounds, shapes, arrows.meta, positioning, calc, chains}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[inkscapepath=../output/svg/]{svg}
\usepackage{graphicx}

% --------- LIST OF SYMBOLS AND ABBREVIATIONS ------

\usepackage{supertabular}

% --------- CHAPTER HEADERS -------

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 

    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% --------- TABLE OF CONTENTS SETUP ---------

\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}
  [0pt]
  {}
  {\bfseries \thecontentslabel.\quad}
  {\bfseries}
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% -------- TABLES AD FIGURES NUMBERING --------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}


% ----- DEFINING ENVIRONMENTS FOR THEOREMS, DEFINITIONS ETC. -----

\makeatletter
\newtheoremstyle{definition}
{3ex}
{3ex}
{\upshape}
{}
{\bfseries}
{.}
{.5em}
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% ------- END OF PREAMBLE PART (MOSTLY) ----------





% ---------- USER SETTINGS ---------

\newcommand{\tytul}{Funkcjonalne jądro systemu operacyjnego: AlkOS}
\renewcommand{\title}{From Bare Metal to a Functional Kernel: The AlkOS Operating System}
\newcommand{\type}{Engineer}
\newcommand{\supervisor}{mgr inż. Paweł Sobótka}



\begin{document}
\sloppy
\selectlanguage{english}

\includepdf[pages=-]{titlepage-en}

\null\thispagestyle{empty}\newpage

% ------ PAGE WITH SIGNATURES ------------

%\thispagestyle{empty}\newpage
%\null
%
%\vfill
%
%\begin{center}
%\begin{tabular}[t]{ccc}
%............................................. & \hspace*{100pt} & .............................................\\
%supervisor's signature & \hspace*{100pt} & author's signature
%\end{tabular}
%\end{center}
%


% ---------- ABSTRACT -----------

{  \fontsize{12}{14} \selectfont
\begin{abstract}

\begin{center}
\title
\end{center}

Operating system development constitutes one of the most challenging disciplines in computer science, requiring deep knowledge of hardware architecture and systems programming. This thesis presents the design and implementation of AlkOS, a monolithic general-purpose operating system kernel for the x86-64 architecture (through a hardware abstraction layer), developed from scratch using C++. The project addresses the scarcity of structured educational resources by serving both as a functional kernel implementation and a comprehensive guide to independent OS development.

Technically, AlkOS features a custom two-stage bootloader that transitions the CPU to 64-bit mode (Long Mode) and establishes a higher-half kernel environment. The system implements advanced memory management with paging and a physical memory manager, alongside a Virtual File System (VFS) with FAT support. The process scheduler utilizes a tickless, meta-scheduler architecture supporting multiple policies, including Round-Robin, Priority Queues, and Multi-Level Feedback Queues (MLFQ), ensuring efficient task management. The validation of the system's capabilities culminated in the successful execution of the Doom engine within a custom graphical shell, a task necessitating the implementation of a C/C++ standard library and a comprehensive system call interface. \\

\noindent \textbf{Keywords:} Operating System Development, Systems Programming, x86-64 Architecture, Bare Metal, Monolithic Kernel, Virtual Memory Management, Tickless Kernel, Preemptive Scheduling, Virtual File System, Demand Paging, Meta-Scheduler, Higher-Half Kernel, User Space.
\end{abstract}
}

\null\thispagestyle{empty}\newpage

{\selectlanguage{polish} \fontsize{12}{14}\selectfont
\begin{abstract}

\begin{center}
\tytul
\end{center}

Tworzenie systemów operacyjnych stanowi jedną z najtrudniejszych dziedzin informatyki, wymagającą głębokiej wiedzy na temat architektur sprzętowych i programowania systemowego. Niniejsza praca przedstawia projekt i implementację AlkOS, monolitycznego jądra systemu operacyjnego ogólnego przeznaczenia dla architektury x86-64 (poprzez warstwę abstrakcji sprzętowej), stworzonego od podstaw przy użyciu języka C++. Projekt odpowiada na niedobór ustrukturyzowanych materiałów edukacyjnych, służąc zarówno jako funkcjonalna implementacja jądra, jak i kompleksowy przewodnik po niezależnym tworzeniu systemów operacyjnych.

Od strony technicznej, AlkOS posiada własny dwuetapowy program rozruchowy (bootloader), który przełącza procesor w tryb 64-bitowy (Long Mode) i ustanawia środowisko jądra w wyższej połowie pamięci (higher-half kernel). System implementuje zaawansowane zarządzanie pamięcią ze stronicowaniem i menedżerem pamięci fizycznej, wraz z wirtualnym systemem plików (VFS) z obsługą FAT. Planista procesów wykorzystuje architekturę meta-dyspozytora bez cyklicznych przerwań (tickless), obsługującą wiele polityk, w tym algorytm karuzelowy (Round-Robin), kolejki priorytetowe i wielopoziomowe kolejki ze sprzężeniem zwrotnym (MLFQ), zapewniając efektywne zarządzanie zadaniami. Walidacja możliwości systemu zakończyła się pomyślnym uruchomieniem silnika gry Doom z własnej powłoki systemowej, co wymagało implementacji biblioteki standardowej C/C++ oraz obszernego interfejsu wywołań systemowych. \\

\noindent \textbf{Słowa kluczowe:} Tworzenie systemów operacyjnych, Programowanie systemowe, Architektura x86-64, Środowisko bez systemu operacyjnego, Jądro monolityczne, Zarządzanie pamięcią, Jądro niewymagające cyklicznych przerwań, Wywłaszczanie procesów, Wirtualny system plików, Stronicowanie na żądanie, Meta-dyspozytor, Jądro w wyższej połowie pamięci, Przestrzeń użytkownika.
\end{abstract}
}

%% --------- DECLARATIONS ------------
%
%%
%%	IT IS NECESSARY OT ATTACH FILLED-OUT AUTORSHIP DEECLRATION. SCAN (IN PDF FORMAT) NEEDS TO BE PLACED IN scans FOLDER AND IT SHOULD BE CALLED, FOR EXAMPLE, DECLARATION_OF_AUTORSHIP.PDF. IF THE FILENAME OR FILEPATH IS DIFFERENT, THE FILEPATH IN THE NEXT COMMAND HAS TO BE ADJUSTED ACCORDINGLY.
%%
%%	command attacging the declarations of autorship
%%
%\includepdf[pages=-]{scans/declaration-of-autorship}
%\null\thispagestyle{empty}\newpage
%
%% optional declaration
%%
%%	command attaching the declaataration on granting a license
%%
%\includepdf[pages=-]{scans/declaration-on-granting-a-license}
%%
%%	.tex corresponding to the above PDF files are present in the 3. declarations folder 
%
\null\thispagestyle{empty}\newpage
% ------- TABLE OF CONTENTS -------
\selectlanguage{english}
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\newpage % IF YOU HAVE EVEN QUANTITY OD PAGES OF TOC, THEN REMOVE IT OR ADD \null\newpage FOR DOUBLE BLANK PAGE BEFORE INTRODUCTION


% -------- THE BODY OF THE THESIS ------------

\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11}
\chapter{Introduction}
% \markboth{}{Introduction}
% \addcontentsline{toc}{chapter}{Introduction}

\section{Background and Context}

The creation of a functional operating system kernel stands as one of the most complex tasks in software engineering. It may implement all and more, a subset of, a combination, or the bare minimum of what will be discussed in this work in the following chapters, but the ultimate goal remains similar --- to make more of the machine than running a simple program.

Perhaps it's to abstract away the underlying hardware and achieve portability of programs between different machines. Most likely, it's the enablement of parallel execution of a different set of programs. Perhaps even the programs can't be trusted --- they must be run as if they existed in isolated environments, unaware of each other and incapable of interfering in each other's work. 

If that were to be the case, certainly they'd require a method of receiving parts of memory to use, of compute time, of whatever resources the machine has at its disposal - cameras, microphones, TPU's, GPU's, NIC's or custom chipsets. If the programs had to access these resources, then the OS would need to provision them, to manage them, and, for that reason, bookkeep and gatekeep them. If a set of programs wanted to communicate with each other, it'd need to facilitate that communication. The OS that has to do it becomes, in consequence, the middleman and the orchestrator of the programs, as well as the master of the machine.

One can imagine that errors in the master of the machine would be quite catastrophic for every operation running on the machine. The impact: evident in every single program. Mere slowness of execution is quite the favorable scenario here --- a total and utter corruption of the system's state and its following crash --- or, in more extreme circumstances --- the damaging of the hardware itself --- are all possible cases. The most insidious being minor bugs that are subtle enough to remain untouched, yet severe enough to impact the outcome of the program, perhaps affecting someone's life in the process, if the system happened to be responsible for a matter of such importance. 

The creation of an OS is therefore not a trivial matter. It requires a precise understanding of the machine, its hardware, its resources, and a very meticulous approach to creating and testing it. It needs to be fast --- the less time it uses, the more time the other programs have to execute. At the same time, it needs to be precise --- resources leaked and unrecovered accumulate over time --- creating a machine that's incapable of running for longer periods of time. 

\subsection{Motivation}

It is precisely because of the aforementioned complexities that the implementation of an operating system stands as such a uniquely rewarding endeavor. Developing this "apex software" --- deconstructing it into its fundamental components and untangling the intricate problems it manages --- demands adherence to the highest standards of software engineering. One might ask why an individual would undertake the creation of an operating system from scratch as a hobby, driven neither by financial incentive nor professional obligation. The answer may be best captured by President John F. Kennedy's address at Rice University:

\begin{quote}
"But why, some say, the moon? Why choose this as our goal? And they may well ask why climb the highest mountain? Why, 35 years ago, fly the Atlantic? [...] We choose to go to the moon. We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard, because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one which we intend to win."
\end{quote}

\subsection{Scale}

Given the magnitude of the task, operating system development is predominantly an enterprise-level initiative involving large teams of seasoned professionals. Independent or hobbyist operating systems, developed by individuals or small groups, are exceptionally rare.

To put this scarcity into perspective, consider the software landscape. The number of PC games is estimated in the tens of thousands, with over 121,000 available on Steam alone as of early 2026. Mobile applications number in the millions, with approximately 2.18 million on the Google Play Store \cite{stats_google_play}. The web is even more vast, with the total number of websites estimated at 1.34 billion \cite{stats_websites}. All of these software artifacts rely on the operating system as their host.

In contrast, the number of operating systems is orders of magnitude smaller. While exact figures are difficult to determine due to the historical prevalence of proprietary mainframe systems, current estimates are telling. The number of active open-source operating systems --- the majority being forks of Linux and BSD --- is tracked at around 900 \cite{stats_distrowatch}. Wikipedia lists approximately 300 notable proprietary and historical operating systems \cite{wiki_list_os}, and there are roughly 100 to 150 market-leading real-time operating systems (RTOS) \cite{wiki_rtos}. As for hobbyist kernels created from scratch? The OSDev wiki lists only about 175 active projects \cite{osdev_projects}.

The creation of a functional kernel from scratch by a team of three students is, therefore, a statistically significant undertaking.

\subsection{Difficulties}

The primary challenge in independent operating system development is the scarcity of high-quality educational materials. This is a direct consequence of the scale of the field; the community is simply too small to sustain a paved learning path. The OSDev Wiki \cite{osdev_main} stands as the central resource for hobbyist developers, yet without it, the task would be nearly impossible. Creating an operating system requires knowledge not just of how the hardware is constructed (as detailed in manuals), but of how to effectively drive it --- a distinction akin to knowing how a car is built versus knowing how to drive it at a professional level.

While the OSDev community is invaluable, its resources often suffer from a lack of depth or coherence. Many articles convey high-level ideas but lack specific implementation details, while others are outdated or incorrect. Tutorials frequently stop shortly after the operating system equivalent of the "Hello World" stage --- establishing a minimal bootable kernel that prints to the screen --- leaving the developer to bridge the massive gap to a fully functional system alone.

Studying existing kernels like Linux or MINIX is often suggested, yet this proves exceptionally difficult for beginners. Without a foundational understanding of the problem space --- why specific components exist, what problems they solve, and how they interact --- reading production kernel code becomes an exercise in reverse engineering rather than learning. There is a distinct absence of a "gentle slope"; one is often faced with a binary choice: knowing nothing, or needing to know everything simultaneously.

Beyond the educational barrier lie the technical hurdles: implementing device drivers, creating memory management algorithms, and designing task schedulers. However, for the uninitiated, the lack of a structured theoretical framework remains the most formidable obstacle.

This thesis aims to address both aspects. It documents the technical creation of the AlkOS kernel --- our design choices, the problems we encountered, and the solutions we engineered (detailed in Chapters \ref{chap:low_level_implementation} and \ref{chap:high_level_subsystems}, with corresponding source code attached to this thesis). Simultaneously, it serves as a guide to the problem space itself (specifically in Chapter \ref{chap:os_from_scratch}), intended to assist future developers in their attempt to create an operating system from scratch.

\section{Scope of the Thesis}
\label{subsec:scope}

The primary objective of this project, as initially defined, was to implement a functional scheduler capable of executing tasks within isolated address spaces. Specifically, the project aimed to achieve the following minimal technical goals:

\begin{itemize} 
    \item Bootloading --- the ability to start the system on bare metal hardware. 
    \item Monolithic design --- implementing the kernel within a single address space. 
    \item Virtual memory support --- implementation of memory management to allocate isolated space for tasks. 
    \item Preemptive scheduling --- the ability to interrupt a running task to switch context. 
    \item General-purpose scheduling --- designing a scheduler suitable for generic workloads. 
    \item Implementation of a scheduling algorithm --- at least something as simple as Round Robin. 
    \item x86-64 Architecture --- targeting the x86-64 architecture only.
\end{itemize}

At the outset of the project, specific requirements were not rigidly defined due to the uncertainty regarding the complexity and effort required for the implementation. It was anticipated that the challenge would be significant, necessitating extensive research, quality assurance, and the development of components from scratch. Driven by a strong interest in the subject matter, development started over a year ago with the objective of implementing as comprehensive a system as time constraints would permit. Ultimately, the scope of the realized system significantly exceeds the initial assumptions. Subsequent chapters describe the complete theory and implementation details, which extend beyond the original scope to include a complex scheduler, file systems, a robust user space environment, and ported software, specifically the Doom game.

\section{Work Division}

\paragraph{Jakub Lisowski}

\paragraph{Jakub Lisowski}
\begin{itemize}
    \item \textbf{Thesis Documentation and Analysis:}
        \begin{itemize}
            \item Section \ref{subsec:scope} --- Scope of the Thesis.
            \item Section \ref{subsec:intrusive} --- Intrusive Data Structures.
            \item Section \ref{sec:comms} --- Establishment of Basic Communication.
            \item Section \ref{subsec:os-tutorial-interrupts} --- Enabling Interrupts and Exceptions.
            \item Section \ref{sec:tracing} --- Tracing System.
            \item Section \ref{sec:discovery} --- Discovering External Devices and System Capabilities.
            \item Section \ref{sec:sched-tutorial} --- Scheduling Theory.
            \item Section \ref{sec:linux-research} --- Linux Research and Analysis.
            \item Section \ref{sec:interrupts} --- Interrupt Implementation.
            \item Section \ref{sec:timing} --- Timing Implementation.
            \item Section \ref{sec:scheduling-impl} --- Scheduling Implementation.
            \item Section \ref{sec:results} --- Results and Conclusions.
        \end{itemize}

    \item \textbf{Kernel Subsystems:}
        \begin{itemize}
            \item \textbf{Scheduling:} Design of the Meta-scheduler, thread/process lifecycle management, and core task structures.
            \item \textbf{Timing:} High-precision timing subsystem, sleep mechanisms, and hardware drivers (TSC, HPET, LAPIC Timer, RTC).
            \item \textbf{Interrupts:} Interrupt subsystem architecture and hardware drivers (APIC, PIC).
            \item \textbf{Observability:} Design and implementation of the kernel tracing and logging subsystem.
        \end{itemize}

    \item \textbf{Data Structures and Standard Library:}
        \begin{itemize}
            \item Implementation of the C Standard Library (\texttt{libc}) components: \texttt{time.h}, standard asserts, and extended diagnostic asserts.
            \item Design of core intrusive data structures: doubly linked lists and Red-Black trees.
            \item Implementation of specialized Hash Map structures.
        \end{itemize}

    \item \textbf{Tooling and Quality Assurance:}
        \begin{itemize}
            \item Development of the unit testing framework and automated regression tests.
            \item Development of the project's automation infrastructure (majority of Bash scripts under \texttt{./scripts}).
        \end{itemize}
\end{itemize}

\paragraph{Adam Ogieniewski}

\begin{itemize} 
    \item opa
\end{itemize}

\paragraph{Łukasz Kryczka}

\begin{itemize} 
    \item opa
\end{itemize}


% ==================================================================

\chapter{Creating an Operating System from Scratch}
\label{chap:os_from_scratch}

This chapter outlines the development sequence of the kernel's most critical components, upon which the majority of the subsequent code depends. The proposed progression is derived from the experience gained and materials consulted during the development process. While this is not the only valid approach to system development, it represents a structured path designed to minimize redundant discovery and facilitate implementation. The discussion focuses exclusively on the general-purpose monolithic kernel design with memory virtualization, where every module resides within the same address space --- the kernel address space. Given that our work was primarily conducted on x86-64 machines, we will use this architecture as the basis for further discussion.

\section{Host Environment Preparation}

Before proceeding to write actual kernel code, we must first do some preparation on our development tools and development environments. Below we can find a brief description of the needed things.

\subsection{Cross-Compilation Toolchain}
\label{subsec:theory_toolchain}

The development process begins with the establishment of a cross-compilation toolchain targeting a generic, OS-independent architecture (e.g., \texttt{x86-64-elf}). Using a compiler provided by the host system may lead to issues, as such compilers typically assume that the generated code will execute within an operating system environment. These assumptions are invalid during development, where no such infrastructure exists initially.

For this reason, the cross-compiler is built from source, allowing it to be configured and, if necessary, patched to accommodate the specific requirements of kernel and user space development. A complete cross-compilation toolchain typically consists of a cross-compiler (e.g., GCC or Clang), a suite of binary utilities (including the assembler and linker), and optionally a cross-debugger such as GDB or LLDB. Although any suitable cross-compiler may be used to build an operating system, GCC is generally preferred due to extensive support within the operating systems development community. For a detailed guide to building a GCC-based cross-compilation toolchain, refer to \cite{osdev-gcc-cross-compiler}.

At later stages of development, particularly when building user space applications, an extended version of the toolchain is required. This toolchain must be capable of automatically linking against our C standard library and including system headers. The process of adapting the toolchain to recognize the new operating system as a valid target is discussed in Subsection~\ref{subsubsec:os-specific-toolchain}.

Two distinct build environments are used throughout the system's development. In a \textbf{hosted} environment, programs are compiled with the expectation that a mature operating system is present, providing runtime services such as a program startup routine, system call interface, and system libraries. In contrast, a \textbf{freestanding} environment makes no assumptions about the existence of such, requiring all runtime support to be provided explicitly by the program or the operating system itself.

Kernel components and early system initialization code are built in a freestanding environment, as they execute before any operating system services are available. User space programs, on the other hand, are compiled in a hosted environment once the operating system and its runtime infrastructure have been established. The required runtime libraries are provided by the operating system (see Subsection~\ref{subsec:theory_std}).

\subsection{Building Machinery}

The construction of an operating system kernel requires the coordinated build of components, including loaders, the kernel, third-party libraries, and user space applications. Managing this through manual compiler invocations or ad-hoc shell scripts does not scale and frequently results in maintenance issues. Consequently, the design of a robust build infrastructure is essential for efficient development. We outline several key considerations for establishing such a system.

\subsubsection{Selection of Build Tools}

Build automation tools can be broadly categorized according to their level of abstraction. Low-level build systems (e.g., Make or Ninja) provide fine-grained control over compilation rules and dependency graphs. While this approach offers flexibility, it often becomes difficult to maintain as project complexity grows.

Meta-build systems, by contrast, operate at a higher level of abstraction. They allow developers to describe build targets, dependencies, and configuration options declaratively, and subsequently generate the necessary low-level build files for a chosen backend. For OS development, meta-build systems are generally preferred due to their scalability and maintainability. Examples of popular meta-build systems include CMake, Meson, and Premake.

\subsubsection{Toolchain Integration}

As required by the freestanding development model described in Section \ref{subsec:theory_toolchain}, the build system must be configured to use the previously established cross-compilation toolchain. This is typically achieved through the use of \textbf{toolchain configuration files}, which act as an adapter layer. 

A toolchain configuration file must fulfill two primary functions:
\begin{enumerate}
\item \textbf{System override:} It must define the target operating system as "Generic" (or an equivalent bare-metal designation). This prevents the build system from attempting to link against the host's standard libraries or searching the host's system paths for headers.
\item \textbf{Binary mapping:} It maps abstract compiler roles (e.g., C Compiler, Assembler, Linker) to the absolute paths of the specific cross-toolchain binaries compiled during the environment preparation phase.
\end{enumerate}

This architectural decoupling ensures that the build logic remains agnostic of the underlying hardware. Retargeting the operating system to a new architecture (e.g., transitioning from \texttt{x86-64} to \texttt{aarch64}) entails only the substitution of the active toolchain file, without requiring modifications to the project's build scripts or source code structure.

\subsubsection{Configuration Management}

Operating systems frequently require compile-time configuration to enable, disable, or parametrize subsystems such as debugging facilities or experimental scheduling policies. As the project evolves, configuration options may become duplicated across multiple layers of the system, leading to inconsistencies and maintenance challenges.

To avoid this issue, the build system should enforce a \textbf{single source of truth} (SSOT) for configuration data. One effective strategy is to define the configuration schema using a structured data format such as YAML, JSON, or TOML. A generation step then can derive three synchronized artifacts from this schema:
\begin{enumerate}
    \item \textbf{Source Header File:} Defines compile-time constants or definitions that allow the kernel source code to adapt its behavior based on the active configuration.
    \item \textbf{Build System Configuration:} Provides variables and conditions that control which source files, libraries, or features are included during compilation.
    \item \textbf{Scripts Configuration:} Supplies runtime scripts or tools with the necessary parameters to operate in accordance with the build configuration.
\end{enumerate}

\subsubsection{Compilation Profiles}

The build system must support compilation profiles to accommodate different phases of the development lifecycle. These profiles are typically expressed as predefined sets of compiler and linker flags:

\begin{itemize}
    \item \textbf{Debug Profile:} Optimized for observability and correctness. Compiler optimizations are generally disabled to preserve a close correspondence between source code and generated machine instructions, simplifying debugging. Debug symbols are enabled, and additional safety mechanisms such as stack protection or sanitizers may be included to detect errors early.
    
    \item \textbf{Release Profile:} Optimized for performance and efficiency. Higher optimization levels are enabled to increase performance and reduce code size. However, optimizations must be carefully selected in a kernel context. For instance, on x86-64 systems, automatic vectorization should be disabled, as the uncontrolled use of SIMD operations may lead to execution attempts before they are properly enabled.
\end{itemize}

In addition to general optimization settings, architecture-specific constraints must be enforced explicitly. On x86-64, the System~V ABI permits the use of a so-called \textbf{red zone}, a fixed region below the stack pointer that leaf functions (functions that do not call other functions) may use without adjusting the stack pointer. In kernel mode, interrupts may overwrite this region, leading to memory corruption. As a result, the red zone must be explicitly disabled.

\subsection{Emulation}

It is essential to establish an efficient testing strategy. In this context, hardware emulation becomes a critical tool. Two primary approaches exist for testing kernel software. The first involves utilizing a dedicated physical test machine, where the system image is flashed and booted for each iteration. While verifying kernel behavior on real hardware is strictly necessary to guarantee correctness, this process is time-consuming and often becomes a bottleneck during rapid development cycles. To address this efficiency issue, various emulation solutions are available, including QEMU \cite{qemu_website}, Bochs (x86 only) \cite{bochs_website}, and VirtualBox \cite{virtualbox_website}. These tools allow for the specification of the target machine architecture, enabling the kernel to run locally within the host system in an emulated environment. This setup significantly accelerates the workflow by facilitating quick execution, debugging, and state exploration. Consequently, the ideal solution combines both methods to maximize development speed while ensuring software quality.


\section{Implementation of the Standard Library}
\label{subsec:theory_std}

A C standard library, commonly referred to as \texttt{libc}, is a collection of pre-written code that provides a set of fundamental functionalities to user space applications. These functionalities include, but are not limited to, input/output operations, string manipulation, memory management, mathematical computations, and system call wrappers. The standard library serves as an intermediary layer between user applications and the underlying operating system, abstracting away low-level details and providing a consistent programming interface.

\subsubsection{Language and Interface}

Although historically associated with the C programming language, the implementation of a standard library is not inherently tied to C. Modern systems programming languages may be used to implement the library's internal logic, provided that a C-compatible Application Binary Interface (ABI) is exposed. Most high-level languages support interoperability with C, so we can leverage the advanced language features to implement the standard library without sacrificing compatibility with C programs.

\subsubsection{Development Methodologies}

When developing an operating system, two principal approaches exist for providing a standard library: using an existing implementation or developing one from scratch.

\paragraph{Using Existing Libraries}

Several open-source \texttt{libc} implementations are explicitly designed for portability. Notable examples include \textbf{Newlib}\cite{newlib_home}, which is widely adopted in embedded systems, and \textbf{mlibc}\cite{mlibc_repo}, which targets hobbyist operating systems. Integrating such libraries typically involves the implementation of required syscalls. The kernel developer supplies backend implementations for a set of low-level system calls, while the library itself provides higher-level functionality such as formatted output or mathematical routines. This approach substantially reduces development effort and enables the early execution of complex user space software.

\paragraph{Implementation from Scratch}

Alternatively, the standard library may be developed specifically for the operating system. This approach offers full control over memory usage, performance characteristics, and integration with kernel-specific features. However, it introduces significant complexity. Comparative studies of existing C standard libraries demonstrate substantial variation in performance, memory footprint, and standards compliance, even among mature implementations \cite{libc_comparison}. Achieving conformance with the language standard requires careful handling of numerous corner cases, including floating-point behavior, complex string formatting semantics, and locale support. Even minor deviations from the specified behavior can lead to subtle incompatibilities when porting third-party software.

\subsubsection{User Space vs. Kernel Space Variants}

Standard library functionality is required in both user space and kernel space. However, these environments impose fundamentally different constraints. User space code executes without privileges and relies on system call interface, whereas kernel code executes in a privileged context, invokes kernel services directly rather than through system calls, and must comply with constraints imposed by the kernel execution environment, such as interrupt safety. As a result, a single, uniform library implementation is insufficient.

To address this, the library is split into a user space variant (\texttt{libc}) and a kernel variant (\texttt{libk}), each compiled with different configuration flags and assumptions. This separation enforces correct usage and prevents kernel code from accidentally relying on functionality that is unavailable or unsafe in kernel context.

\subsubsection{Program Initialization and the C Runtime (CRT)}
\label{subsubsec:theory_crt}

In addition to exposing user-facing APIs, the runtime environment must be initialized before control is transferred to the program's entry function. User space programs do not begin execution at \texttt{main}. Instead, execution starts at a library-provided entry point, conventionally named \texttt{\_start}, which is supplied by a startup object file \cite{osdev_crt}.

This initialization sequence is realized through coordinated interaction between the linker, compiler, and standard library, and typically involves a set of well-defined object files:

\begin{itemize}
    \item \textbf{\texttt{crt0.o} (C Runtime 0):} Defines the \texttt{\_start} symbol and performs early initialization tasks such as stack alignment, optional setup of stack protection mechanisms, and preparation of program arguments before invoking \texttt{main}. Upon return, it transfers control to \texttt{exit}.
    
    \item \textbf{\texttt{crti.o} and \texttt{crtn.o}:} Provide the prologue and epilogue for the \texttt{.init} and \texttt{.fini} sections, respectively. Code in \texttt{.init} executes before \texttt{main}, while code in \texttt{.fini} executes after program termination.
    
    \item \textbf{\texttt{crtbegin.o} and \texttt{crtend.o}:} Supplied by the compiler toolchain, these objects manage registration of global constructors and destructors.
\end{itemize}

The overall control flow of the C runtime initialization and termination process is illustrated in Figure~\ref{fig:crt-init-flow}.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance=0.8cm, % Adjusted distance for better fit
        % Standard block style
        block/.style={
            rectangle, 
            draw=black!70, 
            rounded corners=3pt, 
            minimum width=6cm, 
            minimum height=0.9cm, 
            align=center, 
            fill=gray!10,
            font=\small
        },
        % Highlight user code differently
        userblock/.style={
            block,
            fill=white,
            thick
        },
        arrow/.style={-Latex, thick},
        label_text/.style={font=\scriptsize\ttfamily}
    ]

        % 1. Entry from Kernel
        \node (loader) [block, fill=gray!30] {\textbf{Process Creation}};

        % 2. Entry Point
        \node (start) [block, below=of loader] {\texttt{\_start}\\ \scriptsize (\texttt{crt0.o})};

        % 3. Initialization Phase
        \node (init) [block, below=of start] {\texttt{.init} section prologue\\ \scriptsize (\texttt{crti.o})};
        \node (ctors) [block, below=of init] {Global Constructors\\ \scriptsize (\texttt{crtbegin.o})};

        % 4. User Main
        \node (main) [userblock, below=of ctors] {\textbf{\texttt{main()}}\\ \scriptsize (User Application)};

        % 5. Exit Trigger
        \node (exit_func) [block, below=of main] {\texttt{exit()}\\ \scriptsize (\texttt{libc})};

        % 6. Termination Phase
        \node (dtors) [block, below=of exit_func] {Global Destructors\\ \scriptsize (\texttt{crtend.o})};
        \node (fini) [block, below=of dtors] {\texttt{.fini} section epilogue\\ \scriptsize (\texttt{crtn.o})};

        % 7. System Exit
        \node (syscall) [block, below=of fini, fill=gray!30] {\textbf{Process Termination}\\ \scriptsize (Syscall)};

        % Arrows
        \draw[arrow] (loader) -- (start);
        \draw[arrow] (start) -- (init);
        \draw[arrow] (init) -- (ctors);
        \draw[arrow] (ctors) -- (main);
        \draw[arrow] (main) -- (exit_func);
        \draw[arrow] (exit_func) -- (dtors);
        \draw[arrow] (dtors) -- (fini);
        \draw[arrow] (fini) -- (syscall);

        % Annotations (Right side brackets/text)
        \node [right=0.2cm of init, font=\scriptsize, align=left, color=gray!80!black] {Initialization\\(pre-main)};
        \node [right=0.2cm of dtors, font=\scriptsize, align=left, color=gray!80!black] {Finalization\\(post-main)};

    \end{tikzpicture}
    \caption{Control flow of C runtime (CRT) initialization and termination}
    \label{fig:crt-init-flow}
\end{figure}

Failure to link these components in the correct order results in an incomplete runtime environment, in which global constructors are not executed, and program behavior becomes undefined - particularly for C++ applications or code relying on constructor or destructor attributes.

\section{Bootloader}
\label{subsec:theory_bootloader}

The process of bringing a computer from a powered-off state to a fully functional operating system is governed by a rigid chain of physical and logical constraints. At the hardware level, the Central Processing Unit (CPU) functions as a complex state machine. Upon the application of power or a reset signal, the CPU resets its internal registers to default values and sets the Instruction Pointer to a specific, hardcoded physical address known as the \textit{Reset Vector} \cite{IntelManual-Reset}.

\subsection{The Memory Paradox and Storage}
A fundamental challenge in this sequence is the source of the initial instructions. The standard Random Access Memory (RAM), which serves as the primary workspace for modern operating systems, is volatile. It requires active electrical flow to maintain its state. When the system is powered off, the state is lost. Upon power-up, the memory cells contain random garbage data. Consequently, the CPU cannot fetch valid instructions from standard RAM immediately after a reset.

To resolve this, hardware architects map the Reset Vector address to a non-volatile memory region, typically Flash Memory or Read-Only Memory (ROM), which retains data without power.

\subsection{Embedded vs. Complex Architectures}
In simple embedded architectures (e.g., microcontrollers used in household appliances like washing machines or microwaves), the entire application code is often stored in this non-volatile memory. The memory controller maps this storage directly into the CPU's addressable space. This technique, known as \textbf{Execute In Place (XIP)}, allows the CPU to fetch and execute the developer's code from the very first clock cycle \cite{ARM-CortexM4-Generic-User-Guide}. The developer "owns" the machine from the first nanosecond.

In contrast, more complex architectures (such as ARM-based smartphones or single-board computers like the Raspberry Pi) often store the main operating system on external, complex storage media like SD cards or eMMC chips. The CPU cannot simply memory-map an SD card. It requires a sophisticated software driver to communicate with the storage controller. To bridge this gap, manufacturers embed a tiny, immutable piece of software called the \textbf{BootROM} directly into the silicon. This code initializes the minimal required hardware (often internal SRAM) and loads a secondary bootloader from the external storage into that SRAM, which in turn loads the main software.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    % Standard block style
    block/.style={
        rectangle, 
        draw=black!70, 
        rounded corners=3pt, 
        minimum width=2.2cm, 
        minimum height=1cm, 
        align=center, 
        fill=white,
        font=\small
    },
    % Style for the "Silicon" container
    soc/.style={
        rectangle,
        draw=black!40,
        dashed,
        fill=gray!5,
        rounded corners=5pt,
        inner sep=0.5cm
    },
    % Arrow styles
    arrow/.style={-Latex, thick, color=black!80},
    data/.style={-Latex, thick, dashed, color=black!80},
    % Label style for arrows
    lbl/.style={
        font=\footnotesize\bfseries, 
        fill=white, 
        inner sep=1pt,
        text=black!80
    }
]

% =========================================================
% LEFT SIDE: SIMPLE XIP
% =========================================================

\node (cpu1) [block, fill=blue!10] {CPU};
\node (flash) [block, below=1.5cm of cpu1, fill=orange!10] {NOR Flash\\(Memory Mapped)};
\node (label1) [above=0.1cm of cpu1, font=\bfseries] {Simple (XIP)};

% Arrow
\draw[arrow] (cpu1) -- node[midway, right, font=\footnotesize] {Direct Fetch} (flash);


% =========================================================
% RIGHT SIDE: COMPLEX BOOT
% =========================================================

% We place CPU2 to the right
\node (cpu2) [block, right=6cm of cpu1, fill=blue!10] {CPU};

% Define relative positions for BootROM and SRAM relative to CPU2
\node (bootrom) [block, below left=1.2cm and -0.5cm of cpu2, fill=gray!20] {BootROM\\(Immutable)};
\node (sram) [block, below right=1.2cm and -0.5cm of cpu2, fill=green!10] {Internal\\SRAM};

% Draw the SoC Boundary around them
\begin{scope}[on background layer]
    \node (soc_box) [soc, fit=(cpu2) (bootrom) (sram), label={[anchor=south west, inner sep=5pt]north west:\tiny System on Chip (SoC)}] {};
\end{scope}

% External Storage below the SoC
\node (sdcard) [block, below=1.0cm of soc_box, fill=orange!10] {SD Card / Disk\\(External)};
\node (label2) [above=0.5cm of soc_box, font=\bfseries] {Complex (Bootload)};


% =========================================================
% ARROWS & FLOW (Complex)
% =========================================================

% 1. Power On -> BootROM
\draw[arrow] (cpu2) -- node[lbl, pos=0.6] {1. Init} (bootrom);

% 2. BootROM -> External
\draw[arrow] (bootrom) -- node[lbl, pos=0.4] {2. Load} (sdcard);

% 3. External -> SRAM (Data copy)
\draw[data] (sdcard) -- node[lbl, pos=0.4] {3. Copy} (sram);

% 4. CPU -> SRAM (Execution)
\draw[arrow] (cpu2) -- node[lbl, pos=0.6] {4. Jump} (sram);

\end{tikzpicture}
\caption{Comparison of Boot Architectures}
\label{fig:boot_methods}
\end{figure}

On modern "heavy" machines, such as x86-64 workstations, the initialization process is exponentially more complex. The main CPU is fragile and dependent on a specific environment to function. Before the main cores can execute a single instruction, the hardware requires:
\begin{enumerate}
    \item \textbf{Power Sequencing:} Multiple voltage rails (Vcore, VccSA, VccIO) must be brought up in a specific order with millisecond-precision timing.
    \item \textbf{Clock Stabilization:} Phase-Locked Loops (PLLs) must be tuned and stabilized to generate the gigahertz-range frequencies required by the cores.
    \item \textbf{DRAM Training:} Modern DDR4/DDR5 memory requires a complex calibration process to align signal timing before it becomes usable.
\end{enumerate}

To manage this, modern chipsets often include a smaller, dedicated processor (e.g., the Intel Management Engine or AMD Platform Security Processor) that starts before the main CPU. This co-processor initializes the platform hardware to a state where the main CPU can begin execution \cite{Intel-Datasheet-Vol1}. 

\subsection{The Chain of Trust and Abstraction}
\label{subsubsec:chain_of_trust_and_abstraction}
By the time a kernel begins execution, it is likely the fourth or fifth program in the boot chain. The entity responsible for defining the interface between the hardware and the OS is the \textbf{System Firmware} \cite{UEFI-Base-Spec, UEFI-PI-Spec}.

The firmware's responsibility is to abstract the diverse implementations of different motherboards (e.g., how the disk controller is wired) and provide a mechanism to load an OS from a disk into RAM. However, relying solely on firmware is often insufficient for a portable operating system:
\begin{itemize}
    \item \textbf{Inconsistent State:} Different firmware implementations may leave the CPU in varying states (e.g., different interrupt configurations or privilege modes).
    \item \textbf{Feature Limitations:} Firmware is designed to be simple and compatible, often leaving the CPU in a conservative, low-feature mode with caches or advanced vector units disabled.
    \item \textbf{Interface Variance:} The method used to retrieve a memory map or video configuration can vary wildly between hardware generations.
\end{itemize}

To solve this, a \textbf{Third-Party Bootloader} is often utilized. This program acts as a "Normalizer" \cite{Limine-Spec}. It knows how to talk to various firmware types and storage devices. Its job is to abstract away the firmware differences, load the kernel file into memory, and pass control to the OS in a unified, predictable manner.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1cm,
    stage/.style={rectangle, draw, fill=white, text width=4cm, align=center, minimum height=1cm},
    arrow/.style={-Latex, thick}
]

\node (power) [stage, fill=gray!10] {\textbf{Hardware Power-On}\\(Reset Vector)};
\node (firmware) [stage, below=of power] {\textbf{System Firmware}\\(BIOS / UEFI)};
\node (loader) [stage, below=of firmware] {\textbf{Bootloader}\\(GRUB / Limine)};
\node (trampoline) [stage, below=of loader] {\textbf{OS Trampoline}\\(Arch Specific)};
\node (kernel) [stage, below=of trampoline, fill=gray!30] {\textbf{Kernel Main}\\(Arch Agnostic)};

\draw[arrow] (power) -- node[right, font=\footnotesize] {Init Platform} (firmware);
\draw[arrow] (firmware) -- node[right, font=\footnotesize] {Load from Disk} (loader);
\draw[arrow] (loader) -- node[right, font=\footnotesize] {Normalize State} (trampoline);
\draw[arrow] (trampoline) -- node[right, font=\footnotesize] {Enable Features (AVX, Paging)} (kernel);

\end{tikzpicture}
\caption{Typical Boot Chain for x86-64}
\label{fig:boot_chain}
\end{figure}

\subsection{The OS-Level Trampoline}
Even with a standardized bootloader, the kernel cannot assume full control immediately. A generic bootloader cannot know the specific internal requirements of the OS. For instance:
\begin{itemize}
    \item The kernel may typically require memory to be mapped to a specific virtual address range (e.g., the higher half).
    \item Specific hardware features (like Floating Point Units or Virtualization Extensions) are often disabled by default to save power and must be explicitly enabled.
    \item The OS must define its own memory protection structures to enforce its specific security model.
\end{itemize}

Therefore, a robust operating system must implement its own initialization stage, effectively an \textbf{OS-Level Bootloader} or "Trampoline." This architecture-specific code is responsible for taking the machine from the normalized state provided by the external bootloader, enabling the specific CPU features required by the kernel, and establishing the runtime environment before passing control to the architecture-agnostic kernel main function.

\section{Memory Preloading and Discovery}
\label{sec:mem_discovery}
One of the first and most critical responsibilities of a kernel during the bootstrap phase is to establish an authoritative map of the system's physical memory. Unlike user-space applications, which simply request memory from the operating system via system calls (e.g., \texttt{malloc} or \texttt{mmap}), the kernel is the manager responsible for fulfilling those requests. Upon entry, the kernel does not know how much RAM is available, where it is located, or which memory ranges are reserved for hardware-mapped I/O (MMIO).

This discovery process is not standardized. It is strictly coupled to the target architecture, the silicon vendor, and the residing firmware. Depending on the platform complexity, the kernel may acquire the memory map through one of three primary mechanisms: static definition, firmware interrogation, or hardware description structures.

\subsection{Static Definition}

On strictly embedded architectures (e.g., ARM Cortex-M or AVR), the physical memory layout is immutable. The location and size of SRAM banks, Flash storage, and peripheral registers are defined by the silicon vendor and do not change. In these environments, runtime discovery is redundant.

The memory map is hardcoded directly into the kernel's source code or linker scripts, matching the specific System-on-Chip (SoC) datasheet \cite{ARM-CortexM4-Generic-User-Guide}. The developer explicitly defines the boundary between kernel code, stack, and heap. As illustrated in Figure \ref{fig:cortex_m4_memory}, the address space is rigid. The kernel assumes ownership of specific addresses immediately upon reset without querying external entities.

In this context, the Operating System does not "discover" memory. The kernel code assumes these addresses are valid from the first instruction. For example, a Cortex-M4 kernel may be hardcoded to expect code at 0x00000000 and RAM at 0x20000000. If the software is flashed onto a different chip variant, it will simply fault. Flexibility is sacrificed for minimizing initialization overhead.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    % Main memory block style
    memblock/.style={
        rectangle, 
        draw=black, 
        thick,
        minimum width=4.0cm, 
        align=center, 
        anchor=south,
        outer sep=0pt
    },
    % Left-side exploded box style
    sideblock/.style={
        rectangle,
        draw=black,
        minimum width=3.8cm,
        text width=3.5cm,
        align=center,
        font=\sffamily\scriptsize,
        fill=white
    },
    % Address label style
    addr/.style={
        font=\ttfamily\scriptsize
    }
]

% ==========================================
% 1. DRAW MAIN MEMORY STACK
% ==========================================

% Code
\node[memblock, minimum height=1.5cm] (code) at (0,0) {Code\\ \scriptsize 0.5GB};

% SRAM
\node[memblock, minimum height=1.5cm, above=0cm of code] (sram) {SRAM\\ \scriptsize 0.5GB};

% Peripheral
\node[memblock, minimum height=1.5cm, above=0cm of sram] (periph) {Peripheral\\ \scriptsize 0.5GB};

% External RAM
\node[memblock, minimum height=2.0cm, above=0cm of periph] (extram) {External RAM\\ \scriptsize 1.0GB};

% External Device
\node[memblock, minimum height=2.0cm, above=0cm of extram] (extdev) {External Device\\ \scriptsize 1.0GB};

% PPB
\node[memblock, minimum height=0.6cm, above=0cm of extdev] (ppb) {Private peripheral\\bus \scriptsize 1.0MB};

% Vendor
\node[memblock, minimum height=1.0cm, above=0cm of ppb] (vendor) {Vendor-specific\\memory \scriptsize 511MB};


% ==========================================
% 2. ADDRESS LABELS (RIGHT SIDE)
% ==========================================
\node[addr, anchor=south west] at (vendor.north east) {0xFFFFFFFF};
\node[addr, anchor=south west] at (vendor.south east) {0xE0100000};
\node[addr, anchor=south west] at (ppb.south east)    {0xE0000000};
\node[addr, anchor=south west] at (extdev.south east) {0xA0000000};
\node[addr, anchor=south west] at (extram.south east) {0x60000000};
\node[addr, anchor=south west] at (periph.south east) {0x40000000};
\node[addr, anchor=south west] at (sram.south east)   {0x20000000};
\node[addr, anchor=south west] at (code.south east)   {0x00000000};


% ==========================================
% 3. LEFT SIDE BIT-BANDING (MANUAL PLACEMENT)
% ==========================================

% - SRAM GROUP -
% Define the target point (Base of SRAM)
\coordinate (sram_target) at (sram.south west);

% 1MB Region Box (Level with SRAM base)
\node[sideblock, anchor=east] (sram_region) at (-2.5, 1.2) {1MB Bit band region};
% Address for 1MB Region
\node[addr, anchor=east] at (sram_region.south west) {0x20000000};
\node[addr, anchor=east] at (sram_region.north west) {0x200FFFFF};

% Alias Box (Floated above)
\node[sideblock, anchor=east, minimum height=0.8cm] (sram_alias) at (-2.5, 2.5) {32MB Bit band alias};
% Address for Alias
\node[addr, anchor=east] at (sram_alias.south west) {0x22000000};
\node[addr, anchor=east] at (sram_alias.north west) {0x23FFFFFF};

% Connect lines for SRAM
\draw (sram_alias.north east) -- (sram_target);
\draw (sram_alias.south east) -- (sram_target);
\draw (sram_region.north east) -- (sram_target);
\draw (sram_region.south east) -- (sram_target);


% - PERIPHERAL GROUP -
% Define the target point (Base of Peripheral)
\coordinate (periph_target) at (periph.south west);

% 1MB Region Box (Level with Peripheral base)
\node[sideblock, anchor=east] (periph_region) at (-2.5, 4.2) {1MB Bit band region};
% Address for 1MB Region
\node[addr, anchor=east] at (periph_region.south west) {0x40000000};
\node[addr, anchor=east] at (periph_region.north west) {0x400FFFFF};

% Alias Box (Floated above)
\node[sideblock, anchor=east, minimum height=0.8cm] (periph_alias) at (-2.5, 5.5) {32MB Bit band alias};
% Address for Alias
\node[addr, anchor=east] at (periph_alias.south west) {0x42000000};
\node[addr, anchor=east] at (periph_alias.north west) {0x43FFFFFF};

% Connect lines for Peripheral
\draw (periph_alias.north east) -- (periph_target);
\draw (periph_alias.south east) -- (periph_target);
\draw (periph_region.north east) -- (periph_target);
\draw (periph_region.south east) -- (periph_target);

\end{tikzpicture}
\caption{Cortex-M4 Memory Map with Bit-banding regions (Adapted from \cite{ARM-CortexM4-Generic-User-Guide})}
\label{fig:cortex_m4_memory}
\end{figure}

\subsection{Flattened Device Tree (DTB)}

To allow a single kernel binary to support multiple board configurations without hardcoding, architectures such as ARM64 and RISC-V utilize the \textbf{Flattened Device Tree}. The bootloader passes a pointer to a binary structure (the DTB Blob) which describes the hardware topology, including physical memory ranges and reserved regions.

The DTB format encodes the device tree into a linear, pointerless data structure. It consists of a fixed-size header followed by three variable-sized blocks:
\begin{enumerate}
    \item \textbf{Memory Reservation Block}: Lists physical memory ranges that the kernel must not overwrite (e.g., firmware runtime data).
    \item \textbf{Structure Block}: Describes the device nodes and properties in a linear tree format using token-based tags.
    \item \textbf{Strings Block}: A pool of null-terminated property names referenced by offset.
\end{enumerate}

The kernel parses this blob at boot to discover available RAM. \cite{Devicetree-Spec}

\subsection{Firmware Interrogation}

On general-purpose platforms (x86-64), the hardware is modular. The kernel cannot predict the amount of installed RAM or the physical address map. In this scenario, the kernel must query the system firmware directly. This introduces a dependency on the firmware interface:
\begin{itemize}
    \item \textbf{Legacy BIOS:} Requires invoking interrupt vectors (e.g., \texttt{INT 0x15, EAX=0xE820}) to retrieve a list of memory ranges \cite{osdev-int15}.
    \item \textbf{UEFI:} Requires calling specific boot services (\texttt{GetMemoryMap}) to retrieve descriptors of physical pages and their attributes \cite{UEFI-Base-Spec}.
\end{itemize}

\subsection{Hardware Abstraction}
How does the kernel handle such a diverse set of methods of querying the memory? This is a part of a bigger topic --- namely Hardware / Architecture Abstraction --- and will be discussed in more detail in Section~\ref{sec:arch_abstraction}.

\subsection{Discovering and Enabling CPU Features}
\label{subsec:cpu_discovery}

Modern CPUs are not monolithic entities with a fixed feature set. Instead, they represent an accumulation of decades of architectural extensions. A generic x86-64 processor guarantees a baseline instruction set (User-level ISA), but specific capabilities regarding vectorization (AVX, AVX-512), cryptography (AES-NI), security (SMEP, SMAP), and performance (PCID, TSC-Deadline) vary significantly between processor generations and manufacturers.
\cite{Intel-AVX}

An operating system kernel cannot blindly execute advanced instructions. Doing so on hardware that lacks support results in an \textit{Invalid Opcode} exception, causing a kernel panic. Therefore, a robust kernel must perform a feature discovery handshake during the early initialization phase.

\subsubsection{Feature Identification}
On the x86 architecture, this discovery is performed via the \texttt{CPUID} instruction. This instruction acts as a query interface where the software loads a leaf index into the \texttt{EAX} register (and optionally a subleaf in \texttt{ECX}) and executes \texttt{CPUID}. The processor returns feature bitmaps and vendor information in the general-purpose registers (\texttt{EAX}, \texttt{EBX}, \texttt{ECX}, \texttt{EDX}).
\cite{Intel-CPUID}

While x86 relies on this dynamic instruction-based discovery, other architectures employ different strategies:
\begin{itemize}
    \item \textbf{ARM64 (AArch64)} utilizes special system registers (e.g., \texttt{ID\_AA64PFR0\_EL1}) that the kernel reads to determine support for floating-point units or cryptographic extensions \cite{ARM-Arch-Ref,}.
    \item \textbf{RISC-V} typically employs the Device Tree Blob (DTB) or the \texttt{misa} (Machine ISA) Control and Status Register to inform the kernel about supported standard extensions (e.g., Atomics, Floats) \cite{RISCV-Priv-Spec}.
\end{itemize}

\subsubsection{Feature Enablement}
Identifying that a feature exists is often insufficient. The kernel must explicitly enable it. To maintain backward compatibility and minimize power consumption, processors often boot with advanced features disabled.

A prime example is the Floating Point Unit (FPU) and Vector Extensions (SSE/AVX). On x86-64, even if \texttt{CPUID} reports that AVX is supported, attempting to execute a \texttt{VMOVDQA} instruction will fault unless the kernel has:
\begin{enumerate}
    \item Enabled the FPU by clearing the Emulation bit in Control Register \texttt{CR0}.
    \item Enabled SSE by setting the \texttt{OSFXSR} bit in \texttt{CR4}.
    \item Enabled XSAVE/XRSTOR support by setting the \texttt{OSXSAVE} bit in \texttt{CR4}.
    \item Explicitly enabled AVX state saving in the Extended Control Register (\texttt{XCR0}).
\end{enumerate}
\cite{Intel-ControlRegisters, Intel-XSAVE}

This enabling phase is critical not only for allowing instruction execution but also for the scheduler. The operating system must know the size of the processor's register state (Context) to correctly save and restore threads during context switches. If AVX-512 is enabled, the context size increases significantly compared to standard SSE, impacting memory usage and context switch latency.

\section{Establishment of Basic Communication}
\label{sec:comms}

One of the primary objectives when initializing code on a target architecture is to establish an external communication channel. In an emulation environment, this is often achieved by interacting with the emulator's framework (e.g., QEMU utilizes a serial port \cite{wikibooks-serial} that can be attached to a Linux shell session). On physical hardware, the developer may need to render fonts on a screen (e.g., using the VGA standard on x86-64 desktop platforms \cite{osdev-barebones, osdev-vga}) or implement a basic network stack. It should be noted that at this stage, a rudimentary implementation is often sufficient. However, this serves as a provisional solution, to ensure correct handling in the future, a fully-fledged infrastructure and a proper hardware abstraction layer must be established. The preferred method should support bidirectional communication during the early development stages to facilitate testing and provide input to the kernel. This functionality is primarily required for debugging and testing, and as development progresses, it is advisable to abandon this simple communication or disable it via compilation flags.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\textwidth]{res/debug-output.png}
%     \caption{QEMU serial-port Communication}
%     \label{fig:qemu_comms}
% \end{figure}

% As shown in Figure \ref{fig:qemu_comms}, alongside typical output to the screen and keyboard input, the kernel transmits text to the QEMU serial port, which is then streamed to the host shell. This approach enables host-side scripts to parse logs, detect bugs or failures, and allow manual inspection of the system state immediately preceding a crash.

\section{Enabling Interrupts and Exceptions}
\label{subsec:os-tutorial-interrupts}

Prior to the implementation of memory management (Section \ref{subsubsec:physical_memory_management}), it is essential to establish a basic interrupt handling mechanism. On most platforms, the exception system relies entirely on interrupt mappings. Consequently, without a functional interrupts, the kernel is unable to display debug information on the designated communication device when an error occurs.

Exception handlers should output a descriptive message detailing the failure. This message must include the CPU state, the source of the exception, the instruction pointer where the error occurred, and, if applicable, an error code explaining the cause. An example of an x86-64 kernel dump following an exception is presented below:

\begin{cppcode}[caption={Example Kernel Panic on Exception}, label={lst:kernel_panic}]
    [ KERNEL PANIC ] Received exception: 0 (Divide Error (#DE))
    And error: 0
    At instruction address: 0xffffffff8021b856
    rip:                    0xffffffff8021b856
    rflags:                 0x0000000000010246
    rsp:                    0xffffffff8138dc20
    rax:                    0x0000000000000009
    rbx:                    0x000000000006f000
    rcx:                    0x0000000000000000
    rdx:                    0x0000000000000000
    rsi:                    0xffffffff803648f3
    rdi:                    0xffffffff80818c28
    rbp:                    0xffffffff8138dc40
    r8:                     0xffffffff80305d00
    r9:                     0xffffffff80305d59
    r10:                    0xffffffff8138df10
    r11:                    0xffffffff802fdfaf
    r12:                    0x0000000000000010
    r13:                    0x0000000000180000
    r14:                    0x0000000001a91000
    r15:                    0x0000000000000800
    
    RFLAGS:                 0x0000000000010246
\end{cppcode}

Before initializing Memory Management, exception handling is the primary function required from the interrupt system. At this stage, there is typically no use case for interrupt-driven devices, as the consumers (processes) have not yet been booted, and multicore operations (which require memory for bookkeeping structures) are not active. However, the design of the interrupt subsystem must account for future extensibility and requirements.

\subsection{Design Considerations}

It is important to note that hardware-compliant interrupt handlers differ significantly from standard compiled functions. They often require specific entry and exit instructions, making the runtime swapping of these functions difficult. A robust solution is to create a low-level assembly wrapper that efficiently performs all architecture-dependent operations before invoking a high-level function responsible for the kernel's logic. However, this approach has limitations. The high-level function is typically hardcoded into the low-level handler, preventing runtime reconfiguration.

To address this, a hardware abstraction layer is required to serve as an intermediary between the architecture-specific wrapper and the high-level kernel logic. This abstraction should manage the low-level tables and facilitate the swapping of handlers at runtime. This can be implemented using an array of abstract handlers mapped to hardware interrupts. Furthermore, devices are mapped to hardware interrupts dynamically during runtime based on user configuration and hardware discovery, necessitating the ability to swap drivers and logic dynamically. Without this modularity, the system would require multiple stages of interrupt initialization, multiple architecture-specific handlers, or complex handlers that query the kernel state to determine valid operations (e.g., a page fault handler checking for virtualization support each time it is invoked).

It is also critical to design handlers to perform minimal work. This precludes operations such as waiting, sleeping, or extensive tracing. If device handling requires complex logic or a state change, the handler should create a separate task and invoke the scheduler to decide if we need a context switch upon exiting the interrupt. Executing complex logic within the interrupt context blocks other interrupts and may lead to data loss. Consequently, the interrupt abstraction layer must also support task switching.

When implementing this abstraction, three distinct classes of interrupts can be identified:

\begin{itemize}
    \item \textbf{Exceptions} --- Generated by the CPU to indicate specific conditions requiring immediate attention, such as Page Faults, Division by Zero, or invalid instruction operands.
    \item \textbf{Hardware Interrupts} --- Generated by external devices (e.g., Timers, Keyboard, Disk Controller) to communicate efficiently with the kernel without the need for polling.
    \item \textbf{Software Interrupts} --- Initiated by software instructions. For example, on the x86-64 architecture, the instruction \texttt{INT 0x80} triggers an interrupt with the vector number \texttt{0x80}.
\end{itemize}

Architecture-specific details must also be considered. For example, the x86-64 architecture utilizes the legacy PIC \cite{osdev-pic} and the improved SMP-supporting APIC \cite{osdev-apic}, and it would be wise to support both somehow. 
Finally, to enable Symmetric Multiprocessing (SMP) and utilize multiple cores, the interrupt mechanism serves as the primary method of inter-core communication.

\section{Tracing System}
\label{sec:tracing}

As discussed in Section \ref{subsec:os-tutorial-interrupts}, direct tracing inside interrupt handlers is generally discouraged due to latency concerns. However, in a general context, kernel logs and debug messages must be preserved to a file or terminal. The challenge is that writing to a file or physical device incurs significant latency, while system code must execute as rapidly as possible. To resolve this, the tracing framework must be robust enough to operate in a concurrent environment (handling interrupts and SMP) while decoupling the generation of traces from their output. 

This can be achieved by allocating a large circular buffer for messages, which is then asynchronously flushed to the output device by a dedicated, low-priority task. Furthermore, to facilitate effective debugging in a multi-threaded and multi-core environment, each log entry may automatically capture essential context metadata, specifically the high-precision timestamp, the current Core ID, or the Process ID.

This goes about the general design, but during initial development of the system, there is no need to have a full tracing system, simple print with formatting is enough to proceed.

\section{Testing framework}
\label{sec:testing}

The reliability of an operating system kernel cannot be guaranteed solely through compilation checks or host-based logic verification. While some algorithmic components can be tested in a hosted environment (e.g., on a Linux host), the core kernel functionalities depend heavily on the specific hardware architecture, physical memory layout, and privileged CPU instructions. Consequently, a robust testing framework must be implemented to execute directly within the kernel on the target hardware or emulator.

This in-kernel framework should function similarly to user-space unit testing libraries but operates within the kernel. It allows developers to define test cases for critical subsystems, such as memory allocators, intrusive data structures, and scheduling algorithms, and execute them in the actual runtime environment. This approach is the only way to detect architecture-specific issues, such as unaligned memory accesses, incorrect register usage during context switches, or paging faults caused by invalid table entries, which would remain undetected in a hosted simulation or simply impossible to test.

A critical design requirement for such a framework is test isolation. In a kernel environment, a failure often results in a system panic, infinite loop, or silent memory corruption that compromises the global state. To prevent side effects from a failed test influencing subsequent assertions (creating false positives or negatives), the framework should ideally ensure a clean state for each test suite. In early development stages, this often necessitates implementing mechanisms to reset subsystems between test executions, ensuring that results are deterministic and reproducible.

\section{Physical Memory Management}
\label{subsubsec:physical_memory_management}

The Physical Memory Manager (PMM) constitutes the foundational layer of the operating system's memory subsystem. Its primary responsibility is the accounting of the machine's finite Random Access Memory (RAM). In a monolithic kernel with virtual memory support, the PMM typically operates at two distinct levels of granularity:

\begin{enumerate}
\item \textbf{Page Frame Allocation:} The allocation of raw, contiguous physical memory in fixed-size units called \textit{page frames} (typically 4096 bytes on x86-64). This is primarily used to back Virtual Memory mappings for user-space processes.
\item \textbf{Kernel Heap Allocation:} The sub-allocation of memory within those pages for the kernel's own internal data structures (e.g., thread control blocks, file descriptors). These requests vary wildly in size, from a few bytes to several kilobytes.
\end{enumerate}

While the granularity differs, the fundamental algorithmic challenge remains the same: how to satisfy a request of size $S$ efficiently while minimizing wasted space. The remainder of this section explores the theoretical constraints and algorithmic solutions common to both layers, starting with the arch-nemesis of memory management: fragmentation.

\subsection{The Core Challenge: Fragmentation}

If an allocator simply handed out memory sequentially and never had to accept returns (frees), the implementation would be a trivial pointer increment. Complexity arises because memory is borrowed and returned in an arbitrary order. This leads to \textbf{fragmentation}: the inability to reuse memory that is technically free. Fragmentation manifests in two distinct forms \cite{wilson1995survey}:

\begin{itemize}
    \item \textbf{Internal Fragmentation:} This occurs when the allocator assigns a block larger than what was requested. For example, if a request for 20 bytes is rounded up to a 32-byte block to satisfy alignment requirements, 12 bytes are wasted. This waste is "internal" to the allocated block and unusable by others.
    \item \textbf{External Fragmentation:} This occurs when free memory exists in the system, but it is scattered in small, non-contiguous holes such that a request for a large contiguous block cannot be satisfied. The total free memory might be sufficient, but the geometry of the holes prevents allocation.
\end{itemize}

\textbf{The Root Cause: Isolated Deaths.} Fragmentation fundamentally arises from \textit{isolated deaths} --- when an object is freed, but its neighbors remain allocated, creating an unusable hole. If adjacent objects always died together, their combined space would be reclaimed as one contiguous block. The allocator's core challenge is predicting which objects will die at similar times and placing them contiguously. Since the allocator cannot know future behavior, it must rely on heuristics that exploit regularities in program behavior~\cite{wilson1995survey}.

The goal of any allocator is to balance the minimization of these two types of fragmentation against the computational cost (latency) of the allocation operation.

\subsection{Anatomy of an Allocator}

To understand how different allocators address these challenges, it is helpful to use the taxonomy proposed by Wilson et al. \cite{wilson1995survey}, which separates allocator design into three levels of abstraction:

\subsubsection{1. Strategy}
The strategy is the high-level philosophy used to combat fragmentation. It relies on heuristics about program behavior. Research into allocation traces reveals three dominant patterns \cite{wilson1995survey}:

\begin{itemize}
    \item \textbf{Ramps:} Monotonic accumulation of long-lived data (e.g., building a parse tree).
    \item \textbf{Peaks:} Bursty allocation of temporary structures that are collectively discarded after a phase (e.g., processing one request).
    \item \textbf{Plateaus:} Rapid initial allocation followed by stable long-term usage.
\end{itemize}

These patterns are exploitable: objects allocated together often die together. By clustering objects that are likely to die at the same time, the allocator increases the probability that large contiguous blocks will be freed simultaneously.

\subsubsection{2. Policy}
The policy is the specific decision logic used to select a free block for a given request. Common policies include:
\begin{itemize}
    \item \textbf{First Fit:} Scan the free memory from the beginning and return the first block that is large enough. This is fast but can accumulate small "splinters" of free memory at the start of the list.
    \item \textbf{Best Fit:} Search the entire list to find the smallest block that satisfies the request. This minimizes the wasted remainder (unused space after the split) but requires a comprehensive search, which can be slow.
    \item \textbf{Next Fit}: Resume searching from where the last allocation stopped. Scatters allocations and often
performs poorly
\end{itemize}

\subsubsection{3. Mechanism}
The mechanism is the data structure and algorithm used to implement the policy efficiently. For example, a "Best Fit" policy could be implemented via a linear scan of a linked list (mechanism: $O(N)$), or by looking up a size index in a tree or bitmap (mechanism: $O(log N)$ or $O(1)$).

The following subsections examine specific mechanisms used in OS development, ordered by increasing complexity.

\subsection{Mechanism: Bitmap Allocation}

The bitmap allocator is conceptually the simplest mechanism, often used for \textbf{Page Frame Allocation} due to its static nature. In this scheme, the physical memory is divided into fixed-size units (pages). A separate region of memory, the bitmap, tracks the status of these units. Each bit in the bitmap corresponds to one page frame: a 0 indicates free, and a 1 indicates allocated.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    bit/.style={draw, minimum width=0.5cm, minimum height=0.5cm, anchor=south west},
    freeb/.style={bit, fill=green!20},
    usedb/.style={bit, fill=red!20}
]

% Draw bitmap array
\node[anchor=east] at (-0.2, 0.25) {\textbf{Bitmap:}};
\node[freeb] at (0, 0) {0};
\node[usedb] at (0.5, 0) {1};
\node[usedb] at (1.0, 0) {1};
\node[freeb] at (1.5, 0) {0};
\node[freeb] at (2.0, 0) {0};
\node[usedb] at (2.5, 0) {1};
\node[freeb] at (3.0, 0) {0};
\node[freeb] at (3.5, 0) {0};
\node at (4.2, 0.25) {...};

% Index labels
\node[font=\tiny] at (0.25, -0.3) {0};
\node[font=\tiny] at (0.75, -0.3) {1};
\node[font=\tiny] at (1.25, -0.3) {2};
\node[font=\tiny] at (1.75, -0.3) {3};
\node[font=\tiny] at (2.25, -0.3) {4};
\node[font=\tiny] at (2.75, -0.3) {5};
\node[font=\tiny] at (3.25, -0.3) {6};
\node[font=\tiny] at (3.75, -0.3) {7};

% Physical addresses
    \node[anchor=west, font=\scriptsize] at (5, 0.25) {Frame $i$ $\rightarrow$ Address $i \times PageSize$};

\end{tikzpicture}
\caption{Bitmap Representation of Physical Memory}
\label{fig:bitmap_allocator}
\end{figure}

The primary advantage of bitmaps is space efficiency. To manage 4 GiB of RAM using 4 KiB pages, there are roughly 1 million frames. This requires 1 million bits, or merely 128 KiB of overhead. Furthermore, bitmaps naturally support the allocation of \textit{contiguous} multipage blocks. The allocator simply needs to find a run of $N$ consecutive zeros.

However, the search complexity is linear. To find free memory, the allocator must scan the bitmap. In the worst case (a nearly full system), it may scan the entire map ($O(N)$) before finding a free page or determining that memory is exhausted.

\subsection{Mechanism: Stack-Based Allocation (Free Lists)}

While bitmaps handle contiguous allocations well, their linear search time is often unacceptable for the most frequent operation: allocating a single page. To achieve constant-time $O(1)$ performance, many kernels employ a \textbf{Stack} or \textbf{Linked List} allocator.

In this scheme, the allocator maintains a pointer to a "head" page. This page contains a pointer to the next free page, which points to the next, forming a chain. Since these pages are by definition unused, the OS can store the "next" pointer inside the page itself, requiring no external metadata storage.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    page/.style={draw, minimum width=2.5cm, minimum height=0.8cm, fill=green!10},
    arrow/.style={-Latex, thick}
]

\node[page] (p1) at (0, 0) {Page @ 0x5000};
\node[page] (p2) at (0, -1.2) {Page @ 0x9000};
\node[page] (p3) at (0, -2.4) {Page @ 0x2000};
\node[font=\scriptsize] at (0, -3.4) {...};

\node[anchor=east, font=\bfseries] at (-1.8, 0) {HEAD $\rightarrow$};

\draw[arrow] (p1.south) -- (p2.north);
\draw[arrow] (p2.south) -- (p3.north);

\node[anchor=west, align=left, font=\scriptsize] at (2, -0.6) {Pop: $O(1)$\\Push: $O(1)$};
\node[anchor=west, align=left, font=\scriptsize, text=red!70!black] at (2, -2.0) {Loss of Contiguity};

\end{tikzpicture}
\caption{Stack-Based Free List Allocator}
\label{fig:stack_allocator}
\end{figure}

The stack acts as a cache of "hot" pages. When a page is freed, it is pushed onto the stack; when a page is requested, it is popped. This is extremely fast but suffers from a fatal flaw regarding fragmentation: the stack order reflects \textit{execution history}, not \textit{physical address} order. A stack might contain physical page 0x1000 followed by page 0x5000. If a driver requests two contiguous pages, the stack cannot satisfy the request even if the system has plenty of memory. Consequently, stacks are often used as a fast frontend cache layered on top of a more complex backing allocator.

\subsection{Mechanism: The Buddy System}

To solve the contiguity problem while maintaining decent performance, general-purpose operating systems (like Linux) utilize the \textbf{Buddy System}. The Buddy System is a specific implementation of a broader class of allocators known as \textbf{Segregated Fits}, and its theoretical foundations are thoroughly described by Knuth \cite[Section 2.5]{knuth1973art}.

\subsubsection{Concept: Segregated Fits}
In a segregated fit architecture, memory is not viewed as one monolithic pool. Instead, the allocator maintains an array of free lists. Each list acts as a bin dedicated to blocks of a specific size class. For example, index 0 might hold 4KiB blocks, index 1 might hold 8KiB blocks, and so on. This allows the allocator to quickly locate a block that fits a request without searching through blocks that are vastly too large or too small.

\subsubsection{The Binary Buddy Algorithm}
The Buddy System applies a strict discipline to segregated fits: all block sizes must be powers of two. The allocator maintains an array of lists for orders $0$ to $MAX\_ORDER$. Order $k$ typically represents a block size of $PAGE\_SIZE \times 2^k$.

\textbf{Splitting (Allocation):} When a request for a block of order $k$ arrives:
\begin{enumerate}
    \item The allocator checks the list at index $k$.
    \item If the list is empty, it moves up to list $k+1$, splits a larger block into two halves (buddies), adds one half to list $k$, and returns the other.
    \item If list $k+1$ is also empty, it recurses upward until a block is found or memory is exhausted.
\end{enumerate}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    block/.style={draw, minimum height=0.6cm, fill=blue!10},
    freeblock/.style={draw, minimum height=0.6cm, fill=green!20},
    arrow/.style={-Latex, thick, dashed}
]

% Original block
\node[block, minimum width=8cm] (orig) at (0, 2) {Order 3 Block (32 KiB)};

% Split into two Order 2
\node[freeblock, minimum width=4cm] (l2a) at (-2, 0.8) {Order 2 (16 KiB)};
\node[block, minimum width=4cm] (l2b) at (2, 0.8) {Order 2 (16 KiB)};

% Further split left Order 2
\node[freeblock, minimum width=2cm] (l1a) at (-3, -0.4) {Order 1};
\node[block, minimum width=2cm] (l1b) at (-1, -0.4) {Order 1};

% Further split
\node[freeblock, minimum width=1cm] (l0a) at (-3.5, -1.6) {O0};
\node[block, minimum width=1cm, fill=red!20] (l0b) at (-2.5, -1.6) {O0};

\draw[arrow] (orig.south) -- (l2a.north);
\draw[arrow] (orig.south) -- (l2b.north);
\draw[arrow] (l2a.south) -- (l1a.north);
\draw[arrow] (l2a.south) -- (l1b.north);
\draw[arrow] (l1a.south) -- (l0a.north);
\draw[arrow] (l1a.south) -- (l0b.north);

\node[anchor=west, font=\scriptsize] at (4.5, 0) {Recursive splitting};
\node[anchor=west, font=\scriptsize, text=red!70!black] at (-0.5, -1.6) {Allocated};
\node[anchor=west, font=\scriptsize, text=green!50!black] at (-3, -2.2) {Buddy (free)};

\end{tikzpicture}
\caption{Buddy System: Splitting on Allocation}
\label{fig:buddy_split}
\end{figure}

\textbf{Coalescing (Freeing):} The elegance of the Buddy System lies in how it handles fragmentation. Every block has a unique "buddy" that is its physical neighbor in memory. When a block is freed, the allocator checks if its buddy is also free. If so, they merge (coalesce) into a block of the next higher order. This repeats recursively.

Crucially, the address of a block's buddy can be calculated instantly using an XOR operation, without traversing lists. For a block of size $2^k$ at address $A$, the buddy is located at:
\[ \texttt{buddy\_addr} = A \oplus 2^k \]

\subsubsection{Why Buddy is Insufficient for Real-Time}
While the Buddy System is efficient ($O(log N)$) and handles external fragmentation well via coalescing, it suffers from significant \textbf{Internal Fragmentation}. Because every request must be rounded up to a power of two, a request for 33 KiB requires a 64 KiB block, wasting 47\% of the allocated memory.

Furthermore, the coalescing logic is restrictive. Two adjacent free blocks of size $2^k$ cannot always be merged. They must be \textit{buddies} (i.e., aligned on a strict $2^{k+1}$ boundary). This can lead to cases where contiguous memory is available but unusable because the blocks are "cousins" rather than "buddies" \cite{wilson1995survey}. These limitations motivate the need for more granular allocators in real-time systems, such as TLSF.

\subsection{Mechanism: Two-Level Segregated Fit (TLSF)}

While the Buddy System is fast, its reliance on power-of-two sizing creates unacceptable internal fragmentation for workloads that frequently allocate objects of irregular sizes (e.g., 50 bytes or 800 bytes). Additionally, real-time systems require a strict guarantee of constant-time $O(1)$ performance, regardless of the fragmentation state of the heap.

The \textbf{Two-Level Segregated Fit (TLSF)} allocator \cite{masmano2004tlsf} was designed specifically to address these requirements. It is a segregated fit allocator (like Buddy) but with a much finer granularity of size classes and a more flexible coalescing strategy.

\subsubsection{Addressing Internal Fragmentation: The Two-Level Index}
To reduce internal fragmentation, an allocator needs size classes that are closer together than powers of two. However, having thousands of free lists (one for 10 bytes, one for 11, etc.) makes searching for a free block slow. TLSF solves this by organizing free lists into a two-dimensional matrix, denoted by indices $(f, s)$.

\begin{enumerate}
    \item \textbf{First-Level Index (FLI):} This corresponds to the power-of-two bucket, roughly equivalent to the "Order" in the Buddy System. It is calculated as the position of the most significant bit set in the size: $f = \lfloor \log_2(\text{size}) \rfloor$.
    \item \textbf{Second-Level Index (SLI):} To bridge the large gap between powers of two (e.g., the gap between 256 and 512 is 256 bytes wide), TLSF subdivides each FLI bucket into $2^{SLI}$ linearly spaced sub-buckets.
\end{enumerate}

For example, if we configure the allocator with $SLI = 2$ (which means 4 subdivisions), the range $[256, 512)$ is split into 4 bands: $[256, 320)$, $[320, 384)$, $[384, 448)$, and $[448, 512)$. A request for 260 bytes no longer needs to be rounded up to 512; it fits into the first subdivision. This significantly tightens the bound on internal fragmentation.

\subsubsection{O(1) Search via Bitmaps}
The core innovation of TLSF is how it finds a suitable free block in constant time. The allocator maintains a set of \textbf{Bitmaps} that act as a summary of the free lists.
\begin{itemize}
    \item A bit is set to \textbf{1} if the corresponding free list contains at least one block.
    \item A bit is set to \textbf{0} if the list is empty.
\end{itemize}

When a request arrives, the allocator maps the size to indices $(f, s)$. It checks the bitmap for that list. If the bit is 0 (empty), it does not need to loop through lists. Instead, it uses a hardware instruction --- \textit{Find First Set} (\texttt{ffs}) or \textit{Count Leading Zeros} (\texttt{clz}) --- to atomically find the index of the next available bit in the bitmap. This operation allows the allocator to skip over empty size classes instantly, guaranteeing $O(1)$ allocation time regardless of the heap state.

\subsubsection{Physical Block Management: Boundary Tags}
The Buddy System calculates the address of a neighbor using bitwise XOR. This is fast, but it restricts merging: a block at address 0x2000 of size 0x1000 can \textit{only} merge with its specific buddy at 0x3000. It cannot merge with a free block at 0x1000, even though they are physically adjacent.

TLSF abandons the "buddy" logic in favor of \textbf{Boundary Tags} to support merging any physically adjacent free blocks.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    block/.style={draw, minimum height=1cm, anchor=west},
    header/.style={draw, minimum height=0.3cm, fill=gray!30, anchor=west},
    freeblk/.style={block, fill=green!15},
    usedblk/.style={block, fill=red!15}
]

% Memory layout
\node[header, minimum width=0.8cm] (h1) at (0, 0) {\tiny H};
\node[freeblk, minimum width=2cm] (b1) at (0.8, 0) {Free (64B)};
\node[header, minimum width=0.8cm] (h2) at (2.8, 0) {\tiny H};
\node[usedblk, minimum width=3cm] (b2) at (3.6, 0) {Used (96B)};
\node[header, minimum width=0.8cm] (h3) at (6.6, 0) {\tiny H};
\node[freeblk, minimum width=2cm] (b3) at (7.4, 0) {Free (48B)};

% Arrows for neighbor check
\draw[-Latex, thick, blue] (b2.north west) to[out=120, in=60] node[above, font=\scriptsize] {check prev} (b1.north east);
\draw[-Latex, thick, blue] (b2.north east) to[out=60, in=120] node[above, font=\scriptsize] {check next} (b3.north west);

\node[anchor=north, font=\scriptsize] at (4.5, -0.8) {Headers store size $\rightarrow$ locate neighbors in $O(1)$};

\end{tikzpicture}
\caption{Boundary Tags Enable Arbitrary Coalescing}
\label{fig:tlsf_coalesce}
\end{figure}

Every block in memory (whether free or allocated) is prefixed with a header containing its size and a flag indicating if the \textit{previous} physical block is free.
\begin{itemize}
    \item \textbf{Freeing:} When block $B$ is freed, it checks its header to see if the previous neighbor is free. If so, it reads the previous neighbor's footer (located immediately before $B$'s header) to find its start address and merges with it. It then checks the next neighbor (at address $B + size$). If that is also free, it merges.
    \item \textbf{Result:} This allows TLSF to counteract external fragmentation much more aggressively than the Buddy System, as any contiguous free space is immediately coalesced into the largest possible block.
\end{itemize}

\subsubsection{Summary of Trade-offs}
TLSF is the preferred allocator for real-time kernels because it guarantees worst-case execution time ($O(1)$) and minimizes wasted memory (low fragmentation). However, it is slightly slower on average than the Buddy System due to the overhead of maintaining the doubly linked lists for every size class and the boundary tag manipulation. For general-purpose operating systems where average throughput is more important than worst-case latency, the Buddy System is often retained.

\subsection{Advanced Layering: Object Caching (Slab)}

The mechanisms described so far (Buddy, TLSF) manage raw blocks of memory. However, the operating system kernel rarely wants just "64 bytes". It usually wants a specific object, such as a process control block, a file descriptor, or a synchronization primitive.

Initializing these objects can be expensive. For example, initializing a kernel mutex involves setting internal flags, initializing wait queues, and perhaps interacting with hardware interrupt controllers. If an object is allocated, initialized, used, destroyed, and freed repeatedly, the CPU spends significant cycles just setting up and tearing down the same state.

The \textbf{Slab Allocator}, introduced by Jeff Bonwick for SunOS \cite{bonwick1994slab}, solves this by observing that the state of a freed object is often valid for the next allocation. It separates the concepts of \textit{memory release} and \textit{object destruction}.

\subsubsection{The Concept of Object Caching}
A Slab Allocator organizes memory into caches of specific object types (e.g., a cache for \texttt{task\_structs}, a cache for \texttt{inodes}).
\begin{itemize}
    \item \textbf{Construction:} When the cache grows, it allocates a page from the underlying PMM and calls a \textit{constructor} on every slot in that page. For a mutex, this initializes the lock state.
    \item \textbf{Allocation:} When the kernel requests an object, the allocator pops one from the cache. Crucially, the object is \textit{already constructed}. The mutex is ready to be acquired immediately.
    \item \textbf{Freeing:} When the kernel releases the object, the allocator returns it to the cache but \textit{does not} call the destructor. The state (the initialized mutex) remains intact.
    \item \textbf{Reclamation:} Only when the system is under extreme memory pressure does the allocator reclaim the slab. It calls the \textit{destructor} on the objects and returns the raw pages to the PMM.
\end{itemize}

This effectively turns memory management into a caching problem. The allocator caches \textit{constructed state}, reducing the overhead of allocation to little more than pointer arithmetic.

\subsection{Scalability: Allocating on Multicore Systems}

In a Symmetric Multiprocessing (SMP) environment, a single global heap becomes a major bottleneck. If all cores must acquire the same spinlock to call \texttt{malloc}, the system's performance will degrade as core counts rise due to lock contention.

To solve this, modern allocators use a two-tiered hierarchy:
\begin{enumerate}
    \item \textbf{Per-CPU Caches:} Each processor is assigned a small local heap (a cache of free blocks). Allocations are satisfied from this local cache without acquiring a global lock, as no other CPU touches this memory.
    \item \textbf{Global Heap:} When a local cache empties, it grabs a batch of memory from the global heap (requiring a lock). When a local cache overflows, it returns a batch to the global heap.
\end{enumerate}

\subsubsection{The Hoard Allocator}
A naive implementation of per-CPU heaps suffers from a specific pathology known as \textit{blowup}. Consider a "Producer-Consumer" pattern where Thread A (on CPU 1) continuously allocates packets, and Thread B (on CPU 2) consumes and frees them. CPU 1's heap constantly empties (requesting more from global), and CPU 2's heap constantly fills up (never returning to global). The system runs out of memory despite having ample free space, because that space is trapped on CPU 2.

The \textbf{Hoard Allocator} \cite{berger2000hoard} addresses this by organizing memory into \textit{Superblocks} --- large chunks of memory containing multiple objects of the same size. Hoard tracks the "emptiness" of these superblocks. If a superblock on a local heap becomes mostly empty, it is moved to the global heap, allowing other processors to reuse the memory. This guarantees that the memory consumption of the allocator is bounded within a constant factor of the ideal required memory, solving the blowup problem while maintaining scalability.

\subsection{Hardware Constraints: Zoning}

Finally, the PMM must respect the physical limitations of the hardware. Not all RAM is created equal.
\begin{itemize}
    \item \textbf{DMA Limitations:} Legacy hardware (like ISA devices) may only be able to address the bottom 16 MiB of RAM (24-bit addressing).
    \item \textbf{High Memory:} On 32-bit architectures with more than 4 GiB of RAM, the kernel cannot map all physical memory into its virtual address space simultaneously.
    \item \textbf{NUMA Topology:} On multi-socket systems, physical memory is partitioned across NUMA nodes. Allocating from a remote node incurs latency penalties, so the PMM should prefer local memory when possible.
\end{itemize}

A common solution is to partition the physical address space into disjoint \textbf{regions} or \textbf{zones}, each managed by a separate allocator instance. Allocation requests carry metadata (flags or capabilities) indicating which regions are acceptable. The allocator attempts the preferred region first and falls back to others only if permitted. Critically, a request for DMA-capable memory \textit{cannot} fall back to high memory-the hardware simply cannot address those frames.

\section{Virtual Memory Management}
\label{sec:vmm}

\subsection{The Abstraction of Memory}
\label{subsec:vmm_abstraction}

While the Physical Memory Manager (discussed in Section \ref{subsubsec:physical_memory_management}) is responsible for the accounting of raw storage resources, it provides no mechanisms for isolation, safety, or convenient addressing. In a system lacking a Memory Management Unit (MMU), software operates in a physical addressing mode often referred to as Identity Mapping. In such an environment, the address utilized by a machine instruction corresponds directly to the electrical signals asserted on the memory bus. While this model minimizes hardware complexity and is prevalent in embedded microcontrollers, it presents insurmountable challenges for a general-purpose operating system.

In a physical addressing model, every application must share a single global address space. This tight coupling implies that a programming error in one task --- such as a buffer overflow or a wild pointer dereference --- can corrupt the data structures of another task or even the kernel itself, leading to immediate system instability. Furthermore, compiling software becomes arduous, as every program requires a unique load address to avoid collision, rendering the execution of multiple instances of the same program nearly impossible without complex, position-independent code generation.

To resolve these architectural limitations, modern general-purpose operating systems implement Virtual Memory (VM). Fundamentally, Virtual Memory is an abstraction layer that decouples the \textit{logical view} of memory from the \textit{physical reality} of the hardware. 

By introducing this layer of indirection, the operating system provides each process with the illusion of a private, contiguous, and isolated address space. Address $0x400000$ in Process $A$ and address $0x400000$ in Process $B$ are distinct virtual entities that may map to disjoint physical frames, or to the same frame (in the case of shared libraries), or to no frame at all.

The primary focus in kernel design remains on the protection and organization capabilities of the architecture. The VMM is essential for:
\begin{itemize}
    \item \textbf{Isolation:} Enforcing privilege boundaries so that user-space applications cannot modify kernel memory or interfere with each other.
    \item \textbf{Flexibility:} Allowing programs to be linked to fixed virtual addresses regardless of their actual physical location or the amount of installed RAM.
    \item \textbf{Efficiency:} Enabling advanced mechanisms such as demand paging and Copy-on-Write (CoW), which optimize physical RAM usage by sharing data until modification is strictly necessary.
\end{itemize}

The following sections detail the implementation of this abstraction, moving from the hardware constraints of the MMU to the high-level software architecture and data structures required to manage a fragmented and dynamic address space.


\subsection{The Hardware Reality: Translation and the MMU}
\label{subsec:vmm_hardware}

While the concept of a virtual address space is an abstraction, its implementation relies heavily on specialized hardware. If the operating system were required to intervene and perform a software lookup for every memory access generated by a user program, the performance overhead would be prohibitive. Consequently, modern processors integrate a dedicated hardware component known as the Memory Management Unit (MMU) to handle the translation of Virtual Addresses (VA) to Physical Addresses (PA) on the fly.

Understanding the constraints and behaviors of the MMU is a prerequisite for designing the software management layer. The hardware dictates the granularity of memory management, the structure of the translation tables, and the performance characteristics of memory access.

\subsubsection{The Mechanism of Translation}

The fundamental operation of the MMU is the translation function $f(VA) \rightarrow PA$. A naive implementation might treat memory as a simple array, mapping every virtual byte to a physical byte. However, for a 64-bit address space, a linear mapping table would require exabytes of storage, far exceeding the capacity of any physical device. To solve this, the MMU divides the address space into fixed-size blocks known as \textbf{Pages} (in virtual memory) and \textbf{Frames} (in physical memory).

The most common page size is 4096 bytes (4 KiB). Under this scheme, the lower bits of an address are treated as an \textit{offset} within the page and are passed through untranslated. The upper bits constitute the \textit{Virtual Page Number} (VPN), which the MMU translates into a \textit{Physical Frame Number} (PFN).

To store these mappings efficiently, architectures employ \textbf{Multi-level Page Tables}. Instead of a single linear array, the translation structure is organized as a hierarchical, sparse tree. On the x86-64 architecture, for instance, this is typically a 4-level or 5-level structure (PML4, PDP, PD, PT). The root of this tree is physically anchored in a control register (e.g., \texttt{CR3} on x86, \texttt{TTBR} on ARM).

\begin{figure}[htbp]
    \centering
    % Placeholder for a figure showing 4-level paging translation
    \begin{tikzpicture}[
        font=\sffamily\small,
        >={Latex[length=2mm, width=1.5mm]},
        node distance=1.5cm and 0.8cm,
        % Styles
        addrbox/.style={
            draw=black!70,
            fill=#1!15,
            minimum height=1.0cm,
            minimum width=2.2cm,
            inner sep=4pt,
            align=center,
            font=\sffamily\bfseries\small
        },
        bitrange/.style={
            font=\sffamily\scriptsize,
            text=black!60,
            anchor=north,
            yshift=-2pt
        },
        tablebox/.style={
            draw=black!60,
            fill=white,
            minimum width=1.8cm,
            minimum height=2.5cm,
            rounded corners=2pt
        },
        entrybox/.style={
            draw=black!50,
            fill=white,
            minimum width=1.6cm,
            minimum height=0.5cm,
            font=\sffamily\scriptsize
        },
        highlight/.style={
            fill=#1!20,
            draw=#1!80!black,
            thick
        },
        arrow/.style={
            ->,
            thick,
            draw=black!70,
            rounded corners=5pt
        },
        indexarrow/.style={
            ->,
            draw=black!60,
            dashed
        }
    ]

    % --- 1. Virtual Address Bar (The Spine) ---
    
    \node[addrbox=violet] (l4) at (0,0) {L4 Index};
    \node[bitrange] at (l4.south) {47:39};

    \node[addrbox=blue, right=0.3cm of l4] (l3) {L3 Index};
    \node[bitrange] at (l3.south) {38:30};

    \node[addrbox=teal, right=0.3cm of l3] (l2) {L2 Index};
    \node[bitrange] at (l2.south) {29:21};

    \node[addrbox=orange, right=0.3cm of l2] (l1) {L1 Index};
    \node[bitrange] at (l1.south) {20:12};

    \node[addrbox=gray, right=0.3cm of l1, minimum width=2.5cm] (off) {Offset};
    \node[bitrange] at (off.south) {11:0};

    % Address Bits (Replacing the Title)
    \node[anchor=south, font=\ttfamily\small, text=violet] at ($(l4.north)+(0,0.3)$) {000000001};
    \node[anchor=south, font=\ttfamily\small, text=blue] at ($(l3.north)+(0,0.3)$) {000000000};
    \node[anchor=south, font=\ttfamily\small, text=teal] at ($(l2.north)+(0,0.3)$) {000000001};
    \node[anchor=south, font=\ttfamily\small, text=orange] at ($(l1.north)+(0,0.3)$) {000000000};
    \node[anchor=south, font=\ttfamily\small, text=gray] at ($(off.north)+(0,0.3)$) {000000000000};
    \node[anchor=south west, font=\sffamily\bfseries\small] at ($(l4.west)+(-2.5,0.82)$) {Virtual Address:};


    % --- 2. The Tables (The Teeth - Aligned below) ---
    
    % CR3 - Anchored Left
    \node[draw=black, fill=gray!20, minimum width=1.2cm, minimum height=0.8cm, left=1.5cm of l4 |- l4, shift={(0,-3)}] (cr3) {\textbf{CR3}};

    % Function to draw a table with a highlighted entry
    % #1: Name, #2: Position, #3: Color, #4: Label
    \def\drawtable#1#2#3#4{
        \node[tablebox] (#1) at (#2) {};
        \node[anchor=north, font=\sffamily\bfseries\small, yshift=-2pt] at (#1.south) {#4};
        % Decoration lines
        \draw[gray!30] (#1.west) -- (#1.east);
        \draw[gray!30] ($(#1.west)+(0,0.8)$) -- ($(#1.east)+(0,0.8)$);
        \draw[gray!30] ($(#1.west)+(0,-0.8)$) -- ($(#1.east)+(0,-0.8)$);
        % The Entry
        \node[entrybox, highlight=#3] (#1_entry) at (#1.center) {Entry};
    }

    \drawtable{pml4}{$(l4.south) + (0, -2.5)$}{violet}{PML4}
    \drawtable{pdpt}{$(l3.south) + (0, -2.5)$}{blue}{PDPT}
    \drawtable{pd}{$(l2.south) + (0, -2.5)$}{teal}{Page Dir}
    \drawtable{pt}{$(l1.south) + (0, -2.5)$}{orange}{Page Table}


    % --- 3. Connections ---

    % Vertical Indexing Lines (The "Select" Logic)
    \draw[indexarrow] (l4.south) -- (pml4.north);
    \draw[indexarrow] (l3.south) -- (pdpt.north);
    \draw[indexarrow] (l2.south) -- (pd.north);
    \draw[indexarrow] (l1.south) -- (pt.north);

    % Horizontal Chain (The "flow" Logic)
    
    % CR3 -> PML4 Base
    \draw[arrow] (cr3.east) -- (pml4.west);

    % PML4 Entry -> PDPT Base
    \draw[arrow] (pml4_entry.east) -- ++(0.4, 0) |- ($(pdpt.north west)!0.25!(pdpt.west)$);

    % PDPT Entry -> PD Base
    \draw[arrow] (pdpt_entry.east) -- ++(0.4, 0) |- ($(pd.north west)!0.25!(pd.west)$);

    % PD Entry -> PT Base
    \draw[arrow] (pd_entry.east) -- ++(0.4, 0) |- ($(pt.north west)!0.25!(pt.west)$);


    % --- 4. Physical Address Construction (The Merge) ---

    % PT Entry -> Frame Base
    \coordinate (merge_point) at ($(off.south) + (0.5, -2.5)$);
    
    % Container for Physical Address
    \node[draw=black!80, fill=green!5, minimum width=3.5cm, minimum height=1.2cm, align=center] (phys_addr) at (merge_point) {};
    \node[anchor=south, font=\bfseries] at (phys_addr.north) {Physical Address};
    
    % Split inside
    \node[draw=black!50, fill=green!20, minimum height=0.6cm, minimum width=2.0cm, anchor=west] (pfn_part) at ($(phys_addr.west)+(0.2, -0.1)$) {\small PFN};
    \node[draw=black!50, fill=gray!20, minimum height=0.6cm, minimum width=1.1cm, anchor=west] (off_part) at (pfn_part.east) {\small Offset};

    % Arrow from PT Entry to PFN part
    \draw[arrow] (pt_entry.east) -- ++(0.3,0) |- (pfn_part.west);

    % Arrow from Virtual Offset to Physical Offset (Square from right)
    \draw[arrow, dashed] (off.east) -- ++(1.5, 0) |- (off_part.east) node[pos=0.25, auto, font=\tiny, text=gray] {Copy};

    % Final Result -> RAM
    \node[draw=black, fill=green!20, minimum width=2cm, minimum height=1.5cm, below=0.8cm of phys_addr] (ram) {\textbf{4KB Frame}};
    \node[anchor=north, font=\tiny] at (ram.south) {Physical Memory};

    \draw[arrow] (phys_addr.south) -- (ram.north);

\end{tikzpicture}
    \caption{Conceptual flow of a Multi-level Page Table Walk on x86-64}
    \label{fig:page_walk}
\end{figure}

This hierarchical design allows the operating system to map a very small portion of the vast 64-bit address space while consuming physical RAM only for the table entries that are actually in use. Unmapped regions of virtual memory simply have entries marked as "not present" at high levels of the tree, requiring no storage for the lower levels.

\subsubsection{The Cost of Translation: The TLB}

The sparsity of multi-level page tables solves the storage problem but introduces a significant performance cost. In a 4-level paging hierarchy, a single load instruction issued by the CPU requires the MMU to perform four separate memory reads (walking the tree from root to leaf) just to determine the physical address of the data. Without optimization, this would reduce effective memory bandwidth by a factor of five.

To mitigate this latency, processors employ a specialized, highly associative cache called the \textbf{Translation Lookaside Buffer (TLB)}. The TLB stores the recent results of page table walks (VPN to PFN mappings). When the CPU generates a virtual address, the MMU first checks the TLB. On a hit, translation is instantaneous. On a miss, the hardware performs the expensive "page walk," causing a pipeline stall.

For the operating system designer, the TLB represents a critical resource constraint. The TLB is finite and relatively small (often holding only a few thousand entries). If the working set of a process effectively accesses more pages than can fit in the TLB, the system enters a state of \textbf{TLB Thrashing}, where the CPU spends more cycles walking page tables than executing instructions.

This constraint motivates the support for \textbf{Huge Pages} (or Superpages). As analyzed by Navarro et al. \cite{navarro2002practical}, the concept of \textbf{TLB Reach} --- the total amount of memory accessible without incurring a TLB miss --- is a primary determinant of performance for memory-intensive applications. By mapping memory using 2 MiB or 1 GiB pages instead of 4 KiB pages, a single TLB entry can cover a significantly larger region of memory, reducing the frequency of misses. While implementing superpage support adds complexity to the Physical Memory Manager (requiring the allocation of physically contiguous blocks), the theoretical performance gains make understanding this hardware reality essential.

\subsection{Software Architecture: Separation of Concerns}
\label{subsec:vmm_architecture}

A common pitfall in early operating system development is the attempt to use the hardware page tables as the authoritative record of the system's memory state. While it is tempting to view the page table as the definition of the address space, relying solely on hardware structures introduces significant limitations.

Hardware page tables are designed for the consumption of the MMU, not the operating system developer. They are architecture-specific, rigid in format, and lossy in nature. For example, a standard x86-64 page table entry contains bits for "Present", "Read/Write", and "User/Supervisor", but it has no fields to store high-level concepts such as "this page belongs to the file \texttt{libc.so}" or "this page is a guard page for a thread stack." Furthermore, querying the state of the address space using only page tables --- such as finding a contiguous free region of virtual memory --- requires walking a sparse, multi-level tree, which is algorithmically inefficient ($O(N)$ scanning of a potentially massive structure).

To resolve these issues and ensure portability, a robust Virtual Memory Manager (VMM) should adopt the architectural split pioneered by the Mach microkernel \cite{rashid1987machine}. This design separates the memory subsystem into two distinct layers: the Machine Independent (MI) layer and the Machine Dependent (MD) layer.

\subsubsection{The Machine Independent Layer (VM Maps)}

The high-level layer manages the logical view of an address space. Instead of pages and frames, this layer operates on \textbf{Regions} (or Segments/VM Areas). A Region represents a contiguous range of virtual addresses sharing common properties, such as permissions and backing storage.

For example, a running process might be represented not by a page table, but by a list of abstract Region objects:
\begin{itemize}
    \item \texttt{0x00400000 - 0x00405000}: Code segment, Read-Execute, backed by ELF executable.
    \item \texttt{0x7FFF0000 - 0x7FFFFFFF}: Stack, Read-Write, backed by anonymous memory.
\end{itemize}

This layer is responsible for all policy decisions: identifying where to place a new allocation (e.g., handling \texttt{mmap}), validating memory access permissions during a page fault, and managing object lifecycle. Crucially, this code is written once and remains identical whether the underlying hardware is x86-64, ARM64, or RISC-V.

\subsubsection{The Machine Dependent Layer (Physical Map)}

The lower layer, often referred to as the \textbf{pmap} (Physical Map) in BSD/Mach terminology, serves as the driver for the MMU. It exposes a standardized interface to the MI layer with operations such as \texttt{pmap\_enter(va, pa, prot)} or \texttt{pmap\_remove(va, size)}.

The responsibility of this layer is strictly mechanical: it translates the abstract commands from the MI layer into the specific bit-twiddling required by the hardware page tables. It handles the allocation of intermediate table pages (e.g., allocating a PDPT or PD) and manages the invalidation of the TLB. By encapsulating these details, the complexity of the hardware --- such as the specific layout of a PML4 entry or the differences in caching attributes between architectures --- is contained entirely within this module.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        font=\sffamily\footnotesize,
        layer/.style={draw, fill=white, minimum width=8cm, minimum height=1.5cm, rounded corners},
        component/.style={draw, fill=gray!10, minimum width=2cm, minimum height=0.8cm},
        arrow/.style={-Latex, thick}
    ]

    % MI Layer
    \node[layer, fill=blue!8, minimum width=8cm, minimum height=2.2cm] (mi) at (0, 3) {};
    \node[anchor=north west] at (mi.north west) {\textbf{Machine Independent (MI)}};
    \node[component, fill=white] (vmmap) at (-1.5, 3) {VM Map};
    \node[component, fill=white] (regions) at (1.5, 3) {Regions};
    \node[anchor=south] at (mi.south) {\textit{"Address Space", "Protection", "Sharing"}};

    % Interface
    \draw[dashed] (-4.5, 1.5) -- (4.5, 1.5);
    \node[fill=white, inner sep=2pt] at (-1.5, 1.5) {\textbf{pmap Interface}};

    % MD Layer
    \node[layer, fill=green!8, minimum width=8cm, minimum height=2.2cm] (md) at (0, 0) {};
    \node[anchor=north west] at (md.north west) {\textbf{Machine Dependent (MD)}};
    \node[component, fill=white] (pt) at (-1.5, 0) {Page Tables};
    \node[component, fill=white] (tlb) at (1.5, 0) {TLB Flush};
    \node[anchor=south] at (md.south) {\textit{"PTE Bits", "CR3 Reload", "Invlpg"}};

    % Hardware
    \node[draw, fill=gray!20, minimum width=6cm, minimum height=1cm] (hw) at (0, -2) {Hardware (MMU)};

    % Arrows
    \draw[arrow] (mi) -- (md);
    \draw[arrow] (md) -- (hw);
    
    \end{tikzpicture}
    \caption{Separation of Concerns in Virtual Memory Management}
    \label{fig:vmm_architecture}
\end{figure}

This separation of concerns is the single most important architectural decision in the design of the memory subsystem. It allows the OS to support advanced features such as lazy allocation and copy-on-write (which rely on the logical view) without cluttering the hardware drivers, while simultaneously allowing the kernel to be ported to new architectures by rewriting only the pmap implementation.

\subsection{Data Structures: Managing the Address Space}
\label{subsec:vmm_data_structures}

Once the Virtual Memory Manager is architecturally separated into machine-independent regions (as described in Section \ref{subsec:vmm_architecture}), the operating system faces a classic data structure problem. A process's address space is essentially a collection of non-overlapping regions, separated by unmapped "holes."

The kernel must perform two frequent operations on this collection:
\begin{enumerate}
    \item \textbf{Lookup:} Given a virtual address $A$ (triggered by a page fault), find the Region $R$ such that $R_{start} \le A < R_{end}$, or determine that no such region exists.
    \item \textbf{Insertion/Deletion:} When a process allocates memory (e.g., \texttt{mmap}) or frees it (e.g., \texttt{munmap}), the OS must find a free gap of sufficient size and insert or remove a region object.
\end{enumerate}

The choice of data structure for storing these regions has a direct impact on system responsiveness, particularly for large, complex applications like web browsers or database engines, which may map thousands of distinct memory areas.

\subsubsection{Linked Lists vs. Trees}

The simplest implementation, found in many early-stage kernels, is a sorted singly or doubly linked list. This approach is memory-efficient and simple to implement. However, it imposes an $O(N)$ time complexity on lookups. For a process with thousands of mappings, walking a linear list on every page fault introduces unacceptable latency. While linked lists remain useful for sequential iteration (such as printing a memory map to a debug console), they are insufficient for the hot path of memory management.

To achieve logarithmic time complexity ($O(\log N)$), modern operating systems utilize Balanced Binary Search Trees (BSTs). By indexing regions based on their start address, the kernel can quickly locate the containing region for any faulting address.

\subsubsection{Splay Trees and Temporal Locality}

While generic balanced trees (such as AVL trees) provide worst-case guarantees, the access patterns of virtual memory are not random. They exhibit a property known as \textbf{Temporal Locality of Reference}: if a process accesses an address, it is highly likely to access that address, or one near it, again in the immediate future.

This behavioral characteristic motivates the use of \textbf{Splay Trees}. A Splay Tree is a self-adjusting binary search tree with a unique property: whenever a node is accessed, it is rotated to the root of the tree. Consequently, frequently accessed memory regions tend to stay near the top of the tree, allowing subsequent lookups to occur in near-constant time $O(1)$.

This approach, pioneered by Sleator and Tarjan~\cite{sleator1985splay}, is notably utilized by the BSD family of operating systems (FreeBSD, NetBSD) for their VM map structures. It trades the strict balance of other trees for an amortized efficiency that adapts to the specific runtime behavior of the process. (Figure~\ref{fig:splay_tree})

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\scriptsize,
    node/.style={circle, draw, minimum size=0.6cm, fill=white},
    target/.style={circle, draw, minimum size=0.6cm, fill=red!20, thick},
    arrow/.style={-Latex}
]

% BEFORE SPLAY
\node[anchor=south] at (0, 0.4) {\textbf{Before Access}};
\node[node] (root) at (0, 0) {A};
\node[node] (b) at (-1, -1) {B};
\node[target] (c) at (-0.5, -2) {C};

\draw (root) -- (b);
\draw (b) -- (c);
\draw (root) -- (1, -1);

% ARROW
\draw[-Latex, ultra thick, blue] (1.5, -1) -- (2.5, -1) node[midway, above] {Splay(C)};

% AFTER SPLAY
\node[anchor=south] at (4, 0.4) {\textbf{After Access}};
\node[target] (newroot) at (4, 0) {C};
\node[node] (newb) at (3, -1) {B};
\node[node] (newa) at (5, -1) {A};

\draw (newroot) -- (newb);
\draw (newroot) -- (newa);
\draw (newa) -- (5.5, -2);
\draw (newb) -- (2.5, -2);

\node[align=center, anchor=north] at (2, -2.5) {Node C rotates to root,\\optimizing future access};

\end{tikzpicture}
\caption{Splay Tree Optimization: Deep nodes move to the root upon access.}
\label{fig:splay_tree}
\end{figure}

\subsubsection{Red-Black Trees}

In contrast, the Linux kernel employs \textbf{Red-Black Trees} (RB-Trees) for managing virtual memory areas (\texttt{vm\_area\_struct})~\cite{gorman2004understanding}. RB-Trees enforce a stricter balancing invariant, guaranteeing that the longest path from root to leaf is no more than twice as long as the shortest path. 

While RB-Trees do not adapt to access patterns like Splay Trees, they offer predictable worst-case performance and are generally faster for insertion and deletion operations when rebalancing is required. The choice between Splay Trees (adaptive) and Red-Black Trees (deterministic) represents a classic trade-off in kernel engineering, balancing the speed of the "hot path" (page faults) against the latency of modification (mmap/munmap).

\subsection{The Theory of Locality}
\label{subsec:vmm_locality}

With the software architecture and data structures established, the operating system must adopt a strategy for mapping physical memory to virtual addresses. A naive approach is \textbf{Eager Allocation}: when a process requests memory (e.g., loading a program or calling \texttt{malloc}), the kernel immediately finds free physical frames, zeroes them, and updates the page tables.

While simple, eager allocation is inefficient. Observation of real-world program behavior reveals that applications rarely utilize their entire allocated address space simultaneously. Instead, they exhibit \textbf{Locality of Reference}. As formalized by Peter J. Denning in the \textit{Working Set Model}~\cite{denning1968working}, the subset of memory pages a process is actively using at any specific time interval (its working set) is typically a small fraction of its total virtual size.

Understanding this theory is crucial for optimizing physical RAM usage and system responsiveness. (Figure~\ref{fig:working_set})

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\scriptsize,
    page/.style={draw, fill=white, minimum width=0.4cm, minimum height=0.6cm},
    active/.style={draw, fill=red!30, minimum width=0.4cm, minimum height=0.6cm},
    window/.style={draw=blue, thick, rounded corners, dashed}
]

% Timeline
\draw[->] (0, 0) -- (10, 0) node[right] {Time};

% Page Trace at t=1
\node[page] at (0.5, 0.5) {1};
\node[page] at (1.0, 0.5) {2};
\node[active] at (1.5, 0.5) {1};
\node[active] at (2.0, 0.5) {1};
\node[active] at (2.5, 0.5) {2};

% Window 1
\draw[window] (1.2, 0.1) rectangle (2.8, 1.0);
\node[blue, below] at (2.0, 0) {$WS(t_1)$};

% Page Trace at t=5 (Phase Change)
\node[page] at (4.0, 0.5) {3};
\node[active] at (4.5, 0.5) {4};
\node[active] at (5.0, 0.5) {5};
\node[active] at (5.5, 0.5) {4};

% Window 2
\draw[window] (4.2, 0.1) rectangle (5.8, 1.0);
\node[blue, below] at (5.0, 0) {$WS(t_2)$};

% Explanation
\node[align=left, anchor=west] at (6.5, 0.5) {Working Set $\Delta$\\Changes over time.};

\end{tikzpicture}
\caption{The Working Set Model: Localized access patterns over time.}
\label{fig:working_set}
\end{figure}

\subsubsection{Justification for Laziness}

The principle of locality provides the theoretical justification for \textbf{Lazy Allocation}. If a program allocates a large buffer but only processes it sequentially, or if a program links against a large library but only calls a single function, allocating physical RAM for the unused portions is a waste of resources. 

By deferring allocation until the last possible moment --- specifically, the moment the CPU actually tries to access the data --- the operating system achieves two goals:
\begin{enumerate}
    \item \textbf{Conservation:} Physical RAM is reserved only for pages that contain active data, allowing the system to run more processes concurrently than would be possible if every virtual allocation required immediate physical backing.
    \item \textbf{Latency Reduction:} The cost of program startup is reduced. Instead of pausing to allocate and zero-fill megabytes of memory before execution begins, the program starts immediately, incurring minor costs incrementally as it runs.
\end{enumerate}

\subsection{Mechanisms: Overcommitment and Optimization}
\label{subsec:vmm_mechanisms}

The realization of the Theory of Locality in a modern kernel relies on three specific mechanisms: Demand Paging, Copy-on-Write, and the Zero Page. These features allow the kernel to practice memory overcommitment —-- promising more virtual memory to processes than physically exists --— by deferring actual allocation until access occurs. They fundamentally change the role of the Page Fault Exception (\texttt{\#PF}) from an error condition to a core signaling mechanism of the memory subsystem.

\subsubsection{Demand Paging}

Demand Paging is the implementation of lazy allocation. When a region of memory is mapped (conceptually) in the Machine Independent layer, the Machine Dependent layer does \textit{not} create valid Page Table Entries (PTEs). Instead, the entries are marked as "Not Present."

When the CPU attempts to access such an address, the MMU raises a Page Fault. The kernel's fault handler intercepts this exception and consults the high-level VM Map:
\begin{enumerate}
    \item \textbf{Validation:} The kernel checks if the faulting address lies within a valid Region. If not, the process is terminated (e.g., Segmentation Fault).
    \item \textbf{Allocation:} If the access is valid, the PMM allocates a physical frame.
    \item \textbf{Mapping:} The kernel updates the page tables to map the new frame to the faulting virtual address.
    \item \textbf{Resume:} The kernel executes the \texttt{return from interrupt} instruction. The CPU retries the memory access, which now succeeds transparently. (Figure~\ref{fig:demand_paging_flow})
\end{enumerate}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    state/.style={draw, rounded corners, fill=white, minimum height=1cm, align=center},
    decision/.style={draw, diamond, aspect=2, fill=yellow!10, font=\scriptsize},
    arrow/.style={-Latex, thick}
]

% Nodes
\node[state, fill=blue!5] (alloc) at (0, 4.5) {1. Virtual Alloc (mmap)\\ \textit{Update VMA List}};
\node[state, dashed] (access) at (0, 3.0) {2. User Accesses Memory\\ \textit{Hardware Checks PTE}};
\node[decision] (check) at (0, 1.0) {PTE Present?};

% Success to the Left
\draw[arrow] (check.west) -- node[above] {Yes} (-2.0, 1.0) node[left, align=center] (success) {\textbf{Success}\\ \textit{Process Continues}};

% Fault Path to the Right
\node[state, fill=red!10] (fault) at (4.0, 1.0) {3. Page Fault (\#PF)\\ \textit{Kernel Handler}};
\node[state, fill=green!10] (phys) at (4.0, -1.5) {4. Physical Alloc (PMM)\\ \textit{Get Frame}};
\node[state] (update) at (0, -1.5) {5. Update Page Table\\ \textit{Map Frame -> Page}};

% Flow Connections
\draw[arrow] (alloc) -- (access);
\draw[arrow] (access) -- (check);
\draw[arrow] (check.east) -- node[above] {No} (fault);
\draw[arrow] (fault) -- (phys);
\draw[arrow] (phys) -- (update);

% Resume Loop (Wrapping around the Left)
\draw[arrow] (update.west) to[out=180, in=180, looseness=2.5] node[left, pos=0.5] {6. Resume (Retry)} (access.west);

\end{tikzpicture}
\caption{The Cycle of Demand Paging}
\label{fig:demand_paging_flow}
\end{figure}

\subsubsection{The Zero Page Optimization}

A special case of demand paging involves large regions of empty memory (e.g., the BSS section of an executable or heap memory returned by \texttt{calloc}). Allocating distinct physical frames of zeros for these regions is wasteful if the application reads from them but never writes.

To optimize this, the kernel maintains a single global physical page filled with zeros. When a process requests anonymous memory, the VMM maps this single physical "Zero Page" into the process's page tables as Read-Only. An arbitrary number of virtual pages across all processes can map to this one physical frame.

If a process reads the memory, it sees zeros. If a process attempts to write, the MMU triggers a protection fault. The kernel catches this, allocates a new, private physical frame, copies the zeros into it, updates the mapping to Read-Write, and resumes execution.

\subsubsection{Copy-on-Write (CoW)}

Perhaps the most powerful application of virtual memory is Copy-on-Write, which is essential for the efficient implementation of some operations (e.g., the POSIX \texttt{fork()} primitive in Linux kernel).

When a process is duplicated, copying its entire physical memory to the child process would be slow and memory-intensive. Instead, the VMM performs a "virtual copy":
\begin{enumerate}
    \item The child process receives a copy of the parent's page tables.
    \item Both parent and child point to the \textit{same} physical frames.
    \item All shared pages are marked as \textbf{Read-Only} in both processes.
\end{enumerate}

This ensures that the expensive operation of memory copying occurs only when strictly necessary, and only at the granularity of a single page.

\subsubsection{Swapping}

It is worth noting that a complete virtual memory subsystem typically includes a swapping mechanism to sustain high levels of memory overcommitment. While the mechanisms above allow virtual allocation to exceed physical capacity, swapping ensures stability when physical memory is finally exhausted. This allows inactive pages to be evicted to secondary storage, freeing physical RAM for active tasks. However, the theory presented in this section focuses on the fundamental aspects of memory virtualization: isolation, protection, and hardware abstraction. The implementation of swap and page replacement algorithms is considered a separate, advanced topic extending beyond these core principles.

\section{Discovering External Devices and System Capabilities}
\label{sec:discovery}

Upon the successful initialization of the fully functional memory management subsystem, the kernel can proceed to discover external devices and implement subsystems for capabilities such as storage, input, and networking. It is not necessary to support an exhaustive list of peripherals to achieve a minimal working kernel. Therefore, this section covers only the absolute minimum set of devices required to support the scheduler and execute user-space programs.

\subsection{Discovery and Abstraction (Drivers)}

Device discovery strategies vary fundamentally between hardware architectures, necessitating platform-specific implementations that feed into a unified kernel abstraction. 

On desktop and server platforms, particularly \textbf{x86-64}, the hardware is designed to be self-describing. The \textbf{ACPI} subsystem \cite{ACPI-spec} is utilized to enumerate core platform components (such as CPU cores, LAPICs, and IOAPICs) and handle power management. For peripheral devices, the \textbf{PCI/PCIe} bus \cite{osdev-pci} provides a dynamic enumeration mechanism. The kernel can probe the PCI bus, read vendor and device IDs from configuration space, and automatically load the appropriate drivers without prior knowledge of the hardware topology.

In contrast, embedded architectures such as \textbf{ARM64} and \textbf{RISC-V}, particularly in System-on-Chip (SoC) implementations, often rely on simple system buses (e.g., AXI, AHB) that lack discovery capabilities. A UART controller or a Timer on an SoC is simply mapped to a fixed physical address. The kernel cannot "ask" the hardware if a device exists at a specific address. Attempting to read from an unmapped address could result in a bus error or system hang. 

To address this, these architectures utilize the \textbf{Device Tree} (DTB) mechanism \cite{Devicetree-Spec}. Instead of hardcoding memory addresses into the kernel source (which would require a unique kernel binary for every specific motherboard), the bootloader passes a binary data structure to the kernel at boot. This blob describes the hardware topology, including memory-mapped addresses, interrupt lines, and dependency relationships. The kernel parses this tree to "discover" devices that are physically present but electronically silent.

Regardless of whether a device is found via PCI enumeration, ACPI tables, or Device Tree parsing, the architecture-independent kernel must provide a unified interface for device registration. This ensures that the upper layers of the operating system remain agnostic to how the hardware was detected. An example driver interface, designed to abstract these differences, is presented below:

\begin{cppcode}[caption={Example Driver Abstraction}, label={lst:drive_abstract}]
    struct ExampleDriver {
        /* Device description */
        char* name;
        int frequency;
        int latency;
        
        /* Driver data */
        void *own_data; 
    
        /* Operations */
        struct callbacks {
            u32 (*open)(void* device);
            u32 (*read)(void* device, char* buffer, int size);
            u32 (*write)(void* device, const char* buffer, int size);
            u32 (*close)(void* device);
        } cbs;
    };
\end{cppcode}

This structure must define crucial operations for the specific device type alongside data describing its functionality, allowing the kernel to select the most appropriate driver either through user configuration or automatic selection algorithms.

\subsection{Core Local Storage}

Implementing an SMP kernel necessitates an infrastructure for accessing core-local data early in the boot process to facilitate future scalability and prevent extensive refactoring. This initialization must occur after memory setup and CPU topology discovery, as structures must be allocated for each core individually. 

Fundamentally, Core Local Storage (CLS) enables the kernel to maintain state unique to each processing unit, such as the currently executing thread, scheduler queues, and performance counters, without the contention and latency costs associated with global synchronization primitives. To achieve this efficiently, the hardware architecture must provide a low-latency mechanism to retrieve the address of these structures. While implementation details vary, most modern architectures dedicate a specific register for this purpose. For instance, ARM64 utilizes the specialized \texttt{TPIDR\_EL1}\cite{AArch64-cls} system register, whereas RISC-V reserves the \texttt{tp}\cite{RISCV-registers} (Thread Pointer) general-purpose register by convention.

On the x86-64 architecture, this is typically implemented using the \texttt{GS} or \texttt{FS} segment registers. Since the full 64-bit base address cannot be loaded into these segment selectors directly, the kernel must utilize Model Specific Registers (specifically \texttt{IA32\_GS\_BASE} or \texttt{IA32\_KERNEL\_GS\_BASE} \cite{Intel-segment-loading}) to map the per-core structure to the logical address space. The \texttt{SWAPGS}\cite{osdev-gs} instruction is then employed during interrupt entry and exit to switch between user-space and kernel-space thread-local storage transparently.

\subsection{Core Identification}

A critical design challenge in this domain is the identification of the executing core. While mostly all processors provide unique hardware identifiers (e.g., APIC ID on x86 \cite{Intel-proc-ids} or MPIDR on ARM \cite{ARM-MPIDR-EL1}), these values are intrinsically tied to the physical topology of the silicon. Consequently, they are often non-contiguous, sparse, or hierarchical, rendering them unsuitable for directly indexing internal kernel arrays. To address this, an operating system typically implements a mapping layer during hardware discovery, assigning a dense, sequential \textbf{Logical ID} (ranging from 0 to $N-1$) to each active core. Generally speaking, another abstraction is handy here.

\subsection{Timing Design}

To support the preemptive scheduling model, the kernel requires a robust timing infrastructure to track execution time. Before discussing timing devices, a fundamental design decision must be made regarding the kernel type: \texttt{tickless} or \texttt{ticking}. Ticking kernels rely on a global constant defining the frequency of periodic timing interrupts. These values typically operate with millisecond granularity, as higher frequencies would degrade system performance. In contrast, tickless kernels do not define periodic interrupts. Instead, they schedule the next timing event based on immediate requirements, such as the sleep schedule or the time slice of the currently running process. It is important to note that ticking kernels also require higher granularity interrupts if support for \texttt{nanosleep} is needed (e.g., Linux implementation: \ref{subsec:linux-timing}).

\subsection{Clocks}
\label{subsec:clocks-tutorial}

The first category of devices required is a simple clock capable of tracking the time elapsed since system startup, primarily for gathering statistics and maintaining timekeeping. These are essential for placing tasks on a timeline and scheduling interrupts for high-precision sleep operations. Without such capabilities, waking tasks would require iterating over all sleeping entities to decrement remaining time, resulting in $O(n)$ complexity. Since interrupt handlers must execute rapidly, tasks should be managed in a high-performance timeline-based priority queue. On x86-64, the TSC clock \cite{IntelManual-TSC} is suitable for measuring system uptime (sufficient for the scheduler), while the RTC is used for wall-clock time displayed to the user. If multiple timers are available, the kernel should select the one offering the best performance-accuracy ratio, obviously from the set of supported ones.

\subsection{Event Clocks}
\label{subsec:event_clocks}

At this stage, the approach to timing event logic must be established, depending on the target architecture and the capabilities exposed to users. Crucially, to implement preemption as discussed in Section \ref{subsec:scheduling_approaches}, these devices are required to forcibly interrupt the currently running task when its time slice expires.

The first consideration is sleep granularity. A standard precision of approximately one millisecond is achievable on nearly every architecture using simple periodic interrupts. However, implementing high-precision nanosecond or microsecond sleep requires timer devices capable of extremely rapid reconfiguration. The minimum time interval between consecutive events is strictly limited by the hardware configuration latency. The kernel cannot schedule an interrupt to occur sooner than the time required to program the device. Older timing hardware often requires slow I/O port interactions, rendering them unsuitable for high-precision scheduling, whereas modern devices utilize faster interfaces. Additionally, it is common for a single hardware component to serve a dual purpose, acting as both a timekeeping source (Clock \ref{subsec:clocks-tutorial}) and an interrupt generator (Event Clock).

Another critical factor is the support for multiple cores. In an SMP environment, it is preferable to utilize timing devices that reside locally on the cores and operate independently, thereby maximizing performance. While most modern architectures provide such devices (e.g., LAPIC Timer on x86-64), some older or embedded SMP platforms may lack them. In such cases, the kernel must utilize a global timer and employ interrupt-driven inter-core communication to simulate local interrupts.

Consequently, a suitable timing device must be selected. When supporting multiple architectures, the kernel typically implements drivers for all available options and dynamically selects the appropriate one based on device capabilities.

An example of device availability schemes by architecture is as follows:
\begin{itemize}
\item SMP fast local timer $\rightarrow$ core local interrupt system + nanosleep mechanism available.
\item SMP local periodic-only timer $\rightarrow$ core local interrupt system + low precision msleep only available.
\item SMP fast non-local timer $\rightarrow$ owner core of timer interrupts propagates the interrupt to target cores + nanosleep.
\item SMP non-local periodic-only timer $\rightarrow$ owner core of timer interrupts propagates the interrupt to target cores + low precision msleep only available.
\end{itemize}

Most modern architectures support both local and high-precision timers. For example, on x86-64 with SMP, the presence of local LAPIC Timers \cite{IntelManual-APIC} (high precision, one-shot) is guaranteed, significantly simplifying the implementation.

\subsection{Keyboard}

Lastly, we need some way to provide input to the kernel on the real machine, not only emulated environment. For that, we will need some keyboard handling. First easier way is to utilize some simple ports like PS/2 \cite{osdev-ps21, osdev-ps22} if they are available on our device (it is quite popular on embedded systems). Otherwise, we will probably have to implement some USB drivers, which will be much more difficult \cite{usb20-spec, osdev-usb}.

\section{File Systems}

Once the kernel has established memory management and basic device communication, the next major milestone is the management of persistent data. In a raw hardware context, storage devices (Hard Disk Drives, SSDs, NVMe drives) present themselves as vast, linear arrays of blocks (typically 512~B or 4~KiB) accessed via Linear Block Addressing (LBA).

Operating directly on raw sectors is impractical for a general-purpose operating system. To manage code (executables) and data effectively, the kernel requires an abstraction layer that transforms this linear physical storage into a hierarchical structure. This is the role of the File System. In the context of OS development, three primary challenges arise: defining a unified interface for disparate storage formats, abstracting the physical access to storage media, and bootstrapping the initial file system to load user space programs.

\subsection{Virtual File System (VFS)}

A non-trivial kernel must support multiple file system formats (e.g., FAT32 \cite{fat-spec} for UEFI compatibility, Ext2 \cite{ext2-spec} for Linux compatibility, or ISO9660 \cite{iso9660-spec} for optical media). Implementing system calls like \texttt{open()} or \texttt{read()} separately for each format would lead to a tight coupling between the kernel core and specific driver implementations.

To resolve this, modern kernels implement a \textbf{Virtual File System} (VFS). The VFS is a kernel subsystem that provides a unified abstract interface for file operations. It defines generic structures --- such as file metadata, directory entries, and mounted filesystems --- and a set of operations that concrete file system drivers must implement. By using a function pointer table, the VFS decouples the high-level system calls from low-level storage details.

\subsection{Block Device Abstraction}

While the VFS abstracts the logical format of data (files and directories), the kernel must also abstract the physical location of that data. A file system driver, such as FAT32, contains logic to manipulate File Allocation Tables and directory clusters, but it should not contain logic for communicating with a SATA controller or managing TCP/IP packets.

To achieve this separation of concerns, the kernel implements a \textbf{Block Device Interface}. This interface treats every storage medium as a generic array of fixed-size sectors, regardless of the underlying hardware implementation. The interface typically exposes a minimal contract:
\begin{itemize}
    \item \texttt{read\_sectors(lba, count, buffer)}
    \item \texttt{write\_sectors(lba, count, buffer)}
    \item \texttt{get\_device\_info()} (total sectors, sector size)
\end{itemize}

This abstraction allows specific storage drivers to transparently fulfill requests:
\begin{itemize}
    \item \textbf{Physical Disks}: An AHCI driver translates the LBA request into hardware commands sent over the bus.
    \item \textbf{RAM Disks}: A RAM disk driver simply performs a \texttt{memcpy} to or from a specific offset in physical memory.
    \item \textbf{Network Block Devices}: A driver encapsulates the request into network packets, transmits them to a remote server, and awaits the response.
\end{itemize}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.0cm,
    block/.style={
        rectangle, 
        draw=black!70, 
        rounded corners=3pt, 
        minimum width=3.5cm, 
        minimum height=0.8cm, 
        align=center, 
        fill=white,
        font=\small
    },
    arrow/.style={-Latex, thick}
]

\node (fs) [block, fill=gray!10] {File System Driver\\(e.g., FAT32)};

\node (bdev) [block, below=0.8cm of fs, fill=gray!20, minimum width=8cm] {\textbf{Block Device Interface}\\ \texttt{read(LBA) / write(LBA)}};

\node (ram) [block, below=0.8cm of bdev, fill=gray!10, xshift=-4cm] {RAM Disk Driver\\ \scriptsize (\texttt{memcpy})};
\node (disk) [block, below=0.8cm of bdev, fill=gray!10] {AHCI Driver\\ \scriptsize (SATA Commands)};
\node (net) [block, below=0.8cm of bdev, fill=gray!10, xshift=4cm] {Network Driver\\ \scriptsize (TCP/IP Packets)};

\draw[arrow] (fs) -- (bdev);
\draw[arrow] (bdev.south) -- (ram.north);
\draw[arrow] (bdev.south) -- (disk.north);
\draw[arrow] (bdev.south) -- (net.north);

\end{tikzpicture}
\caption{Decoupling logic via Block Device Abstraction}
\label{fig:block_abstraction}
\end{figure}

By implementing this layer, the exact same FAT32 code can handle a USB stick, a partition in memory, or a remote disk over the network without modification.

\subsection{The Bootstrapping Problem and Initrd}

A "chicken-and-egg" problem exists during the early boot process. To load a user space program (like a shell), the kernel needs a file system driver. However, on a microkernel or a modular monolithic kernel, file system drivers and disk drivers might technically reside on the disk itself. Furthermore, writing an NVMe or AHCI (SATA) driver and a full partition parser is a massive undertaking that blocks the development of other kernel features.

To bypass this complexity during the early stages of development, one can utilize an \textbf{Initial Ramdisk} (Initrd). This is a small file system image loaded into memory by the bootloader (e.g., GRUB via the Multiboot2 protocol) alongside the kernel binary.

Because the bootloader handles the disk I/O to load this image, the kernel does not initially require complex disk drivers. Using the Block Device Abstraction described in Figure~\ref{fig:block_abstraction}, the kernel simply wraps the memory range containing the image in a RAM Disk block driver. It then:
\begin{enumerate}
    \item Initializes the Block Device pointing to the memory address.
    \item Mounts the file system residing on that device as root (\texttt{/}).
    \item Loads user space executable from this memory.
\end{enumerate}

This approach isolates the development of other subsystems from the complexities of physical storage hardware, serving as a critical bridge to a self-hosting system.

\section{Screen Drawing}
\label{sec:screen_drawing_theory}

Visual feedback constitutes the primary interface between the operating system and the user. However, the mechanism by which a kernel renders pixels to a display evolves significantly during the system's lifecycle, transitioning from primitive hardware interfaces to complex, software-managed abstractions.

\subsection{Early Output and Debugging}

In the first stages of kernel initialization, the full graphics subsystem is unavailable. To facilitate debugging and provide immediate feedback, operating systems rely on low-bandwidth, high-reliability communication channels provided by the hardware platform.

The industry standard for this purpose is the \textbf{Serial Interface} (e.g., UART). Available across virtually all architectures --- from embedded RISC-V microcontrollers to server-grade x86-64 and ARM64 SoCs --- serial ports allow the kernel to transmit text streams byte-by-byte. This output is typically captured by a host machine via a physical cable or an emulator's logging facility. Because serial drivers are structurally simple and do not require complex memory management or bus enumeration, they serve as the reliable "black box" recorder for early system crashes.

\subsection{The Linear Framebuffer Abstraction}

As the system boots, it must transition from text streams to graphical output. Modern system firmware (such as UEFI on PC platforms or U-Boot on embedded systems) typically abstracts the display hardware into a \textbf{Linear Framebuffer}.

Fundamentally, a linear framebuffer is a contiguous region of physical memory (Memory-Mapped I/O) where the data represents the visual state of the screen. The CPU interacts with the display by writing directly to these memory addresses. To utilize this region correctly, the kernel must respect specific geometric constraints provided by the firmware or hardware description structures (e.g., Device Tree):

\begin{itemize}
    \item \textbf{Pixel Format:} The layout of individual pixels (e.g., whether the byte order is Red-Green-Blue or Blue-Green-Red, and the bit-width of each channel).
    \item \textbf{Stride (Pitch):} The width of a scanline in memory bytes. This value often exceeds the logical width of the screen ($\text{width} \times \text{bytes\_per\_pixel}$) due to hardware alignment requirements. Failing to account for stride results in skewed or distorted rendering.
\end{itemize}

\subsection{Software Rendering and Console Emulation}

Unlike historical hardware which often included dedicated text-mode generators, modern display controllers are strictly pixel-based. Consequently, the operating system is responsible for all rendering logic.

To display text logs or a command-line interface, the kernel must implement a \textbf{Software Rasterizer}. This subsystem loads a bitmap font, parses the bitmask for a specific character, and manually writes the corresponding pixel colors into the framebuffer. This process effectively emulates a textual console on top of a graphical surface, allowing the OS to present diagnostic information in a human-readable format without relying on specialized hardware modes.

\subsection{The Concurrency Problem}

In a multitasking operating system, the display represents a shared resource. If multiple processes were granted direct write access to the physical framebuffer simultaneously, the screen would become a chaotic collage of uncoordinated pixel updates. Furthermore, direct access to physical memory mapped I/O by user-space applications poses a significant stability and security risk.

To address this, the operating system must virtualize the display hardware, revoking direct access from user applications and enforcing a strict ownership model.

\subsection{Windowing and Composition}

The solution to the concurrency problem is the implementation of a \textbf{Window Manager} or \textbf{Compositor}. In this architectural model, the kernel or a privileged display server acts as the sole owner of the physical framebuffer.

\begin{enumerate}
    \item \textbf{Virtual Surfaces:} Applications render their content into private, off-screen memory buffers (virtual framebuffers) located in standard RAM. They operate under the illusion that they own the entire display or a specific window.
    \item \textbf{Composition:} The Compositor reads these off-screen buffers and "blits" (copies and combines) them onto the physical framebuffer. This step allows the system to enforce window stacking (Z-order), transparency, and clipping.
    \item \textbf{Double Buffering:} To ensure visual consistency, the system typically employs double buffering. The Compositor constructs the next frame in a hidden \textbf{Backbuffer}. Only when the frame is fully rendered is it swapped to the \textbf{Frontbuffer} (the video memory scanned by the display controller). This technique prevents "screen tearing," an artifact that occurs when the display hardware refreshes while the memory is being written to.
\end{enumerate}

\section{Scheduling}
\label{sec:sched-tutorial}

With all requisite low-level components implemented, the development process transitions to the design and implementation of the scheduling subsystem. The initial step involves defining the data structures representing the units of execution. We begin by establishing the concept of a Process.

\subsection{Scheduling Design}
\label{subsec:scheduling_approaches}

Before going further, a fundamental design decision must be made regarding the characteristics of our scheduling algorithm. The choice of scheduling paradigm dictates the complexity of the kernel, the synchronization mechanisms required, and the hardware dependencies.

\subsubsection{Cooperative Scheduling}
In a cooperative scheduling model, the operating system never forcibly pauses a running task. Instead, the currently executing thread retains control of the CPU until it voluntarily yields execution (e.g., via a specific \texttt{yield()} system call) or blocks on an I/O operation.

\begin{itemize}
    \item \textbf{Advantages:} Implementation simplicity. Since context switches occur only at known, controlled points, race conditions are significantly rarer, and complex locking mechanisms are often unnecessary. 
    \item \textbf{Disadvantages:} A single malicious or poorly written program (e.g., one entering an infinite loop) can freeze the entire system, as the kernel never regains control to schedule other tasks.
\end{itemize}

\subsubsection{Preemptive Scheduling}
Preemptive scheduling is the standard for modern general-purpose operating systems. In this model, the kernel assigns a specific time slice to each task. If a task does not yield voluntarily before its time expires, the hardware generates an interrupt (driven by an Event Clock \ref{subsec:event_clocks}), returning control to the kernel. The scheduler then forcibly saves the current task's state and switches to another.

\begin{itemize}
    \item \textbf{Advantages:} Responsiveness and stability. No single task can monopolize the CPU, ensuring that the system remains responsive even if a user program hangs.
    \item \textbf{Disadvantages:} High complexity. Because a task can be interrupted at any instruction, the kernel must employ rigorous synchronization primitives (mutexes, spinlocks) to protect shared data structures from race conditions.
\end{itemize}

\subsubsection{SMP Scheduling}
Symmetric Multiprocessing (SMP) scheduling extends these concepts to systems with multiple processor cores. In this environment, the scheduler must not only decide \textit{which} task to run next but also \textit{where} to run it.

\subsection{Process}

The fundamental unit for resource management is the process. This object represents the execution environment for the threads operating within it. Consequently, it must act as a container for all resources shared between these threads. At a minimum, a process structure must manage:

\begin{itemize}
    \item \textbf{Page Tables} --- Architecture-specific structures describing the mapping between virtual and physical addresses.
    \item \textbf{Mapping Metadata} --- Structures enabling the tracking and allocation of virtual address ranges.
\end{itemize}

While additional resources may be required to implement features such as Inter-Process Communication (IPC), pipes, file operations, or synchronization primitives, the two elements listed above constitute the bare minimum for the process concept. 

A straightforward implementation involves defining separate structures for \texttt{Thread} and \texttt{Process} objects, although some designs merge these into a single task structure (as discussed in Section \ref{subsec:linux-task}).

At this stage, it is prudent to prepare data structures that allow for efficient querying of processes, facilitating operations such as termination, joining, or IPC. 

It is important to note that the creation and destruction of processes are computationally expensive operations. They require building page tables, mapping the kernel address space, and allocating memory for bookkeeping structures. To mitigate latency, some of this workload, particularly during destruction, can be deferred to dedicated kernel worker threads, allowing the system to return control to the caller more rapidly.

\noindent The minimal structure may look like this:
\begin{cppcode}[caption={Process Structure}, label={lst:exampleProcessStruct}]
    struct Process {
        /* Process resources */
        data_structures::DoubleLinkedList<VMemArea *> area_list;
        PPtr<void> page_table_root;
    };
\end{cppcode}

In a production environment, this structure would naturally include additional fields for management purposes, such as synchronization primitives, statistics, configuration flags, and a state enumeration for lifecycle management.

\subsection{Thread}

The second primary component of the scheduling subsystem is the thread. The thread represents the fundamental unit of execution --- the actual code consuming CPU time and utilizing resources from its parent process. To effectively restore, start, and switch execution, the system must track the thread's state, which includes CPU registers, the instruction pointer, and architecture-specific flags. All of this state information can be compressed into a single field: the thread's stack pointer. Although the same thing may be achieved with a different solution.

This approach simplifies the architecture significantly and enhances performance. It is worth noting that while certain architectures provide hardware-assisted context switching mechanisms (such as the legacy Hardware Task Switching on x86), these are rarely utilized in modern general-purpose kernels due to performance overhead and portability concerns. Instead, the standard approach is to unify context storage using the stack. Since the stack is heavily utilized, it is typically cached, eliminating the need to allocate separate memory regions for saving context. When saving context, registers are pushed onto the stack. When initializing a thread, the stack is artificially populated with boot-up values.

\noindent A concise initial structure can be defined as follows:
\begin{cppcode}[caption={Thread Structure First Iteration}, label={lst:os-tutorial-thread-v0}]
struct Thread {
    Process* owner;

    void *user_stack;
    void *user_stack_bottom;
};
\end{cppcode}

To manage the thread lifecycle, it is necessary to introduce a state enumeration. This field allows the scheduler to determine the appropriate action for a given thread. For instance, a waiting thread must be removed from the run queue, while a blocked thread must be removed from the specific wait queue it is blocked on.

\noindent The structure is updated to reflect these requirements:
\begin{cppcode}[caption={Thread Structure Second Iteration}, label={lst:os-tutorial-thread-v1}]
enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kBlocked,
    kTerminated,
};

struct Thread {
    Process* owner;
    ThreadState state;

    void *user_stack;
    void *user_stack_bottom;
};
\end{cppcode}

A critical design consideration regarding context switching is the location where the state is saved. Saving the execution state directly onto the user stack is inherently insecure. A malicious program could manipulate the stack pointer to overwrite critical kernel structures, or the kernel could unintentionally leak sensitive data (such as cryptographic keys) onto the user stack during a switch. Furthermore, to allow kernel code to be preempted or to block (i.e., perform a context switch while inside the kernel), the kernel state must be preserved. This necessitates a separate, protected kernel stack for each thread. Relying on a single kernel stack per core is insufficient for a preemptible kernel.

\noindent The final minimal structure is presented below:
\begin{cppcode}[caption={Thread Structure Third Iteration}, label={lst:os-tutorial-thread-v2}]
enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kBlocked,
    kTerminated,
};

struct Thread {
    Process* owner;
    ThreadState state;

    void *kernel_stack;
    void *kernel_stack_bottom;
    void *user_stack;
    void *user_stack_bottom;
};
\end{cppcode}

\subsection{Context}

\subsubsection{Context Creation}
\label{subsec:context-init}

With the context storage mechanism defined, the next step is context initialization. This requires a function responsible for fabricating a context frame on the thread's stack, adhering to a predefined schema. This schema must mirror the layout of registers and CPU flags expected by the context restoration logic. To create a new context, the stack is populated with initial values: zeros for general-purpose registers (or specific function arguments, depending on the ABI), the address of the entry point function, and architecture-specific default flags.

\subsubsection{Context Conversion}

Before a context switch can occur, an initial context must exist. This implies the creation of the first process and the preparation of all associated structures described previously, utilizing the context initialization logic \ref{subsec:context-init}. Several approaches exist to achieve this. For example, the kernel may spawn an initial user-space program (init) responsible for booting other programs, or it may create a kernel-space process to perform further system initialization.

\subsubsection{Context Switch}

Once multiple tasks exist, the system can perform context switching. Assuming the entire execution state resides on the stack, the context switch operation is relatively straightforward. The sequence of operations is as follows:

\begin{itemize}
    \item Push the current thread's context (registers and flags) onto its stack.
    \item Perform any necessary bookkeeping or state updates.
    \item Switch the stack pointer to the target thread's stack.
    \item Restore the state (registers and flags) from the new stack.
\end{itemize}

\noindent Based on x86-64 assembly, the implementation resembles the following:
\begin{nasmcode}[caption={Context Switch Pseudocode}, label={lst:context_switch_example}]
; RDI must store a pointer to next thread structure
ContextSwitch:
    sub rsp, _context_switch_stack_space_ext     ; Allocate space for saving registers.
    push_all_regs                                ; Save the state

    ; ... Some other booking, state changes -> depends on supported features
.done:
    mov rsp, [rdi+Thread.kernel_stack]   ; Change the stack

    pop_all_regs            ; Restore registers of NEW thread's stack
    add rsp, _all_reg_size  ; Deallocate register save space.
    iretq                   ; Restore state using interrupts mechanism
\end{nasmcode}

\subsection{Scheduler Internal Synchronization}

When designing the scheduler and internal kernel mechanisms, appropriate synchronization strategies are paramount to ensure data consistency. Furthermore, synchronization primitives must be used wisely to minimize latency, guarding only the critical sections. The two primary categories of synchronization required are:

\begin{itemize}
\item \textbf{Interrupt Synchronization} --- In a preemptive scheduling model, interrupts may induce context switches at arbitrary points. Even in non-preemptive scenarios, interrupt handlers may interact with shared kernel data. Consequently, critical sections must be protected against race conditions caused by asynchronous interrupt execution.
\item \textbf{SMP Synchronization} --- In a multiprocessor environment, concurrent access to shared data by multiple cores necessitates rigorous protection. For short critical sections, spinlocks \cite{spinlock} or memory barriers \cite{barriers} are suitable. However, for complex scenarios requiring extended waits, blocking locks (utilizing wait queues \ref{subsec:wait_queue}) are required.
\end{itemize}

\subsection{Scheduler}

With thread and process management structures established, the final component required is the mechanism responsible for selecting the next task for execution during preemption or voluntary yielding. The primary responsibility of the scheduler is to track the execution state of threads and determine the optimal task to run according to a specific policy. The complexity of the implementation is largely dictated by the chosen scheduling paradigm (Section \ref{subsec:scheduling_approaches}).

The simplest approach is a single-core, non-preemptive (cooperative) scheduler. In this model, synchronization concerns within the scheduler are minimal, as context switches occur only at explicit yield points. Furthermore, the sleeping mechanism can be simplified, as tasks are not preempted, allowing wake-up checks to occur solely during yield calls without strict timing reliance.

In contrast, a preemptive scheduler introduces significantly higher complexity. The system must manage CPU time allocation flexibly and safeguard against interrupts that may trigger context switches at any instruction. Additionally, preemptive scheduling requires precise timing mechanisms to handle sleeping tasks and enforce time slices accurately.

Implementing scheduling for Symmetric Multiprocessing (SMP) systems does not fundamentally alter the logic compared to a single-core preemptive scheduler, provided synchronization is handled correctly. The primary challenge lies in coordinating access between cores, typically achieved via spinlocks. Performance can be further optimized by introducing core-local schedulers coupled with a global load-balancing mechanism. In this architecture, internal scheduler synchronization is minimized, requiring synchronization only during task migration protocols.

A specific scheduling algorithm must be selected based on the optimization goals of the system. Common optimization criteria include:

\begin{itemize}
    \item Minimizing latency
    \item Minimizing waiting time
    \item Maximizing throughput
    \item Maximizing fairness
    \item Prioritizing critical tasks
\end{itemize}

No single scheduler can be ideal in all metrics. Therefore, a balanced solution suitable for the system's specific purpose is required. 

Furthermore, task prioritization is essential. For instance, audio or block device drivers require immediate execution to ensure data consistency and user experience. To address this, hybrid policies (such as \texttt{SCHED\_FIFO} in Linux \ref{subsec:linux-scheduler}) allow critical tasks to operate in a cooperative manner, yielding only when necessary, while other tasks remain preemptible.

To illustrate these concepts, we present the simplest form of a preemptive scheduler: a periodic, single-core Round Robin scheduler. This algorithm maintains a single queue of executable tasks. New tasks are appended to the tail of the queue, while the scheduler selects the task at the head for execution. At fixed intervals (e.g., every 10ms), the running task is interrupted, placed back at the tail of the queue, and the next task is dequeued. The following pseudocode demonstrates this logic:

\begin{cppcode}[caption={Round Robin Pseudocode}, label={lst:rr_pseudo_code}]

#define SCHED_PERIOD_TICK 10

struct Scheduler {
    LinkedList<Task*> tasks{};
} g_Scheduler;

extern void InitializeTimingModule(void (*handler)(), int period_ms);
extern void ContextSwitch(Task* task);
extern void LockInterrupts();
extern void UnlockInterrupts();
extern Task* GetCurrentRunningTask();

void AddTask(Task* task) {
    g_Scheduler->tasks.push_back();
}

Task* GetNext() {
    g_Scheduler->tasks.pop_front();
}

void Yield() {
    LockInterrupts();
    AddTask(GetCurrentRunningTask());
    ContextSwitch(GetNext());
    UnlockInterrupts();
}

void TimerHandler() {
    InitializeTimingModule(TimerHandler, SCHED_PERIOD_TICK);
}
\end{cppcode}

This simple model may suffice for an SMP system if implemented as a core-local scheduler, provided a load-balancing mechanism is available to redistribute tasks between cores.

\subsection{Blocking - Wait Queues}
\label{subsec:wait_queue}

Regardless of the scheduling algorithm employed, a mechanism is required to block threads and defer their execution until specific events occur, such as acquiring a lock, joining a thread, or awaiting I/O. This necessitates the management of multiple queues distributed throughout the kernel codebase, in contrast to the centralized run queues of the scheduler.

To distinguish between runnable and waiting threads, a specific \texttt{BLOCKED} state is assigned to tasks residing in a wait queue. This distinction simplifies resource management and structure cleanup. Intrusive data structures (Section \ref{subsec:intrusive}) are particularly advantageous in this context due to their efficient removal properties. The entity owning the wait queue is responsible for waking threads (transitioning them from the wait queue back to the scheduler's run queue) when the event occurs. Conversely, the task itself initiates the blocking process when it reaches a synchronization point.

\subsection{Sleep}

To implement sleeping functionality, two primary operations must be introduced: transitioning a task into a sleeping state and waking it upon the expiration of the requested duration. The first operation is relatively straightforward, similar to blocking on a synchronization primitive. The thread is removed from the scheduler's run queues, inserted into a dedicated sleeping data structure, and its state is updated to reflect that it is sleeping. The second operation is significantly more complex, as it requires determining the appropriate mechanism and timing for waking tasks.

In a cooperative scheduling model, the wake-up logic can simply be integrated into the function responsible for selecting the next task. However, in preemptive and high-precision scenarios, the system must actively manage the timing framework to wake threads without introducing unnecessary latency.

For kernels utilizing a periodic tick, handling sleep operations with granularity aligned to the tick interval is straightforward. Waking up tasks can be performed on every timer interrupt. We just have to count how many times the interrupt fired (how many ticks have passed). Efficient algorithms, such as the timer wheel \cite{linux_timer_wheel}, exist to manage this type of coarse-grained sleeping.

In contrast, for a tickless kernel where interrupts are scheduled dynamically, the implementation is more intricate. This approach requires maintaining an execution timeline, typically implemented using a priority queue to sort tasks by their wake-up timestamps. For high-precision sleep (e.g., \texttt{nanosleep}), hardware constraints become a critical factor. If the requested sleep duration is smaller than the overhead required to interact with the timing module or reconfigure the hardware timer, busy-waiting may be a more efficient alternative. Designing such a framework requires careful consideration of timing constraints, such as the time consumed by timer reconfiguration and ensuring that the calculated wake-up time has not already passed during the configuration process. Additionally, whenever the timer is reprogrammed, the system must ensure that any tasks scheduled to wake up before the new interrupt time are correctly handled.

\section{User Space}

With the foundational kernel components established, the development process transitions to enabling the execution of unprivileged tasks created by the user. To facilitate the execution of user-space programs, several key mechanisms and components must be implemented:

\begin{itemize} 
    \item \textbf{System Calls} --- A mechanism for communication with the kernel, providing a controlled method for privilege escalation.
    \item \textbf{User Space Libraries} --- The porting or implementation of standard libraries (such as \texttt{libc}) to run on top of the kernel interface.
    \item \textbf{Executable Loading} --- A loader capable of parsing a standard executable format (e.g., ELF) and correctly loading the code into memory.
    \item \textbf{Memory Protection} --- The enforcement of isolation through separate address spaces and the configuration of appropriate page-level privilege flags.
    \item \textbf{User Space Transition} --- A bootstrapping mechanism within the kernel to finalize initialization and jump to user space to begin executing user code --- strictly depends on architecture.
    \item \textbf{Compilation Toolchain} --- The extension and configuration of the compiler to support targeting the specific operating system.
\end{itemize}

\subsection{Syscalls}
\label{subsubsec:syscalls}

System calls (syscalls) are the primary interface through which user space applications request services from the operating system kernel. Modern processors enforce a strict separation between unprivileged applications and the operating system kernel to ensure stability and security. This privilege separation is architecturally defined: x86-64 uses Protection Rings (Ring 3 for users, Ring 0 for kernel) \cite{IntelManual-Protection-Rings}, ARM64 uses Exception Levels (EL0 for users, EL1 for kernel) \cite{ARM-Arch-Ref-Exception-Levels}, and RISC-V uses Privilege Modes (User Mode and Supervisor Mode) \cite{RISCV-Priv-Spec-Privilege-Levels}.

A system call is a synchronous transition wherein an application explicitly requests a service from the kernel. This process involves privilege escalation, switching stacks, performing the operation, and safely returning to the unprivileged state.

\paragraph{Design Considerations}

Designing an efficient and secure syscall mechanism is crucial for overall system performance and stability. The syscall interface must balance ease of use, performance, and security. Key considerations include:

\begin{itemize}
    \item \textbf{Performance:} Syscalls should introduce minimal overhead to avoid bottlenecks in application performance. Techniques such as fast syscall instructions and optimized context switching are essential.
    \item \textbf{Security:} The syscall interface must enforce strict access controls to prevent unauthorized access to kernel resources. This includes validating parameters and ensuring that user space applications cannot compromise kernel integrity.
    \item \textbf{Compatibility:} The syscall interface should maintain compatibility with established standards such as POSIX to ensure portability of existing software.
    \item \textbf{Extensibility:} The syscall interface should be designed to accommodate future extensions without breaking existing applications. This can be achieved through versioning and reserved syscall numbers.
\end{itemize}

\paragraph{Triggering Mechanisms}

There are two primary methods for implementing system calls, distinguished by the balance between simplicity and performance.

\begin{enumerate}
    \item \textbf{Software Interrupts:} Historically, systems used software interrupts (e.g., \texttt{int 0x80} on x86) to trigger syscalls. In this model, the CPU looks up a handler in the Interrupt Descriptor Table (IDT), performs extensive security checks (privilege verification, segment checks), and automatically pushes the execution context onto the kernel stack. 
    
    While robust and easy to debug (as it unifies syscalls with exception handling), this mechanism is slow, typically consuming approximately 100--200 CPU cycles per call depending on the architecture. It is often the recommended starting point for new OS projects due to its implementation simplicity.

    \item \textbf{Fast System Call Instructions:} Modern architectures provide dedicated instructions optimized for low-latency transitions (approximately 30--50 cycles). These bypass the complex interrupt logic but require the kernel to perform more manual state management.
    \begin{itemize}
        \item \textbf{x86-64 (\texttt{syscall}/\texttt{sysret}):} The CPU loads the kernel entry point and code segments from Model-Specific Registers (MSRs). The CPU does not switch the stack pointer automatically. The kernel must explicitly save the user stack and load the kernel stack immediately upon entry.
        \item \textbf{ARM64 (\texttt{svc}/\texttt{eret}):} The processor saves the state and return address into specific system registers (SPSR and ELR) \cite{ARM-Arch-Ref-Exception-Levels}. Unlike x86-64, ARM64 hardware automatically switches the stack pointer to the kernel's stack (\texttt{SP\_EL1}).
        \item \textbf{RISC-V (\texttt{ecall}/\texttt{sret}):} Following a minimalist philosophy, the hardware does little more than jump to the trap vector and switch privilege modes \cite{RISCV-Priv-Spec-System-Call}. Specifically, the hardware automatically saves the return address to \texttt{mepc} (or \texttt{sepc}), records the exception cause in \texttt{mcause} (or \texttt{scause}), updates the privilege bits in \texttt{mstatus} (or \texttt{sstatus}), and disables interrupts. However, the kernel is responsible for saving all general-purpose registers and manually swapping the stack pointer, typically using a scratch register.
    \end{itemize}
\end{enumerate}

\paragraph{The Application Binary Interface (ABI)}

Once the processor transitions to kernel mode, the kernel must identify which service is requested and where the arguments are located. This is defined by the ABI. To maximize performance, the ABI specifies that arguments are passed in CPU registers rather than on the stack. This avoids the overhead of the kernel accessing user memory directly, which creates security risks.

\paragraph{Dispatch and Execution}

Upon entry to the kernel, the raw system call number must be mapped to a specific internal kernel function. The standard and most efficient mechanism for this is a plain Dispatch Table. This approach utilizes a contiguous array of function pointers, where the system call number serves as the index. The dispatch logic is minimal:
\begin{enumerate}
    \item \textbf{Bounds Check:} The kernel verifies that the requested number is less than the total number of supported syscalls.
    \item \textbf{Invocation:} The kernel executes an indirect call to the address stored at \texttt{table[syscall\_number]}.
\end{enumerate}
This method guarantees $O(1)$ constant-time performance, ensuring that the latency of invoking a system call remains minimal. This approach requires syscall numbers to be contiguous. Gaps in the numbering space are typically handled by null pointers or stub functions in the table.

\paragraph{Security and Validation}

The syscall interface is the primary attack surface of the kernel. The kernel must treat all data from user space as untrusted.
\begin{itemize}
    \item \textbf{Pointer Validation:} The kernel must ensure that pointers passed by the user actually point to valid user space memory. Failing to perform this check allows a malicious program to trick the kernel into reading or overwriting its own internal structures (see the "Confused Deputy" problem \cite{confused_deputy_problem}).
    \item \textbf{TOCTOU (Time-of-Check to Time-of-Use):} If the kernel validates data in user memory and reads it later, a separate thread could modify that data in between. For example, a kernel might check that a filename pointer is valid, but before it reads the actual filename, another thread could modify that memory to point to a privileged file. To prevent this, the kernel must copy data into kernel-space buffers \textit{before} validation and processing.
\end{itemize}

\paragraph{Performance Optimization}

For high-frequency operations where even the minimal syscall overhead is too high (e.g., querying the system time), one may employ \textbf{vDSO} (virtual Dynamic Shared Object). This mechanism maps a read-only page of kernel memory containing specific data and code into the user process. The application can then execute a standard function call to read this data without ever triggering a context switch or entering kernel mode (e.g., linux \cite{linux_vdso}).

\subsection{Libc System Headers}

Although the system call interface constitutes the fundamental mechanism for interaction with the kernel, it operates at a level of abstraction that is unsuitable for general-purpose application development. In practice, application programmers do not invoke syscalls directly via assembly instructions. Instead, they rely on the \texttt{libc}, whose header files define the API between user space software and the operating system.

In the context of custom operating system development, system headers are commonly structured into three distinct categories in order to ensure clarity, portability, and standards compliance.

\paragraph{Standard ISO C Headers}
To enable existing software to compile and run on a new operating system, the \texttt{libc} implementation must provide all headers required by the ISO C standard \cite{iso_c_std}. These headers must strictly conform to the standardized function signatures and data types.

Internally, such functions act as abstraction layers over the kernel interface. The implementation of \texttt{fopen} typically allocates a user space data structure to represent the file stream and subsequently invokes the kernel's syscall to obtain a file descriptor or handle. This design effectively decouples application-level code from kernel-specific details such as syscall numbers, calling conventions, and ABI constraints.

\paragraph{POSIX and System Headers}
In addition to the ISO C standard, many operating systems implement parts of the POSIX specification \cite{posix_std} to improve compatibility with existing UNIX-like software. Headers associated with POSIX functionality are commonly placed within the \texttt{sys/} directory (e.g., \texttt{<sys/types.h>} or \texttt{<sys/stat.h>}).

\paragraph{OS-Specific Extensions}
Custom operating systems frequently introduce features that fall outside the scope of ISO C or POSIX, such as direct frame buffer access, specialized inter-process communication mechanisms, or hardware-specific controls.

It is considered best practice to place operating system-specific extensions within a clearly separated namespace, for example, under a dedicated directory such as \texttt{<alkos/...>}. This approach enables applications developed specifically for the target operating system to access unique kernel features.

\section{Host Environment --- Working OS}

\subsection{Root File System (Rootfs)}
\label{subsec:rootfs}

The root file system constitutes the fundamental user space environment of an operating system. It provides the configuration files and user space programs (e.g., a shell) that render the system usable and interactive. In the context of operating system development, the construction of a rootfs is a structured process that transforms source code and static resources into a binary file system image that can be parsed and mounted by the kernel.

Conceptually, the root file system construction process can be decomposed into four sequential phases, each responsible for a distinct transformation step within the build pipeline:
\begin{itemize}
    \item \textbf{Staging and Hierarchy Construction:} Creation of a temporary directory tree on the host system that reflects the intended structure of the target file system.
    \item \textbf{Artifact Compilation and Population:} Cross-compilation of user space programs followed by their installation into the corresponding locations within the staging directory.
    \item \textbf{Overlay:} Integration of static configuration files and resources into the staging area to complement the compiled executables.
    \item \textbf{File System Image Creation:} Conversion of the fully populated staging directory into a single binary image adhering to a chosen file system format.
\end{itemize}
An overview of this pipeline is illustrated in Figure \ref{fig:rootfs_pipeline}.

\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{res/rootfs-pipeline.svg}
    \caption{Rootfs creation pipeline}
    \label{fig:rootfs_pipeline}
\end{figure}

\subsubsection{Staging and Hierarchy Construction}

Rootfs creation begins with a staging phase, during which the build system generates a temporary directory tree on the host system. This tree mirrors the intended layout of the target operating system's file system and serves as an intermediate representation prior to image generation.

For early-stage operating systems, strict adherence to comprehensive standards such as the Linux Filesystem Hierarchy Standard (FHS), which mandates directories such as \texttt{/usr}, \texttt{/var}, and \texttt{/opt}, is often unnecessary. Consequently, a simplified directory structure is typically adopted, for example:
\begin{itemize}
\item \textbf{Binaries Directory (\texttt{/bin}):} Contains executable programs compiled for the target architecture.
\item \textbf{Configuration Directory (\texttt{/cfg} or \texttt{/etc}):} Stores text-based system configuration files.
\item \textbf{Library Directory (\texttt{/lib}):} Holds shared libraries when dynamic linking is supported.
\item \textbf{Drivers Directory (\texttt{/drv}):} Contains kernel-loadable drivers.
\end{itemize}
The staging directory is recreated for each build, ensuring reproducibility and eliminating residual artifacts from previous compilation runs.

\subsubsection{Artifact Compilation and Population}

After establishing the directory hierarchy, the staging area must be populated with executable content. User space applications are compiled using a cross-compilation toolchain and linked against standard libraries compatible with the kernel's Application Binary Interface (ABI).

A well-designed build system automates this process and enforces correct dependency ordering. In particular, building the kernel should implicitly trigger the compilation of user space programs. The resulting binaries are then installed into the appropriate locations within the staging directory (e.g., \texttt{/bin}). This automation minimizes the risk of inconsistencies, such as a kernel attempting to execute outdated user space binaries that rely on obsolete system calls.

\subsubsection{Configuration Overlays}

Executable binaries alone are insufficient for a functional operating system. Configuration data and static resources are also required. These include configuration files, initialization scripts, and other static assets.

A common approach to managing such data is the use of a configuration overlay. In this model, the source repository contains a predefined directory tree of static files. During the build process, this tree is recursively merged into the staging directory, augmenting the compiled binaries. This separation of code and configuration promotes flexibility, as system behavior can be modified (for example, by adjusting startup parameters) without recompilation of user space programs.

\subsubsection{File System Image Synthesis}

Once the staging directory has been fully populated, it must be transformed into a single binary image conforming to a specific file system format, such as FAT, EXT2, or an archive-based format.

In academic and hobbyist operating system projects, simpler filesystems such as FAT are frequently preferred over more sophisticated journaling filesystems. FAT offers broad host-side tool support, simplifying inspection and debugging, and its relatively simple on-disk structures reduce driver implementation complexity.

Image synthesis is typically performed using host-side utilities (e.g., \texttt{mtools} and \texttt{mkfs.fat}). These tools determine the required image size, format a blank image file, and populate it with the contents of the staging directory. The output is a file system image ready for deployment alongside the kernel.

\subsubsection{The Initial Ramdisk and the Bootstrapping Problem}

A fundamental challenge in early operating system development is the so-called “chicken-and-egg” problem associated with storage drivers. In order to mount a root file system from persistent storage devices such as NVMe or SATA, the kernel must already possess functioning disk drivers. However, these drivers are commonly stored within the root file system itself.

This problem is addressed through the use of an \textbf{Initial Ramdisk (initrd)}. The bootloader loads the entire image into main memory alongside the kernel and passes its memory location as a boot parameter. The kernel can then expose this memory region as a block device and mount it as the root file system (\texttt{/}) during early initialization. This approach allows the system to complete its initialization sequence before transitioning to a more permanent storage-backed root file system.

\subsection{Compiling User Space Programs}

The compilation of user space applications for a custom operating system differs fundamentally from standard application development on a host machine. Standard host toolchains are configured to target the host environment, automatically linking against its standard library and adhering to its specific runtime initialization model. Consequently, generating binaries for a new operating system requires adaptation of the build system to target the new environment. This section outlines the necessary components and configurations needed to compile user space programs for a custom OS.

\subsubsection{Executable Format}

A standardized executable container format must be selected to serve as a formal contract between the compiler toolchain and the kernel loader. The Executable and Linkable Format (ELF) is the standard for Unix-like systems and custom operating system projects due to its extensibility and broad toolchain support.

ELF binaries utilize "program headers" to describe contiguous regions of the file known as \textbf{segments}. The kernel loader parses these headers to determine:
\begin{itemize}
\item \textbf{Load addresses:} The virtual memory addresses at which segments are mapped.
\item \textbf{Memory footprint:} The distinction between file size and memory size, allowing for the automatic zero-initialization of uninitialized data regions (BSS).
\item \textbf{Access permissions:} The hardware-level protection attributes (read, write, execute) required for each segment.
\end{itemize}

For early-stage operating systems, \textbf{static linking} is often the preferred strategy. This approach embeds all library code directly into the executable, avoiding the complexity of implementing a dynamic linker and shared object support within the kernel. Furthermore, compiling as Position Independent Executables (PIE) enables the kernel to load programs at arbitrary memory addresses, a prerequisite for advanced security features such as Address Space Layout Randomization (ASLR).

\subsubsection{Memory Layout}

To enforce the given memory layout, a custom linker script is required. This script groups program sections according to their protection requirements, enabling the kernel to utilize hardware-level security mechanisms, such as non-executable memory regions. A standard user space memory layout comprises the following sections in order of increasing virtual address:
\begin{enumerate}
\item \textbf{.text}: Executable code --- marked read-only and executable.
\item \textbf{.rodata}: Constants and string literals --- marked read-only and non-executable.
\item \textbf{.data}: Initialized global variables --- marked read-write and non-executable.
\item \textbf{.bss}: Uninitialized global variables --- these occupy no storage space in the binary but are allocated in virtual memory at runtime.
\end{enumerate}

Across all major architectures, these sections are aligned to page boundaries to satisfy the requirements of the memory management unit (MMU), with page sizes typically being 4~KiB, though larger pages may be supported. The use of page-level permissions enables the kernel to prevent unintended execution of data regions and to enforce write protection for code and constant data.

While the exact virtual base address of the \textbf{.text} section is platform- and ABI-dependent, it is commonly chosen to be non-zero in order to mitigate errors related to null pointer dereferences. Similarly, the overall organization of dynamic memory follows a shared convention: the user space heap is initialized immediately after the \textbf{.bss} section and grows toward higher virtual addresses, whereas the user space stack is placed near the upper limit of the user address space and grows downward.

\subsubsection{Runtime Initialization}

User space programs require a well-defined runtime initialization sequence to prepare the execution environment before control is transferred to the program's \texttt{main} function. This sequence is typically provided by the C runtime (CRT), which is described in detail in Section~\ref{subsubsec:theory_crt}.

\subsubsection{OS-Specific Toolchain Adaptation}
\label{subsubsec:os-specific-toolchain}

While a generic cross-compiler (e.g., targeting a bare-metal environment such as \texttt{x86-64-elf}) is sufficient for compiling the kernel itself, it presents limitations for user space development. Generic toolchains are agnostic to the operating system's file system layout and standard library locations, requiring the build system to manually manage header paths and library linkage for every compilation unit. To streamline this process, the toolchain can be patched to target the custom operating system natively  (e.g., via a target like \texttt{x86-64-youros}) \cite{osdev-os-specific-toolchain}. This automates environment detection and dependency resolution, significantly reducing reliance on complex build scripts. From this point onward, one should provide their own toolchain source code (or patches), alongside the OS source code, instead of relying on an upstream one.

\paragraph{System Root (Sysroot)}

The sysroot is a directory hierarchy on the host machine that mirrors the target operating system's root file system. It organizes system headers (e.g., \texttt{sysroot/usr/include}) and compiled libraries (e.g., \texttt{sysroot/usr/lib}) according to standard conventions.

By configuring the cross-compiler with the \texttt{\text{-}\text{-}with-sysroot} option, the toolchain anchors its search paths to this directory. This ensures that inclusion directives, such as \texttt{\#include <stdio.h>}, resolve exclusively to the headers of the custom operating system's standard library, thereby isolating the build process from the host environment.

\paragraph{Implicit Linker Configuration}

An OS-specific toolchain further simplifies the linking phase by implicitly managing runtime dependencies. In a raw compilation setup, the developer must explicitly specify the C runtime startup object (\texttt{crt0.o}), the initialization and finalization objects (\texttt{crti.o}, \texttt{crtbegin.o}, \texttt{crtend.o}, \texttt{crtn.o}), and the appropriate standard library linkage for every user space program.

In contrast, a customized toolchain encapsulates this configuration internally. Upon invocation, the compiler automatically injects the necessary object files and links against the correct version of the system library. This abstraction allows developers to compile user space applications with minimal command-line arguments.

\subsubsection{Build System Integration}

The compilation of user space programs can be automated and synchronized with the kernel build process. The build system manages the dependency graph, ensuring that changes to the system library trigger recompilation of user applications. The resulting binaries are then automatically staged for inclusion in the system image, as described in Section~\ref{subsec:rootfs}.

% ==================================================================

\section{General Design Considerations}

Developing an operating system kernel is a complex software engineering challenge involving massive codebases and strict performance requirements. To manage this complexity and ensure code quality, it is advisable to leverage established architectural patterns and high-quality design solutions used in systems programming. This section discusses general design concepts that are beneficial in kernel development.

\subsection{Intrusive Data Structures}
\label{subsec:intrusive}

Kernel development frequently involves managing collections of objects, such as linked lists or binary trees. Standard library implementations typically manage these collections by allocating a separate node structure for every element to hold the data and the navigation pointers. In a kernel environment, where objects are inserted and removed from lists with high frequency, the overhead of repeated allocation and deallocation is often unacceptable due to performance costs and memory fragmentation.

A common solution to this problem is the use of \textbf{intrusive data structures}. In this paradigm, the pointers (such as \texttt{next} and \texttt{prev}) are embedded directly within the data object itself, rather than in an external node. This pattern is widely recognized in the C++ ecosystem and serves as the foundation for industry-standard libraries such as \textbf{Boost.Intrusive}\cite{boost_intrusive}.

By adopting this approach, the container manipulates pointers that already exist within the object structure. A typical implementation of this pattern using C++ templates is shown in Listing \ref{lst:intrusiveNodesExample}.
\begin{cppcode}[caption={Intrusive Data Structures Example}, label={lst:intrusiveNodesExample}]
template<class T>
struct ListNode {
    T* next;
    T* prev;
};

struct Obj: ListNode<Obj> {
    const char* name;
};

template<class T>
struct LinkedList {
    void PushFront(T* item) {
        T* first = head->next;
        
        first->prev = item;

        item->next = first;
        item->prev = nullptr;

        head->next = item;
    }

    // Other operations...

private:
    T sentinel{};

    T* head = &sentinel;
    T* tail = &sentinel;
};

void RemoveFromLinkedList(T* item) {
    if (item->next == nullptr) {
        return;
    }

    // Valid, because of the sentinel
    item->prev->next = item->next;
    item->next->prev = item->prev;

    // Assure proper unassigned state
    item->prev = item->next = nullptr;
}

\end{cppcode}

The usage of intrusive data structures offers multiple benefits critical for systems programming:
\begin{itemize}
    \item \textbf{No dynamic allocation} --- Insertion and removal operations do not require memory allocation or deallocation, eliminating heap fragmentation and allocation latency.
    \item \textbf{Improved memory access} --- The object is accessed directly, avoiding the pointer indirection required by external node structures.
    \item \textbf{O(1) Verification} --- Determining whether an object belongs to a collection can be performed in constant time by inspecting the object's internal state.
    \item \textbf{O(1) Removal} --- An object can be removed from a doubly linked list in constant time, given only a pointer to the object itself.
    \item \textbf{Independent Removal} --- Removal operations do not require access to the doubly linked list head or the container object.
\end{itemize}

\subsection{Reference Counting}
\label{subsec:refcounting}

In a multitasking operating system, processes and kernel subsystems continuously acquire and release resources such as files, memory objects, and synchronization primitives. These resources are often shared: multiple processes may hold references to the same open file, or several kernel subsystems may access the same internal object concurrently. As long as at least one reference holder exists, the resource must remain valid. However, once the final holder releases its reference, the resource should be reclaimed promptly to avoid memory leaks and resource exhaustion.

\textbf{Reference counting} provides a deterministic mechanism for managing such shared lifetimes. Each managed object maintains a counter representing the number of active references to it. The counter is incremented whenever a new reference is acquired and decremented when a reference is released. When the counter reaches zero, indicating that no execution context retains a reference, the object is safely deallocated. This approach allows the kernel to reclaim resources precisely at the moment they become unused, without requiring global coordination or non-deterministic garbage collection.


\subsubsection{Storage Strategies}

A central design decision in reference counting is the placement of the control metadata, namely the reference counter. As illustrated in Figure~\ref{fig:refcount_strategies_detail}, three principal strategies exist, each with distinct trade-offs in terms of allocation behavior, cache locality, and usability:

\begin{enumerate}
    \item \textbf{Intrusive Model (Embedded):} The reference counter is embedded directly within the object's data structure. Smart pointers maintain a single raw pointer to the object.
    \item \textbf{External (Decoupled):} Common in user space libraries (e.g., \texttt{std::shared\_ptr}), where the control block is allocated separately and the smart pointer stores two pointers.
    \item \textbf{External (Indirection):} The smart pointer references a control block that in turn references the object, effectively introducing a level of indirection.
\end{enumerate}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        font=\sffamily\small,
        % Style for the pointer variable on stack
        smartptr/.style={
            rectangle split,
            rectangle split parts=1,
            draw=black!80,
            fill=blue!5,
            minimum height=0.8cm,
            text width=2.5cm,
            align=center,
            rounded corners=1pt
        },
        % Style for the heap object container
        heapblock/.style={
            rectangle split,
            rectangle split parts=1,
            draw=black!70,
            fill=white,
            text width=3cm,
            align=center,
            minimum height=0.8cm
        },
        % Style for the Control Block (Metadata)
        ctrlblock/.style={
            rectangle split parts=1,
            draw=black!70,
            fill=gray!20,
            text width=3cm,
            align=center,
            minimum height=0.6cm
        },
        % Arrow style
        ptr/.style={
            -{Latex[length=3mm]},
            thick,
            draw=black!70
        },
        labeltext/.style={
            font=\bfseries,
            anchor=west
        }
    ]

    % ==========================================================
    % 1. INTRUSIVE MODEL
    % ==========================================================
    \node[labeltext] at (-1, 0) {1. Intrusive Model};
    
    % Smart Pointer
    \node[smartptr] (sp1) at (0, -1.5) {\texttt{ptr\_object}};
    \node[above=0.1cm of sp1, font=\scriptsize\itshape, color=gray] {Smart Pointer};

    % The Object (One contiguous block)
    \node[ctrlblock] (rc1) at (5, -1.2) {\texttt{ref\_count}};
    \node[heapblock, below=0cm of rc1] (data1) {Object Data};
    % Draw outline around the whole fused object
    \node[draw=black!70, fit=(rc1) (data1), inner sep=0pt] (obj1) {};
    \node[above=0.1cm of obj1, font=\scriptsize\itshape, color=gray] {Object};

    % Arrow
    \draw[ptr] (sp1.east) -- (obj1.north  west);


    % ==========================================================
    % 2. DECOUPLED (Two Pointers in SmartPtr)
    % ==========================================================
    \node[labeltext] at (-1, -3.5) {2. External: Decoupled};

    % Smart Pointer (Split in two)
    \node[smartptr, rectangle split parts=2] (sp2) at (0, -5) {
        \texttt{ptr\_object}
        \nodepart{second}
        \texttt{ptr\_control}
    };
    \node[above=0.1cm of sp2, font=\scriptsize\itshape, color=gray] {Smart Pointer};

    % Control Block (Count Only)
    \node[ctrlblock] (cb2) at (5, -6) {\texttt{ref\_count}};
    \node[above=0.1cm of cb2, font=\scriptsize\itshape, color=gray] {Control Block};

    % Object Data
    \node[heapblock] (obj2) at (5, -4.75) {Object Data};
    \node[above=0.1cm of obj2, font=\scriptsize\itshape, color=gray] {Object};

    % Arrows
    % Arrow from first part of SP to Object
    \draw[ptr] (sp2.text east) -- (obj2.west);
    % Arrow from second part of SP to Control Block
    \draw[ptr] (sp2.second east) -- (cb2.north west);


    % ==========================================================
    % 3. INDIRECTION (Pointer in Control Block)
    % ==========================================================
    \node[labeltext] at (-1, -7.5) {3. External: Indirection};

    % Smart Pointer (One Pointer)
    \node[smartptr] (sp3) at (0, -9) {\texttt{ptr\_control}};
    \node[above=0.1cm of sp3, font=\scriptsize\itshape, color=gray] {Smart Pointer};

    % Control Block (Count + Pointer)
    \node[ctrlblock, rectangle split, rectangle split parts=2] (cb3) at (3.5, -9) {
        \texttt{ref\_count}
        \nodepart{second}
        \texttt{ptr\_object}
    };
    \node[above=0.1cm of cb3, font=\scriptsize\itshape, color=gray] {Control Block};

    % Object Data
    \node[heapblock] (obj3) at (7.5, -9) {Object Data};
    \node[above=0.1cm of obj3, font=\scriptsize\itshape, color=gray] {Object};

    % Arrows
    \draw[ptr] (sp3.east) -- (cb3.north west);
    \draw[ptr] (cb3.second east) -- (obj3.west);

    \end{tikzpicture}
    \caption{Comparison of Reference Counting Storage Strategies}
    \label{fig:refcount_strategies_detail}
\end{figure}

\subsubsection{Rationale for the Intrusive Model in Kernel Space}

Most kernels employ the \textbf{intrusive reference counting model}. Although external models offer greater flexibility by avoiding modifications to object layouts, their associated costs are not justifiable within the kernel context. With an intrusive design, any subsystem holding a raw pointer can acquire ownership by incrementing the embedded counter, allowing objects to move freely between low-level components and higher-level abstractions without compromising correctness.

The usage of reference counting offers multiple benefits:
\begin{itemize}
    \item \textbf{Deterministic deallocation} -- Resources are freed at the precise moment they become unused, without relying on garbage collection or deferred cleanup.
    \item \textbf{Shared ownership} -- Multiple subsystems can safely share references to the same object without explicit coordination.
    \item \textbf{Cache efficiency} -- The intrusive model ensures metadata is colocated with object data, improving memory access patterns.
    \item \textbf{Lock-free operation} -- Atomic hardware primitives ensure safe concurrent access and correct object destruction in SMP environments without incurring the overhead of locks.
    \item \textbf{Compile-time safety} -- Encapsulation within RAII-compliant wrappers (e.g., smart pointers) allows the compiler to enforce ownership semantics, automating reference management and structurally preventing resource leaks.
\end{itemize}

\section{Architecture Abstraction}
\label{sec:arch_abstraction}

While the services provided by an operating system --- such as process scheduling, file I/O, and inter-process communication --- are conceptually uniform, the underlying hardware implementations vary significantly across architectures (e.g., x86-64, ARM64, RISC-V). Registers differ in naming and width, memory consistency models vary, and interrupt controllers function through disparate protocols.

To address this variance without duplicating the entire kernel codebase for every target platform, a strict architectural abstraction is advised. 

\subsubsection{The Necessity of Decoupling}

The primary motivation for architecture abstraction is the portability of the kernel. In a naive implementation, a kernel might embed architecture-specific instructions directly into high-level logic. For example, a \texttt{yield()} function might directly execute x86-64 assembly instructions to load the CR3 register for virtual memory context switching.

This approach, known as direct coupling, presents several risks:
\begin{itemize}
    \item \textbf{Portability Lock-in:} The high-level logic becomes inextricably bound to a specific instruction set architecture (ISA). Porting the system to a different architecture would require rewriting core subsystems rather than simply replacing a driver layer.
    \item \textbf{Maintenance Overhead:} Hardware-specific details clutter algorithmic logic, reducing readability and increasing the cognitive load required to modify core system behaviors.
\end{itemize}

The goal of the abstraction layer in to define a clear boundary where the "what" (the logical operation, such as "switch to this address space") is separated from the "how" (the specific sequence of register moves and operations required).

\subsubsection{Categorization of Kernel Code}

One approach to enforce this separation is to divide kernel software into two distinct domains: Machine Independent (MI) code and Machine Dependent (MD) code.

\subsubsection{Machine Independent Code (MI)}
The Machine Independent layer implements the algorithmic logic and data management of the operating system. It operates on an idealized hardware model and manages abstract resources.

\begin{itemize}
    \item \textbf{Definition:} Code that remains valid regardless of the underlying CPU architecture or platform topology.
    \item \textbf{Characteristics:} This code is written exclusively in standard programming language and relies on fixed-width integer types (e.g., \texttt{u64}, \texttt{size\_t}). It contains no inline assembly and does not access hardware registers directly.
\end{itemize}

\subsubsection{Machine Dependent Code (MD)}
The Machine Dependent layer is responsible for translating the abstract commands issued by the MI layer into specific instruction sequences required by the target hardware.

\begin{itemize}
    \item \textbf{Definition:} Code responsible for the physical actuation of hardware features.
    \item \textbf{Characteristics:} This code is highly specific to the target platform (e.g., x86-64). It frequently utilizes inline assembly, manages control registers (such as CR3, IDT, GDT), and handles hardware-specific quirks (e.g., APIC timer calibration or TLB invalidation).
    \item \textbf{Role:} The MD layer serves as a driver for the CPU, shielding the MI layer from architectural implementation details.
\end{itemize}

\subsubsection{Memory Management Abstraction Example}
The memory subsystem is typically the most complex area to abstract due to the tight coupling between software policies and hardware tables.
\begin{itemize}
    \item \textbf{Machine Independent (MI):} Manages the logical view of the address space. It operates on objects such as Virtual Memory Areas (VMAs) or Regions, handling high-level logic like permissions (Read/Write/Execute), file mappings, and copy-on-write policies.
    \item \textbf{Machine Dependent (MD):} Acts as a driver for the Memory Management Unit (MMU). It implements the specific page table structure (e.g., PML4 on x86-64 vs. Translation Tables on ARM), handles TLB invalidation instructions (e.g., \texttt{invlpg}), and manages the loading of the page table base register (e.g., \texttt{mov cr3}).
\end{itemize}

\subsubsection{Context Switching Abstraction Example}
The scheduler must save and restore the state of execution, but the definition of "state" varies by CPU.
\begin{itemize}
    \item \textbf{Machine Independent (MI):} Tracks the lifecycle of threads (Ready, Blocked, Running) and manages the decision logic of \textit{which} thread to run next. It views a thread simply as a pointer to a stack and a state enum.
    \item \textbf{Machine Dependent (MD):} Performs the actual context switch. It knows exactly which registers must be saved (General Purpose Registers, Floating Point state, Vector registers), how to switch the stack pointer safely, and how to handle architecture-specific state (e.g., segment registers or shadow stack pointers).
\end{itemize}

\chapter{Analysis of Existing Solutions}

\section{Linux}
\label{sec:linux-research}

The Linux operating system family is one of the most widely used globally, alongside Windows. The Linux kernel relies on a monolithic architecture. According to Tanenbaum, the monolithic approach is by far the most common organization for operating systems. In this model, the entire system runs as a single large executable binary program in kernel mode. This design allows for high efficiency, as any procedure within the kernel can directly call any other, facilitating rapid computation and minimal overhead. However, this lack of restriction leads to a system that can be 'unwieldy and difficult to understand'. A significant drawback of this is that a crash in any single procedure, such as a buggy device driver, can effectively take down the entire operating system \cite{tanenbaum2015modern}.

\noindent Advantages:
\begin{itemize}
    \item \textbf{Zero-copy and memory efficiency} --- As all components reside in a single address space, data does not need to be copied between distinct address spaces. Passing a pointer is sufficient to transfer data between modules.
    \item \textbf{Minimized context switching} --- Within the single kernel address space, invoking functions from different modules is a direct function call, avoiding the overhead of context switching required by microkernels to access driver services.
    \item \textbf{Simplified source management} --- A single binary structure allows for a unified build scheme. Developing numerous separately compiled services can easily lead to inconsistencies in behavior or data models.
    \item \textbf{Ease of implementation} --- This architecture is often easier to implement for small to mid-sized projects.
\end{itemize}

\noindent Disadvantages:
\begin{itemize}
    \item \textbf{Stability} --- A bug in a single driver, even for an unused device, can compromise the stability of the entire system.
    \item \textbf{Security} --- Vulnerabilities in drivers can potentially expose the kernel address space to malicious user-space execution.
    \item \textbf{Scalability challenges} --- For large-scale projects, strict standards are necessary. Without them, the code base can easily become unmaintainable due to complex webs of references and dependencies.
\end{itemize}

To enhance extensibility and avoid the need for recompilation when adding drivers, Linux employs Loadable Kernel Modules (LKMs) \cite{linux_lkm}. These allow the kernel functionality to be extended dynamically via well-defined interfaces during runtime.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tikzpicture}
            \tikzstyle{every node}=[font=\large]
            \draw [fill={rgb,255:red,242; green,242; blue,242}, thick] (0,0) rectangle (14, 8);
            \node [anchor=north west, font=\bfseries\Large] at (0.3, 7.7) {Kernel Address Space};
            \draw [fill=white, thick, rounded corners=3.0] (0.5, 4.5) rectangle (13.5, 6.5);
            \node [font=\Large, align=center] at (7, 5.5) {Base Linux Kernel Components};
            \tikzstyle{mod}=[draw, fill=white, thick, rounded corners=3.0, minimum width=2cm, minimum height=1.3cm, font=\small]
    
            \node[mod] at (1.5, 3.2) {vfat};
            \node[mod] at (3.7, 3.2) {fat};
            \node[mod] at (5.9, 3.2) {realtek};
            \node[mod] at (8.1, 3.2) {pcspkr};
            \node[mod] at (10.3, 3.2) {kvm-intel};
            \node[mod] at (12.5, 3.2) {kvm};
    
            \node[mod] at (1.5, 1.5) {nvme};
            \node[mod] at (3.7, 1.5) {nvidia};
            \node[mod] at (5.9, 1.5) {rfkill};
            \node[mod] at (8.1, 1.5) {nf\_tables};
            \node[mod] at (10.3, 1.5) {snd};
            \node[mod] at (12.5, 1.5) {\Huge \dots};
    
        \end{tikzpicture}
    }
    \caption{Linux Monolithic Architecture with Loadable Kernel Modules}
    \label{fig:linux_lkm}
\end{figure}

\subsection{Timing}
\label{subsec:linux-timing}

Historically, Linux utilized a strictly periodic tick (defined by hardware), configuring hardware timers to generate interrupts at a fixed frequency (e.g., 100 Hz or 1000 Hz). While this design was straightforward, it imposed limitations on sleep granularity and power efficiency. To address these issues, Linux introduced High Resolution Timers (\texttt{hrtimers}), which allow the kernel to schedule interrupts with nanosecond precision based on immediate needs rather than a fixed cadence. Despite the capabilities of high-resolution timers, the concept of a periodic tick is maintained within the kernel for performance reasons and architectural legacy, by simulating the tick with high precision framework.

This hight-precision framework has one major drawback - it can get slow for a large number of events, as it is based on a priority queue with $O(\log n)$complexities. Relying exclusively on high-precision mechanisms for every timing event would introduce unacceptable overhead in scenarios involving a massive number of active timers. To mitigate this, Linux maintains the \texttt{jiffies} counter, which increments at the frequency of the system tick. This counter drives the Timer Wheel mechanism \cite{linux_timer_wheel}, a highly efficient algorithm designed for managing low-precision timeouts. The timer wheel offers $O(1)$ complexity for insertion and expiration, making it ideal for subsystems that require the management of thousands or even hundreds of thousands of concurrent events where nanosecond precision is unnecessary. A prime example is the networking stack, which must track timeouts for tens of thousands of open TCP connections. Utilizing high-resolution timers for such a volume would be computationally impossible. The \texttt{jiffies}-based timer wheel allows the system to handle these massive quantities of events with minimal CPU overhead. Consequently, standard API calls such as \texttt{msleep} continue to rely on this coarse-grained, high-performance infrastructure.

\subsection{The Process and Thread Model}
\label{subsec:linux-task}

Unlike many other operating systems that distinguish strictly between processes (containers of resources) and threads (units of execution), Linux treats them almost identically. The core data structure is the \texttt{task\_struct} \cite{linux_task, bovet2005understanding}.
\begin{itemize}
    \item A \textbf{Process} is a \texttt{task\_struct} with a unique memory map and file descriptor table.
    \item A \textbf{Thread} is simply a \texttt{task\_struct} created via the \texttt{clone()} system call with flags such as \texttt{CLONE\_VM} and \texttt{CLONE\_FILES}, causing it to share the address space and resources with its parent.
\end{itemize}

\subsection{Scheduler}
\label{subsec:linux-scheduler}

Linux implements several scheduling policies \cite{linux_policies}\cite{linux_cfs} operating on task priorities. There are two major classes: \textbf{Real Time}, which operates on priorities 1-99, and \textbf{Fair}, which operates on priority 0. Tasks with higher priorities preempt those with lower priorities. Linux provides six policy classes:

\begin{itemize}
    \item \texttt{SCHED\_DEADLINE} --- Takes precedence over any other policy and provides real-time capabilities.
    \item \texttt{SCHED\_FIFO} --- A real-time policy where a task runs until it yields control or is preempted by a higher-priority task.
    \item \texttt{SCHED\_RR} (Round Robin) --- A real-time policy where each task is assigned a time slice. Upon exhaustion, the task is moved to the end of the queue.
    \item \texttt{SCHED\_OTHER} --- The standard policy for the majority of non-real-time tasks.
    \item \texttt{SCHED\_BATCH} --- Designed for non-interactive tasks that run for longer periods without context switches, tolerating longer scheduling latencies.
    \item \texttt{SCHED\_IDLE} --- Used for very low priority tasks that run only when the system is otherwise idle.
\end{itemize}

From version 2.6.23 \cite{linux_cfs} up to 6.6 \cite{linux_eevdf}, the \texttt{CFS} (Completely Fair Scheduler) was the default scheduler for the \textbf{Fair} class. Starting with version 6.6, the \texttt{EEVDF} (Earliest Eligible Virtual Deadline First) scheduler began replacing CFS.

\subsubsection{Completely Fair Scheduler}
The Linux documentation summarizes the design of the \textbf{CFS} as follows:

\begin{quote}
80\% of CFS's design can be summed up in a single sentence: CFS basically models an “ideal, precise multi-tasking CPU” on real hardware.

“Ideal multi-tasking CPU” is a (non-existent :-)) CPU that has 100\% physical power and which can run each task at precise equal speed, in parallel, each at 1/nr\_running speed. For example, if there are 2 tasks running, then it runs each at 50\% physical power - i.e., actually in parallel.
    
On real hardware, we can run only a single task at once, so we have to introduce the concept of “virtual runtime.” The virtual runtime of a task specifies when its next timeslice would start execution on the ideal multi-tasking CPU described above. In practice, the virtual runtime of a task is its actual runtime normalized to the total number of running tasks.
\cite{linux_cfs}
\end{quote}

The implementation uses a red-black tree sorted by virtual runtime, representing the execution timeline. The next task selected is the leftmost node in the tree (the task with the least spent execution time) and runs until another task becomes the leftmost + some granularity to prevent over-scheduling.

\subsubsection{Earliest Eligible Virtual Deadline First Scheduler}

This algorithm is becoming the new standard for Linux scheduling, addressing shortcomings of the previous \textbf{CFS} implementation. The new approach functions similarly to CFS but operates on deadlines rather than accumulated runtime \cite{linux_eevdf}. This modification allows for better prioritization of latency-sensitive tasks.

\subsection{Memory Management}
\label{subsec:linux_mem}

Linux features centralized memory management with various APIs, including \texttt{kmalloc}, \texttt{kzalloc}, \texttt{vmalloc}, and \texttt{kvmalloc} \cite{linux_mem_interfaces}.

\subsubsection{Physical Memory}
As an architecture-independent kernel, Linux abstracts hardware details. Physical memory is partitioned into zones, each serving a distinct purpose \cite{linux_mem_phys}:

\begin{itemize}
    \item \texttt{ZONE\_DMA} --- Memory suitable for DMA (Direct Memory Access) by devices that cannot access the full addressable range.
    \item \texttt{ZONE\_NORMAL} --- Standard memory directly accessible by the kernel.
    \item \texttt{ZONE\_MOVABLE} --- Similar to \texttt{ZONE\_NORMAL}, but the content of pages in this zone can be migrated to different physical frames (preserving virtual address).
    \item \texttt{ZONE\_DEVICE} --- Memory reserved for specific hardware, such as GPUs, mapped to device drivers.
    \item \texttt{ZONE\_HIGHMEM} --- Memory not covered by permanent kernel mappings, used only by some 32-bit architectures.
\end{itemize}

To enhance multi-core performance, Linux employs a two-step allocation strategy. It utilizes a global buddy allocator alongside Per-CPU Pagesets (PCP). According to Bovet and Cesati, these per-CPU caches are further divided into "hot" and "cold" lists to optimize hardware cache usage. "Hot" pages are assumed to be present in the CPU's L1/L2 cache and are preferred for operations where the CPU will immediately write to the page, whereas "cold" pages are used for DMA operations to avoid invalidating useful cache lines \cite{bovet2005understanding}. Allocations are first attempted from these local caches without locking, falling back to the global allocator only when necessary.

\subsubsection{Virtual Memory}

Each process possesses its own Page Table, responsible for mapping virtual addresses to physical ones. Physical frames are acquired from the Physical Memory Manager \ref{subsec:linux_mem}. Virtual address space management for each process utilizes red-black trees to efficiently locate free areas. The kernel address space is mapped into every process. Additionally, Linux implements demand paging, a lazy allocation approach where physical pages are assigned only when the process actually accesses the memory, utilizing the page-fault mechanism.

\subsection{System Interface}

Linux is a Unix-like operating system kernel. The primary interface for system communication is provided via system calls, wrapped by the GNU C Library (glibc).

\subsubsection{Standards}

Linux adheres to the \textbf{POSIX} standard, implementing functions such as \texttt{fork()} and \texttt{exec()}, as well as the concept of \texttt{pthread}.

\subsubsection{Fork Mechanism}

The \texttt{fork()} mechanism is rooted in historical design decisions. While copying the entire parent process state appears inefficient and can introduce performance issues, Linux implements various optimizations, such as Copy-On-Write (\texttt{COW}), to mitigate overhead. This is particularly relevant given that \texttt{exec()} is frequently called immediately after \texttt{fork()}, discarding the copied state. The advantages and disadvantages of this mechanism include:

\noindent Advantages:
\begin{itemize}
    \item Simplified implementation of pipes and pipelining.
    \item Automatic state propagation, eliminating the need to manually specify files or streams in a function call.
\end{itemize}

\noindent Disadvantages:
\begin{itemize}
    \item Complex implementation requiring multiple optimizations to remain efficient.
    \item Requires the duplication of process structures, which is inherently complex.
    \item Potential performance bottlenecks in edge cases, such as with large page tables.
    \item Requires careful resource management by the programmer to release resources before execution.
    \item Conceptually difficult for beginners to grasp.
    \item Not portable to architectures lacking an MMU.
\end{itemize}

\section{Minix 3}

Minix 3 is a microkernel-based operating system designed with a strong emphasis on high reliability. Unlike monolithic kernel architectures, which Tanenbaum argues are inherently prone to failure due to high bug density --- statistically between two and ten bugs per thousand lines of code \cite{tanenbaum2015modern} --- Minix 3 relocates the majority of operating system components into user space. This includes device drivers, file systems, and memory management mechanisms. As a result, the microkernel size is highly reduced, strictly limiting its responsibilities to essential functions such as interrupt handling, process management, and inter-process communication via message passing.

This architectural approach provides significant advantages in terms of fault tolerance. By running each device driver and server as a separate, relatively powerless user process, the kernel ensures that a bug in a single component, such as an audio driver, cannot crash the entire system. A key feature of this design is the "reincarnation server", which monitors system components and automatically replaces those that have failed, making the system effectively "self-healing" \cite{tanenbaum2015modern}. Furthermore, this modularity follows the principle of decoupling mechanism from policy. For example, while the kernel handles the mechanism of switching processes, the scheduling policy can be managed by user-mode processes. This ensures that each component has "exactly the power to do its work and nothing more" \cite{tanenbaum2015modern}, fundamentally enhancing system stability and maintainability.

\begin{figure}[htbp]
  \centering
  \resizebox{1\textwidth}{!}{%
    \begin{tikzpicture}
    \tikzstyle{every node}=[font=\large]

    % - Background Layers -
    % Kernel Mode Background
    \draw [fill={rgb,255:red,242; green,242; blue,242}] (2.5,10.75) rectangle (17.25,8.25);
    % User Mode Background
    \draw [fill={rgb,255:red,242; green,242; blue,242}] (2.5,18.5) rectangle (17.25,11);
    
    % Horizontal Separators
    \draw (2.5,16) -- (17.25,16);
    \draw (2.5,13.5) -- (17.25,13.5);

    % - User Processes Layer (Layer 4) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,18) rectangle node {\large User Processes} (7,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,18) rectangle node {\large Shell} (9.5,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,18) rectangle node {\large Make} (12,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,18) rectangle node {\large User} (14.5,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,18) rectangle node {\LARGE ...} (17,16.5);

    % - Server Processes Layer (Layer 3) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,15.5) rectangle node {\large Server Processes} (7,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,15.5) rectangle node {\large File} (9.5,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,15.5) rectangle node {\large PM} (12,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,15.5) rectangle node {\large Sched} (14.5,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,15.5) rectangle node {\LARGE ...} (17,14);

    % - Device Driver Layer (Layer 2) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,13) rectangle node {\large Device Processes} (7,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,13) rectangle node {\large Disk} (9.5,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,13) rectangle node {\large TTY} (12,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,13) rectangle node {\large Net} (14.5,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,13) rectangle node {\LARGE ...} (17,11.5);

    % - Kernel Layer (Layer 1) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,10.25) rectangle node {\large Kernel} (7.25,8.75);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.75,10.25) rectangle node {\large Clock Task} (12,8.75);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.5,10.25) rectangle node {\large System Task} (17,8.75);
    
    % - Labels and Brackets -
    % User Mode bracket
    \draw [thick] (2.0,11) -- (2.0,18.5);
    \draw [thick] (2.0,11) -- (2.2,11);
    \draw [thick] (2.0,18.5) -- (2.2,18.5);
    \node[left, align=center] at (1.8,14.75) {\large \textbf{User}\\\large \textbf{Mode}};
    
    % Kernel Mode bracket
    \draw [thick] (2.0,8.25) -- (2.0,10.75);
    \draw [thick] (2.0,8.25) -- (2.2,8.25);
    \draw [thick] (2.0,10.75) -- (2.2,10.75);
    \node[left, align=center] at (1.8,9.5) {\large \textbf{Kernel}\\\large \textbf{Mode}};
    
    % - Right Side Layer Labels -
    \node[right] at (17.5, 17.25) {\large \textbf{Layer 4}};
    \node[right] at (17.5, 14.75) {\large \textbf{Layer 3}};
    \node[right] at (17.5, 12.25) {\large \textbf{Layer 2}};
    \node[right] at (17.5, 9.5)   {\large \textbf{Layer 1}};
    
    \end{tikzpicture}
  }%
\caption{The Minix 3 Microkernel Architecture (adapted from \cite{minix_wikipedia})}
\label{fig:minix_architecture}
\end{figure}

\subsection{Scheduler}
Adhering to the microkernel philosophy, Minix 3 separates scheduling policy from the low-level context switching mechanism. While the kernel remains responsible for the mechanics of context switching and interrupt handling, scheduling decisions are delegated to a dedicated user space server, referred to as the \texttt{Sched} server.

The scheduler employs a multi-level priority round-robin algorithm \cite{minix_sched}. The system maintains sixteen priority queues organized hierarchically. Kernel tasks occupy the highest priority levels, followed by device drivers, system servers, and user applications. The lowest priority queue is reserved exclusively for idle tasks. At any scheduling decision point, the scheduler selects the next runnable process from the highest non-empty priority queue.

Every process is assigned a specific time quantum, representing the maximum CPU time allowed before preemption occurs. Upon the exhaustion of a process's time quantum, the kernel notifies the scheduler server. To penalize CPU-bound tasks and maintain system responsiveness, the scheduler lowers the priority of such processes, moving them to a lower queue.

To prevent indefinite starvation of low-priority processes, a \texttt{balance\_queues} function executes periodically at five-second intervals \cite{minix_sched_bq}. This function re-evaluates process states and promotes tasks that have not received CPU time for an extended period, thereby ensuring that interactive applications remain responsive.

\subsection{Timing}
The timing subsystem in Minix 3 is responsible for maintaining system time, coordinating scheduling intervals, and handling alarms and timers. The core timing functionality is encapsulated within the Clock Task, a kernel-level process that manages time-related operations.

\subsubsection{Hardware Abstraction}
Minix 3 provides an abstraction layer over platform-specific timing hardware. On x86 systems, this includes the legacy Programmable Interval Timer (PIT) \cite{osdev-pit}, while ARM-based platforms rely on board-specific timer implementations \cite{minix_arch_clock}. During system initialization, the kernel configures the selected hardware timer to generate periodic interrupts. If not specified in the kernel environment, the default interrupt frequency is architecture-dependent: 60 Hz on x86 and 1000 Hz on ARM systems.

\subsubsection{Timers and Alarms}
The kernel maintains a \texttt{clock\_timers} queue to manage synchronous timers and alarms \cite{minix_kernel_clock}. Upon receipt of a hardware timer interrupt, the \texttt{timer\_int\_handler} routine is executed. If the expiration time of the next scheduled timer in the \texttt{clock\_timers} queue has been reached, the kernel executes the associated callback function. 

In the case of system alarms, this callback is the \texttt{cause\_alarm} function \cite{minix_do_setalarm}. This function sends a notification message from the Clock Task to the requesting process (e.g., the Process Manager or Scheduler), informing it that the requested time interval has elapsed. This mechanism allows user space servers to perform periodic tasks, such as the scheduler's queue balancing.

\subsubsection{Real-Time Clock (RTC)}
Persistent wall-clock time is provided by a dedicated user space driver, \texttt{readclock}. This driver interfaces directly with the CMOS Real-Time Clock (RTC) to retrieve the current date and time. During system initialization, this information is communicated to the Process Manager to establish the system's initial time reference \cite{minix_readclock}.

\subsection{Memory Management}
Memory management in Minix 3 is primarily handled by the user space Virtual Memory (VM) server. The kernel retains control over the hardware Memory Management Unit (MMU) and address space switching but delegates higher-level memory policies to the VM server.

\subsubsection{VM Server}
The VM server is responsible for memory region allocation, page table administration, and page fault handling. It implements a region-based memory model in which a process address space is divided into contiguous regions with defined access permissions (read, write, and execute). To efficiently manage free and allocated memory regions, the VM server uses \textbf{AVL trees} \cite{minix_vm}.

\subsubsection{Allocation and Paging}
Internal memory allocation for kernel data structures and VM metadata is performed using a slab allocator \cite{slab_allocation}. Minix 3 supports demand paging: when a process attempts to access an unmapped virtual address, a page fault is raised. The kernel intercepts this exception and forwards the fault information to the VM server, which resolves it by mapping an appropriate physical frame or retrieving the required page from secondary storage \cite{shenoy_lecture}.

\subsubsection{Memory Grants}
Due to the strict isolation of address spaces in the microkernel architecture, processes cannot directly access the memory of others. To facilitate the exchange of large data structures without violating protection boundaries, Minix 3 provides a capability-based mechanism known as \texttt{Memory Grants}, accessed via the \texttt{safecopy} API \cite{minix_safecopy}.

A process (the grantor) dynamically generates a grant capability that explicitly permits a specific peer process (the grantee) to read from or write to a designated memory range. The grantee utilizes this grant ID to request a data transfer from the System Task. The kernel validates the grant permissions before performing the copy operation between the disjoint address spaces. This mechanism ensures that drivers and servers can operate on client buffers while preventing unauthorized access to arbitrary memory locations.

\subsection{System Interface}
Although Minix 3 conforms to the POSIX standard, its underlying system interface differs substantially from monolithic operating systems due to its message-based microkernel architecture.

\subsubsection{IPC Primitives}
Inter-process communication (IPC) is built upon four fundamental primitives provided by the kernel. For each operation, the kernel mediates communication by copying message contents directly from the sender's address space to that of the receiver.
\begin{itemize}
    \item \texttt{send(dest, \&message)}: Sends a message to the specified destination. The sender blocks if the destination is not ready to receive.
    \item \texttt{receive(source, \&message)}: Blocks the calling process until a message is received from the specified source.
    \item \texttt{sendrec(src\_dest, \&message)}: A combined operation that sends a message and then blocks until a reply is received from the same process.
    \item \texttt{notify(dest)}: A non-blocking signaling mechanism used to inform a destination of an event without transmitting a data payload.
\end{itemize}

\subsubsection{Communication Policies}
IPC in Minix 3 is governed by these strict policies:
\begin{enumerate}
    \item \textbf{Access Control:} Processes may exchange messages only with explicitly authorized peers.
    \item \textbf{Hierarchical Communication:} Message flow generally follows the system's layered architecture (Layer 4 $\rightarrow$ Layer 3 $\rightarrow$ Layer 2).
    \item \textbf{User Space Isolation:} User processes are prohibited from communicating directly with one another or with the kernel.
\end{enumerate}

\subsubsection{System Call Implementation}
In Minix 3, system calls are not implemented as direct kernel invocations. Instead, standard library functions (e.g., \texttt{read}, \texttt{fork}) serve as wrappers that construct a message containing the call arguments and transmit it to the appropriate server using the \texttt{sendrec} primitive \cite{minix_ipc}. If the server needs to perform a privileged operation, it forwards the request to the System Task on behalf of the calling process, as servers possess higher privileges than user processes.

\subsubsection{Server Delegation}
System calls are dispatched to specialized servers according to their functionality \cite{minix_callnr}:
\begin{itemize}
    \item \textbf{Process Manager (PM):} Responsible for process lifecycle operations such as \texttt{fork}, \texttt{exec}, \texttt{exit}, and \texttt{wait}.
    \item \textbf{Virtual File System (VFS):} Handles file-related operations including \texttt{open}, \texttt{read}, \texttt{write}, and \texttt{stat}.
\end{itemize}
The kernel acts solely as a transport mechanism and remains agnostic to the semantics of the system call. After processing a request, the server returns a reply message to the calling process, thereby unblocking it and delivering the result.

\chapter{Low-Level Kernel Implementation} 
\label{chap:low_level_implementation} 

This chapter presents the implementation details of the low-level components of the AlkOS operating system. It covers the bootloader, memory management, interrupt handling, and timing infrastructure that form the foundation upon which all higher-level kernel subsystems are built.

\section{Bootloader}

Before the architecture-agnostic kernel can commence execution, the underlying hardware must be brought to a known, deterministic state. As discussed in Section \ref{subsec:theory_bootloader}, the initialization protocols differ significantly depending on the specific hardware architecture and the firmware interface.

For the AlkOS kernel on the x86-64 architecture, we rely on the \textbf{Multiboot2} specification \cite{Multiboot2-Spec}. This allows us to utilize a battle-tested external bootloader, such as GRUB, to handle the intricacies of storage drivers, file system parsing, and memory map retrieval. GRUB loads our kernel binary into memory and transfers control to our entry point.

However, the state provided by Multiboot2 is insufficient for the immediate execution of our C++ kernel. Upon entry, the system is in \textbf{32-bit Protected Mode}, paging is disabled, interrupts are disabled, and the stack pointer is undefined. Furthermore, the kernel is loaded at a low physical address, whereas AlkOS is designed as a \textbf{higher-half kernel}: its code and data are linked to virtual addresses in the upper portion of the 64-bit address space (\texttt{0xFFFFFFFE00000000}).

To bridge this gap, we implemented a two-stage internal bootloader: \textbf{Loader32} and \textbf{Loader64}. Each stage exists as a separate binary with distinct compilation requirements, as illustrated in Figure \ref{fig:boot_flow}.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=1.8cm,
        stage/.style={draw, fill=gray!15, rounded corners=4pt, minimum width=3.2cm, minimum height=1.2cm, font=\small\bfseries, align=center},
        arrow/.style={-Latex, thick},
        note/.style={font=\scriptsize, align=center, text width=3cm}
    ]
    
    % Stages
    \node[stage] (grub) {GRUB\\Bootloader};
    \node[stage, right=of grub] (loader32) {Loader32\\(32-bit ELF)};
    \node[stage, right=of loader32] (loader64) {Loader64\\(64-bit PIE)};
    \node[stage, right=of loader64] (kernel) {AlkOS Kernel\\(Higher-Half)};
    
    % Arrows with labels
    \draw[arrow] (grub) -- (loader32) node[midway, above, font=\scriptsize] {Protected Mode};
    \draw[arrow] (loader32) -- (loader64) node[midway, above, font=\scriptsize] {Long Mode};
    \draw[arrow] (loader64) -- (kernel) node[midway, above, font=\scriptsize] {Virtual Memory};
    
    % Notes below
    \node[note, below=0.6cm of grub] {Multiboot2\\entry point};
    \node[note, below=0.6cm of loader32] {32-bit instructions\\Enable Long Mode};
    \node[note, below=0.6cm of loader64] {Position-independent\\Load kernel to higher-half};
    \node[note, below=0.6cm of kernel] {Linked at\\0xFFFFFFFE00000000};
    
    \end{tikzpicture}
    }
    \caption{AlkOS Boot Flow: Three-Stage Transition from GRUB to Higher-Half Kernel}
    \label{fig:boot_flow}
\end{figure}

\subsection{Why Two Internal Loaders?}

The separation into Loader32 and Loader64 is not arbitrary, it stems from fundamental constraints of the x86-64 architecture and our kernel design:

\begin{enumerate}
    \item \textbf{Loader32 contains 32-bit instructions}: The transition from Protected Mode to Long Mode requires executing 32-bit code. Instructions such as setting control register bits, loading the GDT, and performing the mode switch \textit{cannot} be encoded in 64-bit instruction format. Thus, Loader32 is compiled as a 32-bit ELF binary (\texttt{elf32-i386}).
    
    \item \textbf{Loader64 is Position-Independent Code (PIC)}: The kernel is linked at a fixed higher-half virtual address (\texttt{0xFFFFFFFE00000000}). However, Loader64 must execute \textit{before} virtual memory is fully configured. GRUB loads Loader64 elf as a Multiboot2 module. This elf must still be loaded in the first place in memory that is big enough to hold it, meaning at an \textit{unknown physical address}. To function correctly regardless of where it is loaded, Loader64 is compiled as a Position-Independent Executable (PIE) using \texttt{-fPIE -pie} flags, employing RIP-relative addressing for all memory accesses.
    
    \item \textbf{The kernel ignores its physical location}: Once virtual memory is established with the higher-half mapping, the kernel code executes using virtual addresses. It is unaware of, and unaffected by, its underlying physical memory location.
\end{enumerate}

\subsection{Loader32: 32-bit to 64-bit Transition}

Loader32 is the true entry point of AlkOS, receiving control directly from GRUB. Its primary responsibility is to transition the CPU from 32-bit Protected Mode to 64-bit Long Mode, enabling the 4-level paging required by the x86-64 architecture \cite{Intel-ControlRegisters}.

\subsubsection{Entry Point}

The Multiboot2 specification defines the machine state upon entry: \texttt{EAX} contains the magic number \texttt{0x36d76289}, and \texttt{EBX} points to the Multiboot information structure. The entry point establishes a stack and delegates to C++ code:

\begin{nasmcode}[caption={Loader32 Entry Point (entry.nasm)}, label={lst:loader32_entry}]
bits 32
; GRUB leaves us in 32-bit Protected Mode
; EAX = Magic number, EBX = Multiboot info pointer

section .text
global Entry
Entry:
    mov esp, stack_top      ; Establish stack
    mov ebp, esp
    and esp, 0xFFFFFFF0     ; Align to 16 bytes (System V ABI)
    
    push ebx                ; Arg2: Multiboot info pointer
    push eax                ; Arg1: Magic number
    call MainLoader32       ; Delegate to C++ code
\end{nasmcode}

\subsubsection{CPU Feature Detection}

Before attempting the Long Mode transition, Loader32 verifies that the CPU supports the required features. This is accomplished through the CPUID instruction \cite{Intel-CPUID}, as shown in Listing~\ref{lst:check_longmode}:

\begin{nasmcode}[caption={Long Mode Detection (checks.nasm)}, label={lst:check_longmode}]
EXTENDED_FUNCTIONS_THRESHOLD equ 0x80000000
LONG_MODE_BIT equ 1 << 29

CheckLongMode:
    ; Check if extended CPUID functions are available
    mov eax, EXTENDED_FUNCTIONS_THRESHOLD
    cpuid
    cmp eax, EXTENDED_FUNCTIONS_THRESHOLD + 1
    jl .no_long_mode
    
    ; Query extended processor features (EAX=80000001h)
    mov eax, EXTENDED_FUNCTIONS_THRESHOLD + 1
    cpuid
    test edx, LONG_MODE_BIT   ; Check EDX bit 29
    jz .no_long_mode
    
    mov eax, 0                ; Success
    ret
.no_long_mode:
    mov eax, 1                ; Failure
    ret
\end{nasmcode}

\subsubsection{Memory Management Initialization}

Before enabling paging, Loader32 initializes a temporary bootstrap Physical Memory Manager (PMM) and Virtual Memory Manager (VMM). The memory map obtained from the Multiboot2 information structure is parsed to identify available physical memory regions. Loader32 then creates an \textbf{identity mapping} of the first 10 gigabytes of physical memory using 1GB pages, as demonstrated in Listing~\ref{lst:loader32_mmap}:

\begin{cppcode}[caption={Identity Mapping Setup (main.cpp)}, label={lst:loader32_mmap}]
MemoryManagers SetupMemoryManagement(MultibootInfo &multiboot_info) {
    auto mmap_tag = multiboot_info.FindTag<TagMmap>();
    const MemoryMap mmap(*mmap_tag);
    
    // Create Physical Memory Manager from Multiboot memory map
    auto pmm = PhysicalMemoryManager::Create(mmap, lowest_safe_addr, ...);
    auto vmm = new VirtualMemoryManager(*pmm);
    
    // Identity map first 10GB using 1GB page granularity
    vmm.Map<PageSizeTag::k1Gb>(
        0,                                   // Virtual address
        0,                                   // Physical address  
        10 * PageSize<PageSizeTag::k1Gb>(),  // 10GB
        kPresentBit | kWriteBit | kGlobalBit
    );
    
    return {*pmm, *vmm};
}
\end{cppcode}

The identity mapping ensures that code continues to execute correctly after paging is enabled, and the instruction pointer remains valid because virtual addresses 0--10GB map directly to the same physical addresses.

\subsubsection{Long Mode Enablement}

The transition to Long Mode follows the sequence defined in the Intel Software Developer's Manual \cite{Intel-ControlRegisters}, as shown in Listing~\ref{lst:enable_longmode}:

\begin{nasmcode}[caption={Enabling Long Mode and Paging (enables.nasm)}, label={lst:enable_longmode}]
LONG_MODE_BIT equ 1 << 8
EFER_MSR      equ 0xC0000080
PAGING_BIT    equ 1 << 31
PAE_BIT       equ 1 << 5

EnableLongMode:
    ; Set LME (Long Mode Enable) in EFER MSR
    mov ecx, EFER_MSR
    rdmsr
    or eax, LONG_MODE_BIT
    wrmsr
    ret

EnablePaging:
    ; Enable PAE (Physical Address Extension) in CR4
    mov eax, cr4
    or eax, PAE_BIT
    mov cr4, eax
    
    ; Load PML4 table address into CR3
    mov eax, [ebp + 8]    ; pml4_table parameter
    mov cr3, eax
    
    ; Enable paging in CR0
    mov eax, cr0
    or eax, PAGING_BIT
    mov cr0, eax
    ret
\end{nasmcode}

\subsubsection{Loading Loader64 as an ELF Module}

A critical responsibility of Loader32 is loading the next stage. Loader64 is provided to GRUB as a separate Multiboot2 module. Loader32 locates this module, parses it as an ELF64 executable, allocates memory for its segments, and performs dynamic relocation (Listing~\ref{lst:load_loader64}):

\begin{cppcode}[caption={Loading Loader64 Module (main.cpp)}, label={lst:load_loader64}]
static u64 LoadNextStageModule(MultibootInfo &multiboot_info, 
                               MemoryManagers mem_managers) {
    auto &pmm = mem_managers.pmm;
    
    // Find the loader64 module by command line string
    auto next_module = multiboot_info.FindTag<TagModule>([](TagModule *tag) {
        return strcmp(tag->cmdline, "loader64") == 0;
    });
    
    // Parse ELF and determine required memory
    auto elf = PhysicalPtr<byte>(next_module->mod_start);
    u64 virt_start, virt_end;
    Elf64::GetProgramBounds(elf.ValuePtr(), virt_start, virt_end);
    
    // Allocate contiguous physical memory for the module
    u64 module_size = virt_end - virt_start;
    auto module_dest = pmm.AllocContiguous32(module_size);
    
    // Load and relocate the PIE executable
    auto entry_point = Elf64::Load(elf.ValuePtr(), module_dest);
    Elf64::Relocate(elf.ValuePtr(), module_dest);
    
    return entry_point;
}
\end{cppcode}

\subsubsection{64-bit GDT and Mode Transition}

Before jumping to 64-bit code, a 64-bit Global Descriptor Table must be loaded. The GDT defines the code and data segments for Long Mode (Listing~\ref{lst:gdt64}):

\begin{nasmcode}[caption={64-bit GDT Definition (gdt.nasm)}, label={lst:gdt64}]
section .data
align 16
GDT64:
    .Null: dq 0
    .Code: equ $ - GDT64
        dw 0x0000         ; Limit (unused in 64-bit)
        dw 0x0000         ; Base (unused in 64-bit)
        db 0x00
        db 0x9A           ; Access: Present, Code, Readable
        db 0x20           ; Flags: Long mode
        db 0x00
    .Data: equ $ - GDT64
        dw 0x0000
        dw 0x0000
        db 0x00
        db 0x92           ; Access: Present, Data, Writable
        db 0x00
        db 0x00
.End:
    .Pointer:
        dw .End - GDT64 - 1
        dq GDT64
\end{nasmcode}

The final transition performs a far jump to flush the CPU pipeline and begin executing 64-bit code (Listing~\ref{lst:transition64}):

\begin{nasmcode}[caption={Transition to 64-bit Mode (transition\_64.nasm)}, label={lst:transition64}]
    ; Save kernel entry address and loader data pointer
    mov eax, [ebp+8]            ; entry_high
    mov [k_ptr+4], eax
    mov eax, [ebp+12]           ; entry_low  
    mov [k_ptr], eax
    mov eax, [ebp+16]           ; loader_data
    mov [loader_data_ptr], eax
    
    ; Load 64-bit GDT
    lgdt [GDT64.Pointer]
    
    ; Set data segments
    mov ax, GDT64.Data
    mov ss, ax
    mov ds, ax
    mov es, ax
    
    ; Far jump to 64-bit code segment
    jmp GDT64.Code:jmp_elf

[bits 64]
jmp_elf:
    ; Now executing in 64-bit Long Mode
    mov rdi, [loader_data_ptr]  ; Pass transition data
    mov rax, [k_ptr]
    jmp rax                     ; Jump to Loader64
\end{nasmcode}

\subsection{Loader64: Position-Independent Higher-Half Setup}

Loader64 executes in 64-bit Long Mode but faces a unique challenge: it is loaded at an unknown physical address by GRUB, yet must set up virtual memory and load the kernel at a fixed higher-half address.

\subsubsection{Position-Independent Compilation}

Loader64 is compiled as a Position-Independent Executable (PIE). The CMake configuration is shown in Listing~\ref{lst:loader64_cmake}:

\begin{cmakecode}[caption={Loader64 CMake Configuration}, label={lst:loader64_cmake}]
set_target_properties(alkos.boot.loader.64 PROPERTIES 
    POSITION_INDEPENDENT_CODE ON
)
target_link_options(alkos.boot.loader.64 PRIVATE
    -fPIE -pie
    "LINKER:-z,norelro"
)
\end{cmakecode}

This ensures all code uses RIP-relative addressing. The entry point demonstrates this (Listing~\ref{lst:loader64_entry}):

\begin{nasmcode}[caption={Loader64 Entry Point with RIP-Relative Addressing (entry.nasm)}, label={lst:loader64_entry}]
section .text
bits 64
Entry:
    ; RIP-relative addressing for position independence
    lea rsp, [rel stack_top]    ; Load stack using RIP-relative
    mov rbp, rsp
    
    mov r10d, edi               ; Save transition data pointer
    
    ; Load GDT using RIP-relative address
    lea rax, [rel GDT64.Pointer]
    lgdt [rax]
    
    ; Set up data segments
    mov ax, [rel GDT64_DATA_SELECTOR]
    mov ss, ax
    mov ds, ax
    mov es, ax
    
    mov edi, r10d
    call MainLoader64
\end{nasmcode}

\subsubsection{State Restoration from Loader32}

Loader32 passes its PMM and VMM state to Loader64 via a \texttt{TransitionData} structure. Loader64 deserializes this state to continue memory management, as shown in Listing~\ref{lst:transition_data}:

\begin{cppcode}[caption={TransitionData Deserialization (main.cpp)}, label={lst:transition_data}]
static auto InitializeLoaderEnvironment(const TransitionData *transition_data) {
    // Reconstruct PMM from serialized state
    PhysicalMemoryManager *pmm = 
        new PhysicalMemoryManager(transition_data->pmm_state);
    
    // Reconstruct VMM with existing page tables
    VirtualMemoryManager *vmm = 
        new VirtualMemoryManager(*pmm, transition_data->vmm_state);
    
    // Reconstruct Multiboot info accessor
    MultibootInfo mb_info(transition_data->multiboot_info_addr);
    
    return {MemoryManagers{*pmm, *vmm}, mb_info};
}
\end{cppcode}

\subsubsection{Kernel Loading at Higher-Half Address}

The kernel ELF is linked to execute at virtual address \texttt{0xFFFFFFFE00000000}. Loader64 allocates virtual memory at this address and loads the kernel segments (Listing~\ref{lst:kernel_load}):

\begin{cppcode}[caption={Higher-Half Kernel Loading (main.cpp)}, label={lst:kernel_load}]
// Kernel virtual address (upper 33 bits set)
static constexpr u64 kKernelVirtualAddressStart = 0xFFFFFFFE00000000ULL;

static u64 LoadKernelIntoMemory(MultibootInfo &multiboot_info,
                                const KernelModuleInfo &kernel_info,
                                MemoryManagers mem_managers) {
    auto &vmm = mem_managers.vmm;
    
    // Allocate virtual memory at the higher-half address
    vmm.Alloc(kKernelVirtualAddressStart, 
              kernel_info.size,
              kPresentBit | kWriteBit | kGlobalBit);
    
    // Load ELF segments into the allocated virtual memory
    byte *module_start = reinterpret_cast<byte *>(kernel_info.tag->mod_start);
    auto entry_point = Elf64::Load(module_start);
    
    return entry_point;
}
\end{cppcode}

\subsubsection{Direct Memory Mapping}

To allow the kernel to access physical memory directly, Loader64 creates a \textbf{direct memory mapping}, a large region of virtual address space that maps linearly to physical memory (Listing~\ref{lst:direct_map}):

\begin{cppcode}[caption={Direct Memory Mapping Setup (main.cpp)}, label={lst:direct_map}]
static constexpr u64 kDirectMemMapAddrStart = 0xFFFF800000000000ULL;
static constexpr u64 kDirectMemMapSizeGb = 512;

static void EstablishDirectMemMapping(MemoryManagers &mms) {
    auto &vmm = mms.vmm;
    
    // Map 512GB of physical memory to higher-half virtual addresses
    vmm.Map<PageSizeTag::k1Gb>(
        kDirectMemMapAddrStart,                        // Virtual base
        0,                                             // Physical base
        kDirectMemMapSizeGb * PageSize<PageSizeTag::k1Gb>(),
        kPresentBit | kWriteBit | kGlobalBit
    );
}
\end{cppcode}

This mapping allows the kernel to convert any physical address \texttt{P} to the virtual address \texttt{0xFFFF800000000000 + P}.

\subsubsection{Kernel Arguments Preparation}

Before transferring control to the kernel, Loader64 collects essential system information into a \texttt{KernelArguments} structure (Listing~\ref{lst:kernel_args}):

\begin{cppcode}[caption={Kernel Arguments Preparation (main.cpp)}, label={lst:kernel_args}]
struct PACK alignas(64) KernelArguments {
    /// Kernel Mem Layout
    u64 kernel_start_addr;
    u64 kernel_end_addr;

    /// VMem
    u64 pml_4_table_phys_addr;

    /// Memory Bitmap
    u64 mem_info_bitmap_addr;
    u64 mem_info_total_pages;

    /// Framebuffer
    u64 fb_addr;
    u32 fb_width;
    u32 fb_height;
    u32 fb_pitch;
    u32 fb_bpp;  // Bits per pixel

    // RGB Format
    u8 fb_red_pos;
    u8 fb_red_mask;
    u8 fb_green_pos;
    u8 fb_green_mask;
    u8 fb_blue_pos;
    u8 fb_blue_mask;

    /// Ramdisk
    u64 ramdisk_start;
    u64 ramdisk_end;

    /// ACPI
    u64 acpi_rsdp_phys_addr;

    /// Multiboot
    u64 multiboot_info_addr;
    u64 multiboot_header_start_addr;
    u64 multiboot_header_end_addr;
};
}
\end{cppcode}

The final transition to the kernel is a simple jump (Listing~\ref{lst:enter_kernel}):

\begin{nasmcode}[caption={Kernel Entry (transition.nasm)}, label={lst:enter_kernel}]
; void EnterKernel(u64 kernel_entry, KernelArguments *args)
;                  [rdi]            [rsi]
EnterKernel:
    xchg rdi, rsi       ; Swap: RDI = args, RSI = entry
    jmp rsi             ; Jump to kernel entry point
\end{nasmcode}

\subsection{Memory Layout}

Figure \ref{fig:memory_layout} illustrates the virtual memory layout established by the bootloader before kernel execution begins.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        memblock/.style={draw, minimum width=8cm, minimum height=0.9cm, font=\small\ttfamily},
        addrblock/.style={font=\tiny\ttfamily, anchor=east},
        scale=0.95
    ]
    
    % Virtual address space
    \node[font=\bfseries] at (2, 6.5) {Virtual Address Space};
    
    % Memory regions
    \node[memblock, fill=red!20] (kernel) at (2, 5.5) {Kernel Code \& Data};
    \node[addrblock] at (-2.3, 5.5) {0xFFFFFFFE00000000};
    
    \node[memblock, fill=blue!20] (direct) at (2, 4.3) {Direct Physical Memory Mapping (512GB)};
    \node[addrblock] at (-2.3, 4.3) {0xFFFF800000000000};
    
    \node[memblock, fill=gray!10, minimum height=2cm] (unused) at (2, 2.6) {Unused Virtual Address Space};
    
    \node[memblock, fill=green!20] (identity) at (2, 0.9) {Identity Mapped (10GB)};
    \node[addrblock] at (-2.3, 0.9) {0x0000000000000000};
    
    % Arrows to physical memory
    \node[font=\bfseries] at (9, 6.5) {Physical Memory};
    
    \node[memblock, fill=yellow!20, minimum width=4cm] (phys) at (10, 3) {Physical RAM};
    
    % Mapping arrows
    \draw[-Latex, thick, dashed] (kernel.east) -- ++(1,0) |- (phys.west);
    \draw[-Latex, thick] (direct.east) -- ++(0.5,0) |- (phys.170);
    \draw[-Latex, thick] (identity.east) -- ++(0.5,0) |- (phys.190);
    
    \end{tikzpicture}
    \caption{Virtual Memory Layout After Bootloader Initialization}
    \label{fig:memory_layout}
\end{figure}

\subsection{Architecture Initialization}

After the kernel receives control, the first architecture-specific function executed is \texttt{arch::ArchInit}. This function enables additional CPU features that were not safe to enable during the bootloader phase, as shown in Listing~\ref{lst:arch_init}:

\begin{cppcode}[caption={Architecture Initialization (init.cpp)}, label={lst:arch_init}]
namespace arch {
void ArchInit(const RawBootArguments &) {
    BlockHardwareInterrupts();
    
    ...
    
    // Enable CPU features (sequence is important)
    EnableOSXSave();    // Enable XSAVE/XRSTOR for context switching
    EnableSSE();        // Enable SSE extensions
    EnableAVX();        // Enable AVX if supported
    EnableNXE();        // Enable No-Execute bit for security
    
    ...
}
}
\end{cppcode}

% ==================================================================

\section{Physical Memory Management}
\label{sec:impl_pmm}

The Physical Memory Management subsystem of AlkOS acts as the ultimate authority over the machine's random access memory (RAM). Its primary responsibilities are tracking the usage status of every physical page frame, satisfying allocation requests from both the kernel and user space, and reclaiming memory when it is no longer needed. To balance the conflicting requirements of boot-time simplicity, runtime performance, and fragmentation minimization, AlkOS does not rely on a single monolithic allocation algorithm. Instead, it implements a hierarchical architecture composed of specialized allocators, each addressing a specific granularity of memory management.

\subsection{Architecture Overview}

The memory subsystem is structured into five distinct layers, designed to gradually increase the complexity and capability of the allocator as the system boots. This layered approach resolves the "chicken-and-egg" problem inherent in kernel initialization, where complex allocators (like the Buddy System) require dynamic memory for their own metadata before they are fully operational.

The architecture consists of the following components:

\begin{itemize}
    \item \textbf{Metadata Layer (\texttt{PageMetaTable})} --- The foundation of the entire subsystem. It is a static, global array that exists in a one-to-one mapping with physical page frames. It creates an abstraction layer allowing higher-level allocators to store state (such as free-list pointers or flags) without writing to the physical pages themselves.
    
    \item \textbf{Bootstrap Allocator (\texttt{BitmapPmm})} --- A primitive allocator used exclusively during the early boot phase. It manages memory using a simple bitmap provided by the Multiboot2 loader. Its sole purpose is to provide enough memory to initialize the Metadata Layer.
    
    \item \textbf{Frame Allocator (\texttt{BuddyPmm})} --- The primary engine for page-level allocation. Once the system is initialized, it takes ownership of all free memory. It uses the binary buddy algorithm to manage memory in power-of-two blocks (orders), facilitating efficient splitting and coalescing of physical frames.
    
    \item \textbf{Object Allocator (\texttt{SlabAllocator})} --- Operating on top of the Frame Allocator, this layer subdivides 4 KiB pages into smaller, fixed-size objects (e.g., 32 bytes, 64 bytes). This is essential for minimizing internal fragmentation when allocating kernel structures such as threads or file descriptors.
    
    \item \textbf{Global Heap Interface (\texttt{Heap})} --- A unified facade that exposes a standard C-style API (\texttt{KMalloc}, \texttt{KFree}). It acts as a router, dispatching large requests to the Frame Allocator and small requests to the Object Allocator.
\end{itemize}

This separation of concerns ensures that the kernel code remains decoupled from the specific allocation mechanics. The rest of the kernel interacts primarily with the \texttt{Heap} interface, unaware of the complex machinery operating beneath it.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance=1.0cm,
        layer/.style={
            rectangle, 
            draw=black!70, 
            rounded corners=3pt, 
            minimum width=8cm, 
            minimum height=1.0cm, 
            align=center, 
            fill=white,
            font=\small
        },
        arrow/.style={-Latex, thick}
    ]

    \node (facade) [layer, fill=gray!10] {\textbf{Facade Layer}\\ \texttt{Heap::Malloc(size)}};
    
    \node (slab) [layer, below left=1.0cm and -3cm of facade, minimum width=3.5cm, fill=blue!10] {\textbf{Object Layer}\\ \texttt{SlabAllocator}};
    
    \node (buddy) [layer, below right=1.0cm and -3cm of facade, minimum width=3.5cm, fill=green!10] {\textbf{Frame Layer}\\ \texttt{BuddyPmm}};
    
    \node (meta) [layer, below=2.8cm of facade, fill=orange!10] {\textbf{Metadata Layer}\\ \texttt{PageMetaTable}};
    
    \draw[arrow] (facade.south) -- node[left, font=\scriptsize] {$< 4096$ B} (slab.north);
    \draw[arrow] (facade.south) -- node[right, font=\scriptsize] {$\ge 4096$ B} (buddy.north);
    \draw[arrow] (slab.east) -- node[above, font=\scriptsize] {Request Pages} (buddy.west);
    \draw[arrow] (buddy.south) -- node[right, font=\scriptsize] {Manage State} (meta.north);

    \end{tikzpicture}
    \caption{Hierarchy of Physical Memory Allocators in AlkOS}
    \label{fig:pmm_hierarchy}
\end{figure}

\subsection{Page Metadata Management}

A fundamental challenge in physical memory management is tracking the state of every page frame --- whether it is free, allocated, part of a slab cache, or serving as a page table --- without consuming excessive memory or creating circular dependencies. Storing this metadata inside the page frames themselves is impossible for allocated pages, as the user data would overwrite the metadata. Conversely, dynamically allocating metadata nodes (e.g., using \texttt{malloc}) is not feasible because the memory manager itself is required to perform those allocations.

To resolve this, AlkOS implements a system inspired by the Linux \texttt{struct page} architecture. We pre-allocate a global array of metadata structures, called the \texttt{PageMetaTable}, where the index $i$ corresponds directly to Physical Frame Number (PFN) $i$.

\subsubsection{The \texttt{PageMeta} Structure}

To minimize memory overhead, the \texttt{PageMeta} structure utilizes a C++ \texttt{union} to multiplex its data fields. A physical page acts in mutually exclusive roles at any given time: it can be a free block in the Buddy System, a slab cache, or a generic allocated page, but never multiple simultaneously. By overlapping the storage requirements for these roles, the structure remains compact.

As shown in Listing \ref{lst:page_meta}, the structure contains a fixed header (type and order) and a polymorphic data section.

\begin{cppcode}[caption={Polymorphic Page Metadata Structure}, label={lst:page_meta}]
struct PageMeta {
    union {
        // State: Page is free and resides in Buddy Allocator
        struct BuddyMeta {
            VPtr<PageMeta> next; // Intrusive list pointer
            VPtr<PageMeta> prev; // Intrusive list pointer
        } buddy;

        // State: Page is being used as a Slab
        struct SlabMeta {
            VPtr<PageMeta> next; // List of partial/full/free slabs
            VPtr<PageMeta> prev;
            VPtr<KmemCache> cache; // Owner cache
            VPtr<void> freelist;   // Head of object free list
            u16 inuse;             // Active object count
        } slab;

        // State: Page is used as a Page Table (PML4/PDPT/etc)
        struct PageTableMeta {
            u16 ref_count;         // For shared mapping management
        } page_table;
        
        // ... other states
    } data;

    PageMetaType type; // Discriminator (Buddy, Slab, Allocated, etc.)
    u8 order;          // Size class of the block (2^order pages)
    
    // Padding ensures alignment and consistent size (40 bytes)
    u8 _padding[4]; 
};
\end{cppcode}

This design results in a structure size of exactly 40 bytes. Given a standard page size of 4096 bytes, the metadata overhead is approximately $0.97\%$, representing a highly efficient usage of physical memory for bookkeeping.

\subsubsection{The Metadata Table}

The \texttt{PageMetaTable} class manages the global array of these structures. It serves as the translation layer between physical addresses and their metadata. Since the array is contiguous and indexed by PFN, lookups are performed in $O(1)$ time using simple pointer arithmetic.

\begin{equation}
    \text{Meta}(Addr) = \text{Base} + \left( \frac{Addr}{\text{PageSize}} \right) \times \text{sizeof}(\text{PageMeta})
\end{equation}

This table allows high-level allocators to manipulate physical memory conceptually (e.g., "remove this block from the free list") without needing to access the physical page itself, which might not even be mapped into the kernel's virtual address space at the time of manipulation.


\subsection{Bootstrap Allocation}

The initialization of the physical memory subsystem presents a dependency cycle. The primary allocator (the Buddy System) relies on the \texttt{PageMetaTable} to function. However, the \texttt{PageMetaTable} itself is a substantial data structure that must be stored in memory. For a system with 32 GiB of RAM, this table requires approximately 320 MiB of contiguous physical memory. Allocating this memory requires an allocator, but the Buddy System is not yet initialized.

To resolve this deadlock, AlkOS implements a transient bootstrap allocator, the \texttt{BitmapPmm}.

\subsubsection{Mechanism}
The \texttt{BitmapPmm} operates on the raw memory map provided by the Multiboot2-compliant bootloader. During the earliest phase of kernel initialization, it calculates the total size of physical memory and selects a reserved region to store a simple bit array (bitmap). In this bitmap, a single bit represents one 4 KiB physical page frame (0 for free, 1 for used).

The allocation algorithm implemented in \texttt{BitmapPmm::Alloc} is a primitive linear search. To satisfy a request for $N$ pages, it scans the bitmap from the last known allocation index until it finds a run of $N$ contiguous zero bits.

\begin{cppcode}[caption={Bootstrap Linear Allocation Logic}, label={lst:bitmap_alloc}]
expected<FindBlockResult, MemError> BitmapPmm::FindContiguousBlock(
    size_t start_pfn, size_t end_pfn, u64 num_pages)
{
    size_t current_pfn = end_pfn - 1;
    u64 count = 0;

    // Backward linear scan to find free range
    do {
        if (IsFree(current_pfn)) {
            count++;
        } else {
            count = 0; // Reset counter on obstruction
        }

        if (count == num_pages) {
            return FindBlockResult{current_pfn};
        }
    } while (current_pfn-- > start_pfn);

    return unexpected(MemError::OutOfMemory);
}
\end{cppcode}

\subsubsection{Transition to Buddy System}
While the bitmap allocator is robust, its performance characteristics are poor ($O(N)$ allocation time), and it lacks support for sub-page allocations or efficient fragmentation management. Therefore, its lifespan is strictly limited to the kernel boot sequence.

Once the \texttt{BitmapPmm} has successfully allocated the memory required for the \texttt{PageMetaTable} and the kernel's initial paging structures, the memory module performs a handover. It iterates through the bitmap one final time. Every page marked as "free" in the bitmap is explicitly freed into the Buddy System, effectively transferring ownership of the remaining physical RAM to the advanced allocator. Following this transition, the \texttt{BitmapPmm} is discarded.

\subsection{Page Frame Allocator (The Buddy System)}

Upon system initialization, the \texttt{BuddyPmm} takes over as the authoritative source of physical memory. It implements the classic Binary Buddy System algorithm, managing memory in blocks sized by powers of two, ranging from Order 0 (4 KiB) to Order 10 (4 MiB). This structure allows for efficient splitting of large blocks to satisfy smaller requests and the coalescing of adjacent free blocks to reduce external fragmentation.

\subsubsection{Intrusive Free Lists}

To achieve high performance and zero memory overhead during runtime, the allocator utilizes an array of intrusive doubly linked lists, one for each order. It leverages the \texttt{PageMeta} structure described previously. When a block is free, its metadata entry functions as a node in the corresponding free list.

\begin{cppcode}[caption={Buddy System State Management}, label={lst:buddy_lists}]
class BuddyPmm {
public:
    static constexpr u8 kMaxOrder = 10;
private:
    // Array of pointers to the head of each order's free list
    VPtr<PageMeta> freelist_table_[kMaxOrder + 1];
    
    // ...
};
\end{cppcode}

This design ensures that pushing or popping a block from the free list involves only pointer manipulation within the pre-allocated metadata table, requiring no auxiliary dynamic allocation.

\subsubsection{Allocation Strategy (Splitting)}

When a request for a block of order $k$ is received, the allocator inspects the free list at index $k$. If the list is empty, it searches for the smallest available block at a higher order $j > k$. Once found, the large block is recursively split in half ("buddies") until a block of the desired order is produced. The unused halves generated during this process are inserted into their respective free lists for future use.

\subsubsection{Deallocation and Coalescing}

The strength of the Buddy System lies in its ability to rapidly defragment memory. When a page frame is freed, the allocator attempts to merge it with its "buddy" --- the unique adjacent block of the same size that makes up a larger aligned block.

Because block sizes are powers of two, the physical frame number (PFN) of a block's buddy can be calculated instantly using bitwise operations, without traversing any lists.

\begin{cppcode}[caption={Calculating Buddy PFN}, label={lst:buddy_xor}]
static size_t GetBuddyPfn(size_t pfn, u8 order)
{
    // The buddy's PFN is found by XORing the current PFN 
    // with the block size (1 << order).
    return pfn ^ (1 << order);
}
\end{cppcode}

The \texttt{Free} operation performs a loop: it calculates the buddy's address and checks the metadata table. If the buddy is currently free and belongs to the same order, it is removed from the free list, combined with the current block, and the order is incremented. This process repeats until a buddy is found to be occupied or the maximum order is reached.

\begin{cppcode}[caption={Coalescing Logic}, label={lst:buddy_merge}]
VPtr<PageMeta> BuddyPmm::MergeBlock(VPtr<PageMeta> block)
{
    u8 order = block->order;
    size_t pfn = pmt_->GetPageFrameNumber(block);

    while (order < kMaxOrder) {
        size_t buddy_pfn = GetBuddyPfn(pfn, order);
        PageMeta &buddy = pmt_->GetPageMeta(buddy_pfn);

        // Check if buddy is free and has the same order
        if (buddy.type == PageMetaType::Buddy && buddy.order == order) {
            ListRemove(&buddy);
            
            // Normalize PFN to the start of the merged block
            pfn = std::min(pfn, buddy_pfn);
            order++;
        } else {
            break; // Buddy is busy, cannot merge further
        }
    }
    
    // ... update metadata for new larger block
}
\end{cppcode}


\subsection{Object Allocator (Slab Allocation)}

While the Buddy System effectively manages memory at the granularity of 4 KiB pages, it is unsuitable for allocating small kernel objects. Allocating a full page for a 64-byte data structure (such as a synchronization primitive) results in over 98\% wasted memory (internal fragmentation). To address this, AlkOS implements a \textbf{Slab Allocator} layered on top of the Buddy System.

The Slab Allocator operates by pre-allocating contiguous blocks of physical memory (Slabs) and carving them into fixed-size slots. These slots are managed via caches (\texttt{KmemCache}), where each cache is dedicated to objects of a specific size class (e.g., 32, 64, 128 ... 4096 bytes).

\subsubsection{Slab Architecture and Lifecycle}

A cache maintains three distinct lists of slabs based on their utilization:
\begin{itemize}
    \item \textbf{Full:} All slots contain active objects.
    \item \textbf{Partial:} Contains at least one allocated object and at least one free slot.
    \item \textbf{Free:} All slots are unused.
\end{itemize}

Allocation requests are satisfied from the \texttt{Partial} list to maximize density. If no partial slabs exist, a free slab is utilized. If the free list is empty, the cache requests a new memory block from the Buddy System to construct a new slab. Conversely, when an object is freed, the slab may transition from Full to Partial, or Partial to Free, allowing the underlying pages to be returned to the Buddy System when memory pressure is high.

\subsubsection{Compile-Time Efficiency Optimization}

A naive slab implementation typically assigns one memory page per slab. However, this often leads to poor utilization due to alignment padding. For instance, if an object is 2050 bytes, a 4096-byte page can hold only one object, resulting in nearly 50\% wasted space. If the slab size were increased to 8192 bytes (Order 1), it could hold three objects (3 $\times$ 2050 = 6150 bytes), significantly improving efficiency.

AlkOS employs advanced C++ Template Metaprogramming techniques to solve this optimization problem at compile time. The \texttt{SlabEfficiency} module analyzes every supported object size against every possible Buddy System block order (0 to 10). It calculates the \textbf{Space Efficiency} metric:

\begin{equation}
    E(S_{obj}, O_{block}) = \frac{N_{capacity} \times S_{obj}}{\text{Size}(O_{block})}
\end{equation}

The system automatically selects the block order that yields the highest efficiency for a given object size.

\begin{cppcode}[caption={Compile-Time Slab Efficiency Calculation}, label={lst:slab_efficiency}]
template <size_t ObjectSize, u8 BlockOrder>
struct SlabEfficiencyInfo {
    static constexpr size_t kBlockSize = BuddyPmm::BuddyAreaSize(BlockOrder);
    
    // Calculate required metadata size (byte vs word vs dword indices)
    using FreeListItemType = ...; 
    static constexpr size_t kTotalStoredBytesPerObj = 
        ObjectSize + sizeof(FreeListItemType);

    // Calculate how many objects fit
    static constexpr size_t kCapacity = 
        (kBlockSize / kTotalStoredBytesPerObj);

    // Final efficiency ratio (0.0 to 1.0)
    static constexpr f64 kSpaceEfficiency =
        (static_cast<f64>(kCapacity * ObjectSize) / kBlockSize);
};
\end{cppcode}

This ensures that the allocator statically guarantees the most memory-efficient layout for every cache without runtime overhead.

\subsubsection{Metadata Strategy: On-Slab vs. Off-Slab}

To track free slots within a slab, the allocator maintains a freelist. The storage location of this freelist depends on the object size:

\begin{itemize}
    \item \textbf{On-Slab (Small Objects):} For small objects, the freelist indices are interleaved with the objects within the slab page itself. This improves cache locality.
    \item \textbf{Off-Slab (Large Objects):} For larger objects, embedding metadata might reduce the number of objects that fit in a page (e.g., preventing a 4th object from fitting by a few bytes). In these cases, metadata is stored in a separate, dedicated cache.
\end{itemize}

The allocator determines the optimal strategy automatically based on the efficiency calculations described above.

\subsection{The Global Heap Interface}

The top-most layer of the physical memory subsystem is the \texttt{Heap} class. It functions as a facade, hiding the complexity of the underlying allocators and providing a unified, standard-compliant API to the rest of the kernel. This design allows kernel developers to request memory without needing to decide manually whether to use the Slab allocator or the Buddy System.

\subsubsection{Request Routing}

The implementation of \texttt{Heap::Malloc} acts as a router based on the requested size. The threshold is the system page size (4096 bytes).

\begin{itemize}
    \item \textbf{Small Allocations ($< 4096$ bytes):} These are routed to the \texttt{SlabAllocator}. The heap requests the appropriate cache for the size class and pops an object. This ensures minimal overhead for small, frequent allocations.
    \item \textbf{Large Allocations ($\ge 4096$ bytes):} These are routed directly to the \texttt{BuddyPmm}. The heap calculates the required page order ($\lceil \log_2(size / 4096) \rceil$) and requests a block of raw pages.
\end{itemize}

\subsubsection{Aligned Allocation}

Standard allocations guarantee natural alignment suitable for the object size (e.g., 8-byte alignment for 64-byte objects). However, kernel operations often require stricter alignment (e.g., page alignment for paging structures or 64-byte alignment for AVX vectors).

To support this, AlkOS implements \texttt{KMallocAligned}. Since the underlying allocators cannot arbitrarily shift the start address of a block, this is achieved through over-allocation and pointer adjustment.

\begin{cppcode}[caption={Aligned Allocation Strategy}, label={lst:aligned_alloc}]
struct AllocationHeader {
    VPtr<void> original_ptr;
};

expected<VPtr<void>, MemError> Heap::MallocAligned(KMallocRequest r) {
    size_t alloc_size = r.size + r.alignment + sizeof(AllocationHeader);
    auto raw_ptr = Malloc(alloc_size); // Allocate extra padding

    uptr raw_addr = PtrToUptr(*raw_ptr);
    
    // Calculate aligned address with space for header
    uptr header_end = raw_addr + sizeof(AllocationHeader);
    uptr aligned_addr = AlignUp(header_end, r.alignment);

    // Store original pointer immediately before the aligned address
    auto *header = reinterpret_cast<AllocationHeader*>(
        aligned_addr - sizeof(AllocationHeader)
    );
    header->original_ptr = *raw_ptr;

    return reinterpret_cast<VPtr<void>>(aligned_addr);
}
\end{cppcode}

When \texttt{KFreeAligned} is called, the system simply steps back \texttt{sizeof(AllocationHeader)} bytes from the provided pointer to retrieve the \texttt{original\_ptr}, which is then passed to the standard \texttt{Free} routine.

\subsection{Summary of Memory Flow}

The complete flow of a physical memory request in AlkOS can be summarized as follows:

\begin{enumerate}
    \item \textbf{Request:} The kernel calls \texttt{KMalloc(64)}.
    \item \textbf{Routing:} The \texttt{Heap} detects size $<$ 4KiB and delegates to \texttt{SlabAllocator}.
    \item \textbf{Cache Lookup:} \texttt{SlabAllocator} selects the \texttt{KmemCache} for 64-byte objects.
    \item \textbf{Allocation:}
    \begin{itemize}
        \item \textit{Case A (Hot Path):} The cache has a partial slab. An object is popped from the freelist. Cost: $O(1)$.
        \item \textit{Case B (Cold Path):} The cache is empty. It requests a physical page (Order 0) from \texttt{BuddyPmm}. The Buddy allocator updates the \texttt{PageMetaTable} to mark the page as a Slab, initializes the freelist, and returns the memory.
    \end{itemize}
    \item \textbf{Return:} The virtual address is returned to the caller.
\end{enumerate}

This hierarchical structure ensures that AlkOS balances the fast, low-fragmentation needs of small objects with the efficient, contiguous page management required for virtual memory mappings.

\section{Virtual Memory Management}
\label{sec:impl_vmm}

While the Physical Memory Manager accounts for raw RAM usage, the Virtual Memory Manager (VMM) allows the kernel to shape the execution environment. Our implementation aims to provide a robust, architecture-agnostic interface that supports advanced features like lazy allocation and efficient kernel-space management, while abstracting away the specifics of the x86-64 paging hardware.

\subsection{Design Overview and Architecture}

The cornerstone of the AlkOS memory layout is the \textbf{Higher-Half Kernel} architecture combined with a \textbf{Direct Physical Mapping}. 

Upon initialization, the bootloader (Loader64) ensures that the kernel executable is linked and loaded at the top of the virtual address space, specifically starting at \texttt{0xFFFFFFFE00000000}. This design choice keeps the lower half of the address space (canonical addresses starting with \texttt{0x0000...}) strictly reserved for user-space applications, enforcing a clean separation between kernel and user contexts.

\subsubsection{The Direct Physical Map (HHDM)}

A specific challenge in OS development is accessing arbitrary physical memory (e.g., modifying page tables or reading ACPI tables) when paging is enabled. Without a specific strategy, the kernel would need to temporarily map individual physical pages into a reserved virtual window every time it needed to read or write to them.

To solve this, we implemented a Direct Physical Mapping strategy. A massive region of the kernel virtual address space (512 GB, starting at \texttt{0xFFFF800000000000}) acts as a linear window into physical RAM. Within this window, virtual address $V$ corresponds to physical address $P$ via a simple linear transformation:
\[ V = P + \texttt{kDirectMapAddrStart} \]

This allows the kernel to access any physical object instantly, without modifying page tables. This logic is encapsulated in the helper templates found in \texttt{mem/types.hpp}:

\begin{cppcode}[caption={Physical to Virtual Address Translation helpers}, label={lst:phys_to_virt}]
template <typename T>
FORCE_INLINE_F VPtr<T> PhysToVirt(PPtr<T> physAddr)
{
    return reinterpret_cast<VPtr<T>>(
        reinterpret_cast<uintptr_t>(physAddr) + hal::kDirectMapAddrStart
    );
}

template <typename T>
FORCE_INLINE_F PPtr<T> VirtToPhys(VPtr<T> virtAddr)
{
    uintptr_t vaddr = reinterpret_cast<uintptr_t>(virtAddr);
    
    // Handle Direct Map range
    if (vaddr >= hal::kDirectMapAddrStart) {
        return reinterpret_cast<PPtr<T>>(vaddr - hal::kDirectMapAddrStart);
    }

    // Handle Kernel Linkage range
    return reinterpret_cast<PPtr<T>>(vaddr - kKernelSpaceStart);
}
\end{cppcode}

\subsubsection{Hardware Abstraction Layer}

To decouple the core VMM logic from the x86-64 specific paging structures (PML4, PDPT, etc.), we utilize the Hardware Abstraction Layer (HAL). The \texttt{hal::Mmu} class exposes high-level operations such as \texttt{Map}, \texttt{Unmap}, and \texttt{SwitchRoot}. 

Crucially, the interface abstracts architecture-specific protection bits into a generic \texttt{PageFlags} structure. The HAL implementation translates these generic flags into the specific bitmask required by the hardware (e.g., the NX bit or the User/Supervisor bit on x86-64).

\begin{cppcode}[caption={Generic Page Flags Abstraction}, label={lst:page_flags}]
struct PageFlags {
    bool Present : 1;
    bool Writable : 1;
    bool UserAccessible : 1;
    bool WriteThrough : 1;
    bool CacheDisable : 1;
    bool Global : 1;
    bool NoExecute : 1;
};
\end{cppcode}

\subsection{The Address Space Abstraction}

In AlkOS, the concept of a virtual memory context is encapsulated by the \texttt{AddressSpace} class. This class serves as the container for all memory mappings associated with a specific process (or the kernel itself). It maintains two parallel representations of the memory state:

\begin{enumerate}
    \item \textbf{Hardware Representation:} A physical pointer to the root of the page table hierarchy (e.g., the PML4 address). This is the value loaded into the CR3 register during a context switch.
    \item \textbf{Logical Representation:} A sorted, doubly linked list of \texttt{VMemArea} objects. This list defines the high-level semantics of memory regions, which are invisible to the hardware MMU.
\end{enumerate}

To ensure data consistency in a potentially multi-threaded environment (or during interrupt handling), operations on the logical list are protected by a \texttt{Spinlock}.

\subsubsection{User Space Initialization}

When creating a new user address space, the system must ensure that the process can still access essential kernel services (such as interrupt handlers and syscall entry points). The \texttt{AddressSpace::InitUser} method allocates a new, empty PML4 table and immediately copies the kernel-space mappings from the global kernel address space. This creates a "split" view where the lower half is empty (ready for user program code), and the upper half mirrors the kernel.

\subsection{Virtual Memory Areas (VMAs)}

The \texttt{VMemArea} class is the fundamental unit of logical memory management. It is an abstract base class that defines a range of virtual addresses \texttt{[start, start + size)} and a set of permission flags. Crucially, it exposes a virtual method \texttt{HandleFault}, which implements the polymorphic behavior required for demand paging.

When a Page Fault occurs, the VMM locates the VMA corresponding to the faulting address and delegates the resolution logic to that specific object. AlkOS implements three distinct types of VMAs:

\subsubsection{Anonymous Memory}

The \texttt{AnonymousVMemArea} represents standard, volatile RAM used for stacks, heaps, and BSS sections. It does not map to any specific physical address at creation time. Its \texttt{HandleFault} implementation interacts with the Physical Memory Manager (PMM) to allocate a fresh physical frame, zeroes it for security, and maps it into the page table.

\subsubsection{Direct Mapping}

The \texttt{DirectMappingVMemArea} is used when a specific range of physical memory must be exposed to virtual memory. This is essential for:
\begin{itemize}
    \item \textbf{Memory Mapped I/O (MMIO):} Mapping hardware device registers.
    \item \textbf{Framebuffers:} Exposing the video memory to user-space applications (as seen in the Window Manager implementation).
\end{itemize}

Unlike anonymous memory, this area ignores the "Not Present" fault mechanism for allocation; it simply maps the pre-existing physical range.

\subsubsection{Kernel Synchronization Area}

A unique feature of the AlkOS VMM is the \texttt{KernelSyncVMemArea}. This area covers the entire kernel address range in every user process. 

In a standard x86-64 higher-half kernel, kernel page tables are shared across all processes. However, if the kernel allocates dynamic memory (e.g., via \texttt{vmalloc}) that requires a new entry in the top-level PML4 table, that change is not automatically propagated to existing user processes.

Instead of iterating through every active process to update their page tables whenever the kernel heap grows, we utilize a lazy synchronization approach. If a process accesses a valid kernel address that is missing from its specific PML4, a page fault is triggered. The \texttt{KernelSyncVMemArea::HandleFault} method intercepts this, copies the missing entry from the master kernel page table to the process's page table, and resumes execution.

\begin{cppcode}[caption={Lazy Kernel Synchronization Logic}, label={lst:kernel_sync}]
bool KernelSyncVMemArea::HandleFault(
    VPtr<void> fault_addr, const PageFaultData::ErrorCode &, AddressSpace &as)
{
    auto &vmm = MemoryModule::Get().GetVmm();
    
    // Master kernel tables
    auto kernel_root  = vmm.GetKernelAddressSpace().PageTableRoot();
    auto current_root = as.PageTableRoot();

    // Check if the mapping exists in the master kernel table
    auto &mmu = MemoryModule::Get().GetMmu();
    if (mmu.SyncMapping(current_root, kernel_root, fault_addr)) {
        // Entry copied successfully, invalidate TLB and retry
        MemoryModule::Get().GetTlb().InvalidatePage(AlignDown(fault_addr));
        return true;
    }

    return false; // Genuine crash
}
\end{cppcode}

\subsection{Initialization and Handover}

The transition from the bootloader to the kernel's internal memory management is a critical phase. When the kernel's main function receives control, the system is running on page tables created by \texttt{Loader64}. These tables contain a temporary identity mapping (0-10 GB) that is necessary for the loader but undesirable for the kernel, as it exposes physical memory to user-space (NULL pointer dereferences would be valid accesses to physical address 0).

To sanitize this state, we implemented the \texttt{Mem::Boot::BootMmuCleaner}. Its responsibilities are twofold:

\begin{enumerate}
    \item \textbf{Cleaning Identity Mappings:} It iterates through the lower half of the PML4 table and unmaps all entries. This ensures that the user-space range is completely empty before the first process is spawned.
    \item \textbf{Metadata Reconstruction:} The physical frames used by the bootloader to store the page tables themselves must be accounted for. The \texttt{BootMmuCleaner} walks the existing page table hierarchy. For every table frame encountered, it updates the global \texttt{PageMetaTable} to mark that frame's type as \texttt{PageTable} and initializes its reference count. This prevents the Physical Memory Manager from accidentally treating these critical frames as free memory.
\end{enumerate}

\subsection{The Virtual Memory Manager}

The \texttt{VirtualMemoryManager} (VMM) serves as the central orchestrator for all virtual memory operations. Exposed as a singleton via the \texttt{MemoryModule}, it provides the high-level API required by the kernel to create address spaces, allocate stack and heap regions, and switch contexts.

\subsubsection{Allocation Strategy}

Unlike physical memory, virtual memory is abundant (48-bit address space on x86-64). The primary challenge is not finding space, but finding a \textit{contiguous} gap of free addresses that satisfies alignment requirements.

The VMM delegates this geometric search to the \texttt{AddressSpace::FindGap} method. This method iterates through the sorted list of existing VMAs. For every pair of adjacent VMAs, it calculates the free space between the end of the first and the start of the second. If the gap is sufficient, it returns the start address aligned to the page size.

\begin{cppcode}[caption={Gap Search Logic}, label={lst:find_gap}]
expected<GapInfo, MemError> AS::FindGap(size_t size, VPtr<void> range_start, VPtr<void> range_end)
{
    // ... setup search bounds ...
    
    for (auto *vma : area_list_) {
        uptr curr_start = PtrToUptr(vma->GetStart());
        
        // Calculate gap between previous end and current start
        uptr gap_start = AlignUp(prev_end, hal::kPageSizeBytes);
        uptr gap_end   = AlignDown(curr_start, hal::kPageSizeBytes);

        if (gap_end > gap_start && (gap_end - gap_start) >= size) {
            return GapInfo{UptrToPtr<void>(gap_start), size};
        }

        prev_end = curr_start + vma->GetSize();
    }
    // ... check tail gap ...
}
\end{cppcode}

\subsubsection{Specialized Allocators}

To structure the user address space efficiently, the VMM provides specialized allocation helpers that enforce the standard memory layout:

\begin{itemize}
    \item \textbf{User Stack:} The \texttt{AllocUserStack} method searches for free space starting from the top of the user address space (\texttt{kUserSpaceEndExclusive}) downwards. This ensures that the stack is placed as high as possible, allowing it to grow downwards without colliding with the heap.
    \item \textbf{User Heap:} The \texttt{AllocUserHeap} method searches from the bottom of the user address space upwards (typically starting after the ELF binary segments).
\end{itemize}

\subsubsection{Context Switching}

The VMM is also responsible for the low-level mechanics of address space switching. The \texttt{SwitchAddrSpace} method interacts with the HAL to load the new physical root (CR3) into the CPU. Critically, this operation implicitly flushes the Translation Lookaside Buffer (TLB), ensuring that the CPU does not retain stale translations from the previous process.

\subsection{Page Fault Handling Workflow}

The heart of the virtual memory subsystem is the page fault handler. It transforms hardware exceptions into legitimate resource allocation events. The handling flow in AlkOS proceeds as follows:

\begin{enumerate}
    \item \textbf{Exception Entry:} The CPU triggers interrupt vector 14. The assembly wrapper saves the execution context and calls the C++ dispatcher.
    \item \textbf{Data Parsing:} The generic interrupt dispatcher invokes \texttt{Mem::PageFaultHandler}. The first step is to convert architecture-specific registers (CR2 for the address, and the error code from the stack) into a platform-agnostic \texttt{PageFaultData} structure using the HAL.
    
    \item \textbf{VMA Lookup:} The handler acquires the lock for the current \texttt{AddressSpace} and attempts to find a \texttt{VMemArea} that covers the faulting address.
    \begin{itemize}
        \item If no VMA is found, the access is invalid. If the fault originated from user space, the process is terminated (Segmentation Fault). If from kernel space, the kernel panics.
    \end{itemize}

    \item \textbf{Permission Check:} If a VMA is found, the handler verifies the access type against the VMA flags. For example, attempting to write to a Read-Only area triggers a protection fault, which is treated as a fatal error (unless Copy-on-Write logic intervenes).
    
    \item \textbf{Resolution:} If the access is valid but the page is missing (Demand Paging), the VMA's \texttt{HandleFault} method is invoked to allocate and map the required memory.
\end{enumerate}

\subsection{Address Space Layout}

The Virtual Memory Manager enforces a strict layout for user processes to ensure security and stability. The resulting virtual address space is organized as follows:

\begin{itemize}
    \item \textbf{\texttt{0x0000000000000000}} --- \textbf{Null Guard}: The first page is strictly unmapped to catch null pointer dereferences immediately.
    \item \textbf{User Code and Data}: Loaded from the ELF executable, typically starting at a dynamic base address.
    \item \textbf{User Heap}: Located after the BSS section, growing upwards towards higher addresses.
    \item \textbf{User Stack}: Located at the very top of the user half of the address space (\texttt{0x00007FFFFFFFFFFF}), growing downwards.
    \item \textbf{\texttt{0xFFFF800000000000}} --- \textbf{Direct Physical Map}: Kernel-only access window to all physical RAM.
    \item \textbf{\texttt{0xFFFFFFFE00000000}} --- \textbf{Kernel Image}: The code and static data of the kernel itself.
\end{itemize}

This layout, combined with the separation of Machine Independent logic and the Hardware Abstraction Layer, provides a flexible foundation for the higher-level operating system services described in subsequent sections.

\section{Interrupts}
\label{sec:interrupts}

Interrupt handling is a critical component of any operating system. Without interrupts, the system would rely on manually polling devices or checking states, which is inefficient and wasteful of resources. Furthermore, interrupts enable the system to be significantly more responsive. For example, when a keyboard key is pressed, the operating system may need to immediately switch execution to the thread responsible for consuming the input, rather than waiting for the current task to finish. More broadly, the OS utilizes the interrupt mechanism to perform context switches. If a task exceeds its allocated execution time, timing devices such as the LAPIC Timer trigger an interrupt, allowing the scheduler to preempt the current task and grant CPU time to others.

\subsection{x86-64 Interrupts}

\subsubsection{Interrupt Handling}
On the x86-64 architecture, every interrupt is assigned a unique number (vector) which maps directly to an entry in the \textbf{Interrupt Descriptor Table} (IDT) \cite{IntelManual-Interrupts}. This table contains instructions for the CPU on how to react to specific interrupts. The entry layout is as follows:

\begin{cppcode}[caption={IDT Entry Layout}, label={lst:idtEntry}]
  enum class IdtPrivilegeLevel : u8 { kRing0 = 0, kRing1 = 1, kRing2 = 2, kRing3 = 3 };

  struct PACK IdtEntryFlags {
      IdtGateType type : 4;
      u8 zero : 1;
      IdtPrivilegeLevel dpl : 2;
      u8 present : 1;
  };
  struct PACK IdtEntry {
      u16 isr_low;    // The lower 16 bits of the ISR's address
      u16 kernel_cs;  // The GDT segment selector that the CPU
                      //  will load into CS before calling the ISR
      u8 ist;         // The IST in the TSS that the CPU will load into RSP
      IdtEntryFlags attributes;  // Type and attributes
      u16 isr_mid;  // The higher 16 bits of the lower 32 bits of the ISR's address
      u32 isr_high; // The higher 32 bits of the ISR's address
      u32 reserved; // Set to zero
  };
\end{cppcode}

The most critical component of the IDT entry is the address of the function to be invoked (split into \texttt{isr\_low}, \texttt{isr\_mid}, and \texttt{isr\_high}). Another important field is \texttt{kernel\_cs}, which specifies the code segment \cite{IntelManual-Segments} loaded before executing the handler. In x86-64, there are four privilege levels (Rings). We assume the kernel always operates in Ring 0 (the most privileged level). Therefore, the Ring 0 kernel code segment is always written to this field. Conversely, the \texttt{IdtPrivilegeLevel dpl} field specifies the minimal privilege level required to trigger the interrupt via software, which is essential for implementing system calls (syscalls) invoked from user space.

\subsubsection{Interrupt Service Routines}

Functions handling interrupts differ significantly from standard functions generated by the compiler. When the CPU calls an interrupt handler, it first switches the stack pointer to the kernel stack if a privilege level change occurs (e.g., Ring 3 to Ring 0). This transition is managed via the Task State Segment (TSS) mechanism (for details, refer to \cite{osdev-tss}). The CPU then changes the code segment as specified in the \textbf{IDT entry} (Listing \ref{lst:idtEntry}). Subsequently, it pushes the state of the interrupted procedure onto the stack.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=0cm,
      start chain=going below,
      stacknode/.style={
          draw, 
          minimum width=5cm, 
          minimum height=1cm, 
          outer sep=0pt, 
          font=\ttfamily
      },
      labelnode/.style={
          minimum height=1cm,
          font=\footnotesize\sffamily,
          anchor=east
      }
  ]
  
  \node (highmem) at (0, 1) {Higher address (High Memory)};
  \draw[->] (highmem) -- (0, 0.2);

  \node [stacknode, on chain, fill=gray!10] (ss) {Stack Segment (SS)};
  \node [stacknode, on chain, fill=gray!10] (rsp) {Stack Pointer (RSP)};
  \node [stacknode, on chain, fill=gray!10] (rflags) {RFLAGS};
  \node [stacknode, on chain, fill=gray!10] (cs) {Code Segment (CS)};
  \node [stacknode, on chain, fill=gray!10] (rip) {Instruction Pointer (RIP)};
  \node [stacknode, on chain, fill=gray!10] (error) {Error Code (optional)};
  \draw[<-, thick, red] (error.east) -- ++(1.5,0) node[right, text=red] {Current RSP};

  \node [below=0.5cm of error] (lowmem) {Lower address (Low Memory)};
  \draw[->] (lowmem.north) -- (error.south);

  \end{tikzpicture}
  \caption{Interrupt Service Routine stack layout on entry}
  \label{fig:stackframe}
\end{figure}

The layout illustrated in Figure \ref{fig:stackframe} is known as the \textbf{Interrupt Frame}. This structure is fundamental to the kernel architecture, serving as the basis for context switching, context conversion, and jumping to user space (Ring 3). A key distinction is that an \textbf{ISR} must return using the special instruction \textbf{IRETQ} \cite{IntelManual-Interrupts}, which reverses the actions described above, including restoring the privilege level and code segment. 

A significant challenge with standard compiler-generated functions is that they may modify the stack frame (prologue/epilogue) in ways that interfere with the hardware-defined layout. To maintain full control and prevent stack corruption, we implement assembly wrappers. These wrappers perform the necessary architecture-specific actions before invoking the architecture-agnostic interrupt handling code defined in C++.

\begin{nasmcode}[caption={Assembly ISR wrapper}, label={lst:isr_asm}]
%macro context_switch_if_needed 0
  cmp rax, 0
  je .done                         ; Omit context switch if there is no need

  mov r13, rax                     ; save next TCB

  mov rdi, r13
  mov rsi, rsp
  call cdecl_ContextSwitchOnInterrupt

  mov rsp, [r13+Thread.kernel_stack]   ; Change the stack
.done:

  load_user_gs_if_needed

  pop_all_regs                    ; Restore registers.
  add rsp, _all_reg_size          ; Deallocate register save space.
  add rsp, 8                      ; Pop error code.
  iretq
%endmacro

; Macro for hardware or software interrupts.
; Calls a handler with the signature 'void handler(u16 lirq, void* frame)'.
%macro interrupt_wrapper 3 ; %1: Logical IRQ, %2: idt idx %3: C handler function
isr_wrapper_%+%2:
    push 0                          ; Push a dummy error code for unification.
    sub rsp, _all_reg_size          ; Allocate space for saving registers.
    push_all_regs                   ; Save registers.

    load_kernel_gs_if_needed

    cld                         ; Clear direction flag for string operations.
    mov rdi, %1                 ; Arg1: mapped lirq number.
    mov rsi, rsp                ; Arg2: pointer to stack frame.
    call %3                     ; Call the specific ISR handler.

    context_switch_if_needed
%endmacro
\end{nasmcode}

\subsubsection{Synchronization}

Since an interrupt can potentially trigger a context switch, synchronization is crucial. This applies to both kernel-space threads at any moment of their lifetime and user space programs executing system calls. It would be catastrophic if a timer interrupt forced a context switch while the kernel was in the middle of updating scheduler structures or memory tables. Therefore, even on a single-core system, synchronization must be enforced. This is achieved by disabling hardware interrupts (using \textbf{CLI} and \textbf{STI} instructions on x86-64) during critical sections.

\subsection{Unified Interrupt Frame}
\label{subsec:unifiedFrame}

To simplify interrupt handling, we introduced a unified frame structure based on the hardware Interrupt Frame (Figure \ref{fig:stackframe}). Since the state of a thread must be preserved before a context switch, all general-purpose registers must be saved. To achieve this efficiently, these registers are pushed onto the stack by the assembly wrapper, creating the \textbf{Unified Interrupt Frame}.

The x86-64 architecture introduces an inconsistency in the stack layout depending on the interrupt source. Certain exceptions, such as Page Faults (vector 14) or General Protection Faults (vector 13), automatically push an error code onto the stack by the CPU. Hardware interrupts and other exceptions do not. To use a single, unified C++ structure for all interrupt handling (\texttt{IsrErrorStackFrame}), the assembly entry wrappers must normalize the stack. For vectors that do not produce a hardware error code, the wrapper explicitly pushes a dummy value (typically 0) before saving the general-purpose registers (as seen in Listing \ref{lst:isr_asm}). This ensures that the stack pointer is always aligned correctly and points to a uniform structure when the C++ handler is invoked.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=0cm,
      start chain=going below,
      stacknode/.style={
          draw, 
          minimum width=5cm, 
          minimum height=1cm, 
          outer sep=0pt, 
          font=\ttfamily
      },
      labelnode/.style={
          minimum height=1cm,
          font=\footnotesize\sffamily,
          anchor=east
      }
  ]
  
  \node (highmem) at (0, 1) {Higher address (High Memory)};
  \draw[->] (highmem) -- (0, 0.2);

  \node [stacknode, on chain, fill=gray!10] (ss) {Stack Segment (SS)};
  \node [stacknode, on chain, fill=gray!10] (old_rsp) {Old Stack Pointer (RSP)};
  \node [stacknode, on chain, fill=gray!10] (rflags) {RFLAGS};
  \node [stacknode, on chain, fill=gray!10] (cs) {Code Segment (CS)};
  \node [stacknode, on chain, fill=gray!10] (rip) {Instruction Pointer (RIP)};
  \node [stacknode, on chain, fill=gray!10] (error) {Error Code (or Dummy)};
  \node [stacknode, on chain, fill=gray!10] (genregs) {General Registers (RDI, RSI, ...)};
  \node [stacknode, on chain, fill=gray!10] (rax) {Last Saved Register};
  
  \draw[<-, thick, red] (rax.east) -- ++(1.5,0) node[right, text=red] {RSP (New Stack Pointer)};

  \node [below=0.5cm of rax] (lowmem) {Lower address (Low Memory)};
  \draw[->] (lowmem.north) -- (rax.south);

  \end{tikzpicture}
  \caption{Unified Interrupt Frame (IsrErrorStackFrame)}
  \label{fig:unifiedstackframe}
\end{figure}

\subsection{Context Switch}

As previously mentioned, the OS utilizes the interrupt mechanism to perform task switching. The interrupt mechanism handles most of the necessary context-switching operations automatically:
\begin{itemize}
\item Manages ring permissions (automatically swapping code and stack segments).
\item Automatically swaps the stack from user stack to kernel stack (using the TSS).
\item Restores \textbf{RFLAGS} (which includes the \textbf{interrupt flag} responsible for enabling/disabling hardware interrupts).
\item Jumps back to the code address pointed to by \textbf{RIP}.
\end{itemize}

To switch contexts, we must ensure the current thread's state is preserved in a \textbf{Unified Interrupt Frame} (Figure \ref{fig:unifiedstackframe}) on its kernel stack. Additionally, a valid frame must exist for the target thread. If a thread has run previously, it will have saved its frame naturally during its last preemption. However, for a new thread that has never executed, this frame must be constructed manually. 

We assume that all threads begin execution in kernel space before eventually jumping to user space. Since all interrupt handling occurs within the kernel, the interrupt frame always resides on the kernel stack. Therefore, we can initialize the kernel stack of the new thread with a fabricated frame before the context switch. The initialization procedure is shown in Listing~\ref{lst:threadStack}:

\begin{cppcode}[caption={Thread stack initialization}, label={lst:threadStack}]
void InitializeThreadStack(void **stack, const Sched::Task &task)
{
    /* NOTE: Thread entry always starts in Kernel Code */
    auto stack_top = static_cast<byte *>(*stack) - sizeof(IsrErrorStackFrame);
    auto frame     = reinterpret_cast<IsrErrorStackFrame *>(stack_top);

    memset(stack_top, 0, sizeof(IsrErrorStackFrame));

    /* Initialize IsrErrorStackFrame (Hardware Part + Error Code) */
    frame->isr_stack_frame.rip    = reinterpret_cast<u64>(task.func);
    frame->error_code             = 0; // Dummy error code
    frame->isr_stack_frame.cs     = static_cast<u64>(cpu::GDT::kKernelCodeSelector);
    frame->isr_stack_frame.rflags = kInitialRFlags;
    frame->isr_stack_frame.rsp    = reinterpret_cast<u64>(*stack);
    frame->isr_stack_frame.ss     = static_cast<u64>(cpu::GDT::kKernelDataSelector);

    /* Initialize function arguments (Software Part / Registers) */
    if (task.args_count > 0) {
        frame->registers.rdi = task.args[0];
    }

    // ... other args

    if (task.args_count > 5) {
        frame->registers.r9 = task.args[5];
    }

    /* Save adjusted stack address */
    *stack = reinterpret_cast<void *>(stack_top);
}
\end{cppcode}

With both stack frames prepared, the context switch logic is straightforward: swap the \textbf{RSP} (current stack pointer) to the kernel stack of the target thread and execute the \textbf{IRETQ} instruction. This instruction restores the state and resumes execution, as demonstrated in the \textbf{context\_switch\_if\_needed} macro (Listing \ref{lst:isr_asm}).

\subsection{Jumping to User Space}

Transitioning to user space is performed similarly to a thread context switch. We construct an artificial interrupt frame, but in this case, the code segment and stack segment within the frame are set to the user space selectors, and the stack pointer is set to the user space stack. The C++ implementation is shown in Listing~\ref{lst:userSpaceJump}, and the corresponding NASM code is shown in Listing~\ref{lst:userSpaceJumpNasm}:

\begin{cppcode}[caption={User Space Jump C++ code}, label={lst:userSpaceJump}]
extern "C" void cdecl_JumpToUserSpaceEntry(void *addr, IsrStackFrame *frame)
{
    ASSERT_NOT_NULL(addr);
    ASSERT_NOT_NULL(frame);
    ASSERT_NOT_NULL(hardware::GetCoreLocalTcb());

    auto thread          = hardware::GetCoreLocalTcb();
    thread->kernel_stack = thread->kernel_stack_bottom; // reset kernel stack

    frame->rip    = reinterpret_cast<u64>(addr);
    frame->cs     = static_cast<u64>(cpu::GDT::kUserCodeSelector);
    frame->rflags = static_cast<u64>(kInitialRFlags);
    frame->rsp    = reinterpret_cast<u64>(thread->user_stack_bottom);
    frame->ss     = static_cast<u64>(cpu::GDT::kUserDataSelector);

    const u64 t            = TimingModule::Get().GetSystemTime().ReadLifeTimeNs();
    thread->kernel_time_ns = t - thread->timestamp;
    thread->timestamp      = t;

    SetThreadGs(thread);
    __asm__ volatile("swapgs" ::: "memory");
}
\end{cppcode}

\begin{nasmcode}[caption={User Space Jump NASM code}, label={lst:userSpaceJumpNasm}]
; c_decl
; void JumpToUserSpace(void (*func)(), void* arg)
;   RDI = func
;   RSI = arg
; Note: Caller is responsible for ensuring proper environment before calling (disabling IRQs)
JumpToUserSpace:
    sub rsp, _jump_userspace_stack_space
    push rsi
    ; aligned properly

    mov rsi, rsp
    add rsi, 8
    call cdecl_JumpToUserSpaceEntry

    xor rax, rax
    mov rax, _user_data_selector
    mov ds, ax
    mov es, ax
    mov fs, ax
    mov gs, ax

    pop rsi
    mov rdi, rsi ; prepare void* arg for func if needed
    iretq
\end{nasmcode}

\subsection{Interrupts Hardware Abstraction}

To maintain architectural independence, the Interrupt Table is abstracted. Upon any interrupt, control is passed to the \textbf{Logical Interrupt Table (LIT)}, which is responsible for executing the necessary actions common to all interrupts. The architecture-specific code establishes the mapping between the hardware interrupts and the logical interrupt table, from that point forward, interrupt management is handled entirely by the LIT. Responsibilities of the LIT include:

\begin{itemize}
\item Managing interrupt handlers, allowing the kernel to modify interrupt responses dynamically.
\item Tracking interrupt nesting levels.
\item Collecting statistics, such as interrupt counts per thread, kernel time, and user space time.
\item Interacting with hardware interrupt drivers such as the \textbf{PIC} or \textbf{APIC}.
\item Masking (blocking) individual interrupts.
\end{itemize}

The LIT handling procedures may return a pointer to the next thread scheduled for execution. If such a pointer is returned, the hardware wrapper performs the context switch upon exit. This mechanism enables the system to react rapidly to state changes or preempt the current thread via a timer interrupt.
\section{Timing}
\label{sec:timing}

Prior to the implementation of the Scheduler, and alongside functional memory management and interrupt handling, it was necessary to establish the infrastructure and drivers for timing mechanisms. This includes devices capable of measuring the system's uptime, referred to as \textbf{Clocks}. Additionally, the system requires devices capable of generating interrupts at specific intervals. These are essential for preempting the currently executing thread if it fails to yield the CPU voluntarily, thereby ensuring fair scheduling. Such devices are referred to as \textbf{Event Clocks}. To unify the timing subsystem, all devices and mechanisms rely on nanoseconds as the fundamental unit of time abstraction.

\subsection{Infrastructure}

The architecture-specific code is responsible for detecting available hardware and registering it within the central timing infrastructure, specifically the \textbf{ClockRegistry} and \textbf{EventClockRegistry}. Subsequently, architecture-agnostic components, such as the \textbf{ACPI} subsystem, may register additional clocks by parsing system description tables. Finally, the architecture-defined functions \texttt{hal::PickSystemClockSource()} and \texttt{hal::PickSystemEventClockSource()} are invoked during the initialization of the timing module to select the optimal sources for each purpose. The infrastructure allows the scheduler to implement either a tick-based or a tickless strategy.

\subsection{Clocks}
\label{subsec:clocks}

Clock Drivers are defined by the structure shown in Listing~\ref{lst:clockDriver}:

\begin{cppcode}[caption={Clock Driver Structure}, label={lst:clockDriver}]
struct alignas(arch::kCacheLineSizeBytes) ClockRegistryEntry : data_structures::RegistryEntry {
  /* Clock numbers */
  u64 frequency_kHz; // Frequency in kHz
  u64 ns_uncertainty_margin_per_sec;  
    // Uncertainty margin in femtoseconds per second
  u64 clock_numerator; // For conversion to nanoseconds, this is the numerator
  u64 clock_denominator; // For conversion to nanoseconds, this is the denominator

  /* Callbacks */
  u64 (*read)(ClockRegistryEntry *);
  bool (*enable_device)(ClockRegistryEntry *);
  bool (*disable_device)(ClockRegistryEntry *);
  void (*stop_counter)(ClockRegistryEntry *);
  void (*resume_counter)(ClockRegistryEntry *);

  /* Own data */
  void *own_data;
};
\end{cppcode}

For the x86-64 architecture, clock selection is prioritized based on availability and precision in the following order: TSC > HPET > RTC > PIT. Currently, support is implemented only for the TSC and HPET drivers.

\subsubsection{TSC}
The Time Stamp Counter (TSC) is the optimal clock source due to its high precision and core-locality. Accessing the TSC involves reading a CPU register, which requires only a few cycles, unlike external clocks that may require hundreds. However, the TSC has historical limitations. On older processors, the counter's frequency was tied to the core frequency. Consequently, frequency scaling (throttling) caused the time measurement to drift, rendering it unreliable. Modern Intel processors introduced the Invariant TSC, which ensures a constant frequency regardless of the core's power state. Another limitation is that on certain CPUs, the TSC frequency is not explicitly known and must be measured against a known reference. For this purpose, the system utilizes the HPET, which serves as the minimal hardware requirement for reliable calibration (see \cite{IntelManual-TSC}).

\subsubsection{HPET}
The High Precision Event Timer (HPET) also offers high precision and is generally more stable than the TSC on older hardware. However, it is an external device mapped via Memory-Mapped I/O (MMIO). As a result, accessing the HPET is significantly slower than reading the TSC, potentially taking up to 1000 cycles per read. Due to this performance overhead, the HPET is designated as the secondary choice. It is primarily utilized for calibrating other clocks rather than for frequent timekeeping operations.

\subsection{Event Clocks}

Event Clock Drivers are defined by the structure shown in Listing~\ref{lst:eventClockDriver}:

\begin{cppcode}[caption={Event Clock Driver Structure}, label={lst:eventClockDriver}]
struct PACK EventClockFlags {
    bool IsCoreLocal : 1;
    u32 padding : 31;
};
static_assert(sizeof(EventClockFlags) == sizeof(u32));

enum class EventClockState : u8 {
    kDisabled = 0,  // Clock is disabled
    kPeriodic,      // Clock is in periodic mode
    kOneshot,       // Clock is in oneshot mode
    kOneshotIdle,   // Clock is in oneshot mode but no event is scheduled
    klast,
};

struct alignas(arch::kCacheLineSizeBytes) EventClockRegistryEntry : data_structures::RegistryEntry {
    /* Clock numbers */
    u64 min_next_event_time_ns;  // Minimum time for the next event in nanoseconds

    /* Clock specific data */
    EventClockFlags flags;     // Features of the event clock, e.g., core-local
    CoreMask supported_cores;  // Cores that support this event clock

    /* infra data */
    u64 next_event_time_ns;  // Time for the next event in nanoseconds
    EventClockState state;   // Current state of the clock

    /* Driver data */
    void *own_data;  // Pointer to the clock's own data, used for callback

    /* callbacks */
    struct callbacks {
        // Callback to set next event time
        u32 (*next_event)(EventClockRegistryEntry *, u64);  
        // Callback to set clock state
        u32 (*set_oneshot)(EventClockRegistryEntry *);
        // Callback to set clock state
        u32 (*set_periodic)(EventClockRegistryEntry *);     
        void (*on_entry)(EventClockRegistryEntry *);        // optional
        void (*on_exit)(EventClockRegistryEntry *);         // optional
    } cbs;
};
\end{cppcode}

For x86-64, the selection of event clocks is based on the following priority order: LAPIC Timer > HPET > PIT. Currently, only the LAPIC Timer is supported as it provides the core-local interrupt capabilities required for efficient scheduling.

\subsubsection{LAPIC Timer}
Similar to the TSC, the LAPIC Timer is local to the processor core, ensuring extremely low-latency access to its registers. A significant advantage of this architecture is that each core possesses an independent timer, which eliminates the need for shared resource management or synchronization between cores. However, like the TSC, the LAPIC Timer operates at a frequency derived from the CPU bus or core frequency, which is not standard across various CPUs. Consequently, it requires calibration against a reference clock, for which the HPET is utilized.

% ==================================================================

\chapter{High-Level Subsystems}
\label{chap:high_level_subsystems}

This chapter describes the high-level subsystems of AlkOS that build upon the low-level kernel infrastructure presented in the chapter \ref{chap:low_level_implementation}. These components are the file system and the scheduler. 

\section{File System}
\label{section:fs}

The file system is a fundamental part of any operating system, providing mechanisms to store, access, and manage data on storage devices. We have designed a unified abstraction layer known as the Virtual File System (VFS). The VFS abstracts the underlying implementation details of specific file systems, providing the kernel and user space with an API for operations such as reading, writing, creating, moving, and deleting files and directories.

\subsection{VFS}

The VFS enables the operating system to interact with various file system types (e.g., FAT12, FAT16, FAT32) through a uniform interface, eliminating the need for the kernel to understand the specific implementation details. This modularity is achieved through a combination of compile-time enforced interfaces (Concepts) and a runtime dispatch mechanism.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=1.5cm,
      block/.style={
        rectangle, draw, fill=gray!10,
        text width=5cm, text centered,
        rounded corners, minimum height=1cm
      },
      component/.style={
        rectangle, draw, fill=gray!10,
        text width=5cm, text centered,
        minimum height=1cm
      },
      interface/.style={
        rectangle, draw, dashed, fill=gray!10,
        text width=5cm, text centered,
        minimum height=1cm
      },
      storage/.style={
        cylinder, draw, fill=gray!10,
        shape aspect=0.5,
        text width=4.5cm, text centered,
        minimum height=1cm
      },
      arrow/.style={-Latex, thick}
  ]

  \node (user_process) [block] {User Process};
  \node (fd_manager) [block, below=of user_process] {File Descriptor Manager};
  \node (vfs_module) [block, below=of fd_manager] {VFS Module};
  \node (fs_interface) [interface, below=of vfs_module] {File System Interface};
  \node (fs_driver) [component, below=of fs_interface] {File System Driver \\ (FAT12 / FAT16 / FAT32)};
  \node (vfs_io) [interface, below=of fs_driver] {VFS I/O Interface};
  \node (storage_device) [storage, below=of vfs_io] {Storage Device \\ (RAM, Disk, Network)};

  \draw [arrow] (user_process) -- (fd_manager) node [midway, right] {Syscalls};
  \draw [arrow] (fd_manager) -- (vfs_module) node [midway, right] {File Operations};
  \draw [arrow] (vfs_module) -- (fs_interface) node [midway, right] {Dispatch};
  \draw [arrow] (fs_interface) -- (fs_driver) node [midway, right] {Implements};
  \draw [arrow] (fs_driver) -- (vfs_io) node [midway, right] {Uses};
  \draw [arrow] (vfs_io) -- (storage_device) node [midway, right] {I/O Operations};

  \end{tikzpicture}
  \caption{High-level VFS Architecture}
  \label{fig:vfs_architecture}
\end{figure}

As illustrated in Figure \ref{fig:vfs_architecture}, the VFS architecture is composed of four primary layers:

\begin{itemize}
\item \textbf{VFS Module}: The central orchestrator for path resolution and operation delegation.
\item \textbf{File System Interface}: A struct containing function pointers that abstract specific driver operations.
\item \textbf{File System Driver}: The concrete implementation of a specific file system format.
\item \textbf{VFS I/O Interface}: An abstraction for block-level data access.
\end{itemize}

The following subsections describe each of these components in detail.

\subsubsection{VFS Module}

The VFS module serves as the entry point for all file operations and is responsible for managing file system mounts. It exposes the internal kernel API for operations such as opening, reading, and writing files. The module's primary responsibility is to translate these requests into calls to the appropriate file system instance based on the provided file path and the currently registered mount points.

When a VFS operation (e.g., \texttt{CreateFile}) is invoked, the module executes the following sequence:
\begin{enumerate}
    \item \textbf{Find Mount Point}: The system utilizes a \textbf{crit-bit tree} \cite{critbit} to efficiently locate the longest prefix match for the given path among all registered mount points. This step identifies the specific file system driver responsible for handling the operation.
    \item \textbf{Check Permissions}: The module validates the operation against the mount point options (e.g., ensuring write operations are not attempted on read-only mounts).
    \item \textbf{Path Translation}: The absolute system path is translated into a path relative to the mount point root.
    \item \textbf{Delegation}: The operation is delegated to the corresponding method of the specific file system driver.
\end{enumerate}
An example implementation is shown in Listing~\ref{lst:vfsModuleCreateFile}.

\begin{cppcode}[caption={VfsModule CreateFile Implementation}, label={lst:vfsModuleCreateFile}]
Result<> internal::VfsModule::CreateFile(const Path &path)
{
    auto mount_result = FindMountPoint(path);
    RET_UNEXPECTED_IF_ERR(mount_result);

    MountPoint *mount = mount_result.value();

    RET_UNEXPECTED_IF(mount->options.read_only, VfsError::kReadOnly);

    Path relative_path = GetRelativePath_(path, mount->path);
    return mount->fs.CreateFile(relative_path);
}
\end{cppcode}

\subsubsection{File System Interface}

The \texttt{vfs::Filesystem} struct acts as a uniform interface that all concrete file system drivers must expose (Listing~\ref{lst:vfsFilesystem}). It contains function pointers for various file and directory manipulations, alongside metadata about the file system. Each function pointer accepts a \texttt{void* ctx} argument, allowing the generic VFS layer to pass the concrete driver instance.

\begin{cppcode}[caption={vfs::Filesystem Structure}, label={lst:vfsFilesystem}]
struct Filesystem {
    struct Operations {
        // File operations
        Result<> (*create_file)(void *ctx, const Path &path);
        Result<size_t> (*read_file)(
            void *ctx, const Path &path, void *buffer, size_t size, size_t offset
        );
        Result<size_t> (*write_file)(
            void *ctx, const Path &path, const void *buffer, size_t size, size_t offset
        );
        Result<> (*delete_file)(void *ctx, const Path &path);
        // ... (additional operations omitted for brevity)
    };

    struct Info {
        Type type;
        const char *name;  // e.g., "FAT32", "FAT16"
    };
    
    Filesystem() = delete;
    explicit Filesystem(void *context, const Operations &operations, const Info &info)
    : context_(context), ops_(operations), info_(info) {}

private:
    void *context_;  // Pointer to the concrete driver instance
    Operations ops_;
    Info info_;
};
\end{cppcode}

\subsubsection{File System Driver}

Each file system driver implements the logic required for a specific file system type (e.g., FAT12, FAT16, FAT32). The driver interprets the raw data structures on the storage device and performs the requested operations. For instance, the FAT32 driver handles the manipulation of the File Allocation Table, directory entries, and cluster chains according to the FAT32 specification (for details, see \cite{fat-spec}).

To implement these drivers efficiently, we employ the Curiously Recurring Template Pattern (CRTP) \cite{crtp}. Each driver class inherits from a templated base class that provides common functionality, while the derived class implements format-specific details (Listing~\ref{lst:fatCrtp}). This approach enables static polymorphism, reducing the runtime overhead typically associated with virtual function calls.

\begin{cppcode}[caption={FAT Driver CRTP Base Class}, label={lst:fatCrtp}]
template <template <typename> typename T, typename IO>
class Fat
{
    using ImplT  = T<IO>;
    using Traits = FatTraits<T, IO>;

protected:
    // ... common FAT structures, validation, and operations

public:
    NODISCARD Filesystem GetFilesystem()
    {
        return Filesystem(
            this, // 'this' pointer is passed as the context
            Filesystem::Operations{
                .create_file = &Fat::CreateFileCallback_,
                // ...
            },
            Filesystem::Info{
                .type = ImplT::kFsType,
                .name = ImplT::kFsName,
            }
        );
    }

private:
    FAST_CALL Result<> CreateFileCallback_(void *ctx, const Path &path)
    {
        return static_cast<Fat *>(ctx)->CreateFile(path);
    }
    // ...
};
\end{cppcode}

Each \texttt{Fat} derivative implements its specific logic (e.g., Getting FAT entries differs between FAT12 and FAT32/16, as shown in Listing~\ref{lst:fat12GetFatEntry}) and provides a static \texttt{IsValid(IO \&io)} method to probe whether a given I/O device contains a valid instance of that file system.

\begin{cppcode}[caption={Fat12 GetFATEntry Implementation}, label={lst:fat12GetFatEntry}]
    NODISCARD FORCE_INLINE_F ClusterNumT GetFATEntry_(ClusterNumT cluster) const
    {
        ASSERT_LT(
            cluster, BaseT::cluster_count_ + BaseT::kFirstClusterNumber,
            "Cluster number out of range"
        );
        const size_t fat_offset = cluster + (cluster / 2);
        const size_t sector_number =
            BaseT::fat_region_.start + (fat_offset / boot_sector_.fat.bytes_per_sector);
        const size_t sector_offset = fat_offset % boot_sector_.fat.bytes_per_sector;

        // Load 2 sectors if entry spans two sectors
        size_t count =
            (sector_offset == static_cast<size_t>(boot_sector_.fat.bytes_per_sector - 1)) ? 2 : 1;
        auto range = BaseT::io_.ReadRange({sector_number, count});
        if ((cluster % 2) == 0) {  // Even cluster
            return internal::get<ClusterNumT>(range, sector_offset) & kClusterMask;
        } else {
            return internal::get<ClusterNumT>(range, sector_offset) >> 4;
        }
    }
\end{cppcode}

\subsubsection{VFS Interfaces and Concepts}

To enforce architectural compliance at compile time, we utilize C++20 Concepts to define strict contracts for both file system drivers and low-level storage operations.

The \texttt{VFSInterface} concept (Listing \ref{lst:vfsInterfaceConcept}) mandates that any compliant driver class must provide a constructor accepting an I/O backend, a static method to validate the file system signature on the storage medium, and a method to retrieve the runtime function table.

\begin{cppcode}[caption={VFSInterface Concept Definition}, label={lst:vfsInterfaceConcept}]
template <template <typename> typename T, typename IO>
concept VFSInterface = VFSIO<IO> and requires(T<IO> fs, IO io) {
    T<IO>(io);
    { T<IO>::IsValid(io) } -> std::same_as<bool>;
    { fs.GetFilesystem() } -> std::same_as<Filesystem>;
};
\end{cppcode}

The VFS I/O interface abstracts low-level data access. The \texttt{VFSIO} concept (Listing \ref{lst:vfsioConcept}) establishes a contract for block-based input and output. By adhering to this concept, file system drivers remain decoupled from the underlying hardware, enabling seamless interaction with diverse storage backends --- such as RAM disks, physical partitions, or network storage --- without requiring implementation changes.

\begin{cppcode}[caption={VFSIO Concept Definition}, label={lst:vfsioConcept}]
template <typename IO>
concept VFSIO =
    requires(IO io, size_t offset, io::SectorRange range, std::span<const byte> data, size_t size) {
        { io.ReadRange(range) } -> std::same_as<std::span<byte>>;
        { io.ReadSector(offset) } -> std::same_as<std::span<byte>>;
        { io.WriteRange(range, data) } -> std::same_as<void>;
        { io.WriteSector(offset, data) } -> std::same_as<void>;
        { io.GetSectorSize() } -> std::same_as<size_t>;
    };
\end{cppcode}

\subsection{File Descriptors}

The file descriptor system provides a hierarchical structure to handle file I/O operations initiated by user processes. This design tracks open files, maintains current read/write offsets, and enforces access rights.

\begin{figure}[htbp]
  \centering
  \includesvg[width=0.9\textwidth]{res/fd-diagram.svg}
  \caption{File Descriptor Design}
  \label{fig:fd_architecture}
\end{figure}

As depicted in Figure \ref{fig:fd_architecture}, the system comprises three main tables:

\subsubsection{FdTable (Per-Process)}
Each process maintains its own \texttt{FdTable}, which is an array mapping integer file descriptors to \texttt{RefPtr<OpenFileEntry>} objects (Listing~\ref{lst:fdTableAllocate}).

\begin{cppcode}[caption={FdTable Allocation}, label={lst:fdTableAllocate}]
FdResult<fd_t> FdTable::Allocate(data_structures::RefPtr<OpenFileEntry> global_entry)
{
    std::lock_guard lock(lock_);
    for (size_t i = 0; i < kMaxFdsPerProcess; ++i) {
        if (entries_[i] == nullptr) {
            entries_[i] = std::move(global_entry);
            ++count_;
            return static_cast<fd_t>(i);
        }
    }
    return std::unexpected(FdError::kFdTableFull);
}
\end{cppcode}

\subsubsection{OpenFileTable (Global)}

The \texttt{OpenFileTable} is a system-wide pool of \texttt{OpenFileEntry} objects. Each entry represents a unique "open" instance of a resource (file or pipe), maintaining dynamic state such as the current read/write offset, access flags (e.g., Read/Write), and a handle to the underlying resource. Multiple file descriptors from different processes can point to the same \texttt{OpenFileEntry}, allowing them to share the cursor position (Listing~\ref{lst:openFileTableOpenFile}).

\begin{cppcode}[caption={OpenFileTable Creation}, label={lst:openFileTableOpenFile}]
FdResult<data_structures::RefPtr<OpenFileEntry>> OpenFileTable::OpenFile(File *file, OpenMode flags)
{
    RET_UNEXPECTED_IF(file == nullptr, FdError::kInvalidArgument);

    std::lock_guard lock(lock_);

    const size_t idx = entries_.Allocate();
    RET_UNEXPECTED_IF(idx == std::numeric_limits<size_t>::max(), FdError::kIoError);

    OpenFileEntry *entry = entries_.Get(idx);
    ASSERT_NOT_NULL(entry);

    new (entry) OpenFileEntry();
    entry->pool_idx_ = idx;

    entry->handle    = FileHandle::Wrap(file);
    entry->flags     = static_cast<u32>(flags);
    entry->offset    = 0;
    entry->is_append = HasMode(flags, OpenMode::kAppend);
    ++count_;

    return data_structures::RefPtr(entry);
}
\end{cppcode}

The \texttt{FileHandle} utilizes a \texttt{NonOwningTaggedPtr} to hold either a pointer to a \texttt{File} object or a \texttt{Pipe} instance, enabling the \texttt{OpenFileEntry} to manage both cases seamlessly.

\subsubsection{FileTable (Global)}
The \texttt{FileTable} manages unique \texttt{File} objects system-wide (Listing~\ref{lst:fileTableGetOrCreate}). Each \texttt{File} object acts as the in-memory representation of a physical file on the file system, storing static attributes such as the path and file size. This ensures that a file on the VFS has a single representation in memory, regardless of how many times it has been opened.

\begin{cppcode}[caption={FileTable GetOrCreate}, label={lst:fileTableGetOrCreate}]
FdResult<data_structures::RefPtr<File>> FileTable::GetOrCreate(const vfs::Path &path)
{
    RET_UNEXPECTED_IF(path.IsEmpty(), FdError::kInvalidArgument);

    File *existing = Find(path);
    if (existing != nullptr) {
        return data_structures::RefPtr(existing, false);
    }

    const size_t idx = files_.Allocate();
    RET_UNEXPECTED_IF(idx == std::numeric_limits<size_t>::max(), FdError::kIoError);

    File *file = files_.Get(idx);
    ASSERT_NOT_NULL(file);

    new (file) File();
    file->pool_idx_ = idx;

    file->size = 0;
    file->mode = 0;
    file->path = path;
    ++count_;

    return data_structures::RefPtr(file);
}
\end{cppcode}

\subsubsection{FdManager}
The \texttt{FdManager} integrates these three tables and exposes file descriptor operations (e.g., \texttt{Open}, \texttt{Close}, \texttt{Read}). When a process requests to open a file, the \texttt{FdManager} coordinates the interaction: it validates existence via the VFS module, retrieves the \texttt{File} object, creates a new entry in the \texttt{OpenFileTable}, and finally assigns a file descriptor in the process's \texttt{FdTable} (Listing~\ref{lst:fdManagerOpen}).

\begin{cppcode}[caption={FdManager Open Implementation}, label={lst:fdManagerOpen}]
FdResult<fd_t> FdManager::Open(const vfs::Path &path, OpenMode flags)
{
    // Check if file exists in VFS
    auto exists_result = VfsModule::Get().FileExists(path);
    RET_UNEXPECTED_IF(!exists_result, FdError::kIoError);
    RET_UNEXPECTED_IF(!*exists_result, FdError::kNotFound));

    auto file_result = file_table_.GetOrCreate(path);
    RET_UNEXPECTED_IF_ERR(file_result);

    auto open_result = open_file_table_.OpenFile(file_result->Get(), flags);
    RET_UNEXPECTED_IF_ERR(open_result);

    FdTable *fd_table = GetCurrentProcessFdTable();
    RET_UNEXPECTED_IF(fd_table == nullptr, FdError::kIoError);

    auto fd_result = fd_table->Allocate(std::move(*open_result));
    RET_UNEXPECTED_IF_ERR(fd_result);

    return *fd_result;
}
\end{cppcode}

\subsubsection{Resource Lifecycle Management}

To ensure efficient memory usage and prevent resource leaks, the life cycle of \texttt{OpenFileEntry} and \texttt{File} objects is managed through intrusive reference counting (Section~\ref{subsec:refcounting}). Both structures inherit from a \texttt{RefCounted} base class and are managed via smart pointers (\texttt{RefPtr} and \texttt{TaggedPointer}).

When a process closes a file descriptor (or terminates), the reference to the corresponding \texttt{OpenFileEntry} in the process's \texttt{FdTable} is released. If the reference count drops to zero --- indicating that no other file descriptors (across any process) refer to this open stream --- the \texttt{OpenFileEntry} is automatically deallocated and removed from the global \texttt{OpenFileTable}.

Consequently, the destruction of an \texttt{OpenFileEntry} releases its hold on the underlying \texttt{File} object. Similarly, if the reference count of the \texttt{File} object reaches zero, it implies that no active streams are reading from or writing to that specific file. The system then automatically reclaims the associated memory from the \texttt{FileTable}. This cascading deallocation mechanism ensures that kernel memory is only consumed for files that are actively in use by at least one process.

\section{Scheduling}
\label{sec:scheduling-impl}

The scheduling module is the final core component of the operating system, relying heavily on and utilizing the previously described modules. Although the current implementation targets a single-core architecture for simplicity, the design is extensible to multi-core systems, as discussed later in this document.

\subsection{Process Structure}

The process structure is defined in Listing~\ref{lst:processStruct}:

\begin{cppcode}[caption={Process Structure}, label={lst:processStruct}]
struct PACK Pid {
    u16 id;
    u64 count : 48;

    bool operator==(const Pid &other) const = default;
};

struct PACK ProcessFlags {
    bool KernelSpaceOnly : 1;
    bool PreserveFloats : 1;
};
static_assert(sizeof(ProcessFlags) == 1);

enum class ProcessState : u64 {
    kReady = 0,
    kWaitingForJoin,
    kTerminated,
    kLast,
};
static_assert(sizeof(ProcessState) == sizeof(u64));

struct Process : hal::Process {
    static constexpr size_t kMaxNameLength = vfs::kMaxComponentSize;

    /* Management */
    char name[kMaxNameLength];
    Pid pid;
    ProcessFlags flags;
    Thread *threads;
    u64 live_threads;
    u64 threads_to_clean;
    ProcessState state;
    WaitQueue<Thread, 3> *wait_queue;
    int status;

    /* Process resources */
    Mem::VPtr<Mem::AddressSpace> address_space;

    /* File descriptor table */
    Mem::VPtr<Fs::FdTable> fd_table;

    /* Standard I/O pipes (owned by process) */
    IO::Pipe<Fs::kStdioBufferSize> stdin_pipe;
    IO::Pipe<Fs::kStdioBufferSize> stdout_pipe;
    IO::Pipe<Fs::kStdioBufferSize> stderr_pipe;
};
\end{cppcode}

\subsubsection{PID}
\label{subsubsec:pid}

The system imposes a hard limit on the maximum number of existing processes, exactly 4096 processes. This constraint allows for optimized process lookup via direct indexing. The \textbf{PID} (Process Identifier) is composed of a reservable identifier and an atomic counter, ensuring that each PID remains unique throughout the entire lifetime of the system.

\subsubsection{Process Flags}

\textbf{KernelSpaceOnly} --- enables the creation of kernel-only threads and processes. These processes do not require a separate user-space address space or user-space stack. This minimization of resource consumption improves performance. Additionally, context switching between kernel-only threads does not necessitate switching the address space.

\textbf{PreserveFloats} --- indicates that all threads within the process save and restore floating-point registers during context switches by default. Disabling this flag allows the system to omit these potentially expensive operations for threads that do not utilize floating-point arithmetic.

\subsubsection{Process State}

The process state has less significance than the Thread State (described later) and is primarily utilized for resource management when waiting for a process to terminate. It ensures that process resources are not deallocated more than once.

\subsubsection{Address Space}

The address space is a fundamental component of the process structure. It defines all information regarding virtual memory, specifically establishing the translation between the virtual and physical layers. Each process operates within its own virtual address space, facilitating simpler memory management regarding fragmentation, performance, and security. Refer to Section~\ref{sec:impl_vmm} for more information.

\subsubsection{File Descriptor Table}

This table contains all file descriptors opened by threads executing within the process. Refer to the File system section \ref{section:fs} for more information.

\subsubsection{Pipes}

Pipes handle the buffered I/O of the process, primarily used for standard input and output communication with the user.

\subsubsection{Name}

The name of the process typically corresponds to the name of the executable. Unlike the unique PID, the process name is not required to be unique.

\subsubsection{Threads}

This field points to the first element of a doubly linked list of threads running in this environment. It is used primarily for operations such as termination (kill/exit).

\subsubsection{Live Threads and Threads to Clean}

These counters are used for synchronization with the \textbf{ProcessRipper}. Refer to Kernel Workers \ref{subsubsec:kworkers} for more details.
 
\subsubsection{Status}

The exit status of the process, which is passed to any thread waiting for the process to complete.

\subsubsection{Wait Queue}

The \textbf{Wait Queue} stores threads that are waiting for this process to finish. All threads in this queue are woken up upon process termination.

\subsection{Kernel Address Space}

The entire operating system shares a single Kernel Address Space. Every kernel process operates within this space. Additionally, the kernel address space is mapped into every user-space process's address space. This design choice enhances performance and simplifies architecture, as user threads frequently invoke system calls. Switching address spaces on every system call would incur significant performance overhead and complicate the logic required for handling syscalls.

\subsection{Thread Structure}

The thread structure is defined as follows:

\begin{cppcode}[caption={Thread Structure}, label={lst:threadStruct}]
enum class UserPriority : u8 { kLow = 0, kMediumLow, kMedium, kMediumHigh, kHigh, kLast };

struct PACK Tid {
    u16 id;
    u64 count : 48;

    bool operator==(const Tid &other) const = default;
};

struct PACK ThreadFlags {
    SchedulingPolicy policy : 8;
    u8 priority : 8;
    UserPriority user_priority : 3;
    bool preserve_floats : 1;
    bool detached : 1;
    u64 padding : 43;
};
static_assert(sizeof(ThreadFlags) == 8);

enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kSleeping,
    kBlockedOnWaitQueue,
    kWaitingForJoin,
    kTerminated,
    kLast,
};
static_assert(sizeof(ThreadState) == sizeof(u64));

static constexpr int kSchedulingIntrusiveLevel  = 0;
static constexpr int kSleepingIntrusiveLevel    = 1;
static constexpr int kProcessListIntrusiveLevel = 2;
static constexpr int kWaitQueueIntrusiveLevel   = 3;

struct Thread : data_structures::IntrusiveRbNode<Thread, u64, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveRbNode<Thread, u64, kSleepingIntrusiveLevel>,
                data_structures::IntrusiveListNode<Thread, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveListNode<Thread, kSleepingIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kWaitQueueIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kProcessListIntrusiveLevel> {
    /* Management */
    Tid tid;
    Pid owner;
    ThreadFlags flags;
    ThreadState state;
    void *retval;
    WaitQueue<Thread, kWaitQueueIntrusiveLevel> *wait_queue;

    /* Thread resources */
    void *kernel_stack;
    void *kernel_stack_bottom;
    void *user_stack;
    void *user_stack_bottom;

    /* Statistics */
    u64 kernel_time_ns;
    u64 user_time_ns;
    u64 timestamp;
    u64 timestamp_execution_start_ns;
    u64 num_interrupts;
    u64 num_syscalls;
    u64 num_context_switches;

    /* Arch */
    hal::Thread arch_data;

    NODISCARD u64 CalculateCpuTime();
};
\end{cppcode}

\subsubsection{TID}

Similar to the PID \ref{subsubsec:pid}, the system limits the number of available threads. This property is utilized to create unique \textbf{TIDs} (Thread Identifiers), allowing for rapid lookups.

\subsubsection{Owner's PID}

The PID of the process that owns the thread.

\subsubsection{Thread Flags}

A collection of bitfields that determine how the scheduler manages the thread:

\textbf{SchedulingPolicy policy} --- defines the scheduling policy under which the thread currently operates. Refer to Policies \ref{subsec:policies} for more details.

\textbf{u8 priority} --- defines the internal priority of the thread within its assigned policy. This value is managed by the kernel.

\textbf{UserPriority user\_priority} --- may be specified by User Space to inform the scheduler of the relative importance of threads.

\textbf{bool preserve\_floats} --- specifies whether the floating-point state is preserved during context switches.

\textbf{bool detached} --- specifies whether the scheduler should automatically clean up the thread upon termination, without waiting for a join operation.

\subsubsection{Thread State}

Describes the current status of the thread. This state tracking allows the kernel to monitor each thread individually and verify system integrity, as only specific state transitions are permitted. The thread state transitions are illustrated in Figure~\ref{fig:thread-states}.

\newgeometry{top=1cm, bottom=1.5cm, left=1cm, right=1cm} 

\begin{figure}[p]
    \centering
    \includesvg[height=0.95\textheight, width=\textwidth, keepaspectratio]{res/thread-states.svg}
    \caption{Thread State Transitions Graph}
    \label{fig:thread-states}
\end{figure}

\restoregeometry

\subsubsection{Stacks}

Two distinct stacks are required for each thread: one for user space and one for kernel space. A separate kernel stack is essential for security and stability. Furthermore, sharing a single kernel stack is inefficient because context switches may occur within kernel code, such as during thread joining or other synchronization events.

\subsubsection{Statistics}

Statistics tracking allows for the analysis of execution specifics for each thread. This data is used to dynamically adjust the scheduling module and to debug or test the scheduler.

\subsubsection{Wait Queue}

Allows other threads to block and wait for a specific thread to finish execution (Thread joining).

\subsection{Kernel Workers}
\label{subsubsec:kworkers}

Before initializing the scheduling module, the operating system creates three essential kernel workers:

\textbf{Trace Dumper} --- responsible for dumping kernel traces to the terminal or storage files, ensuring debuggability and stability. This task cannot be performed directly by the kernel code during critical execution paths, as writing to devices or files is too slow for system calls or interrupt handlers. The buffers are dumped periodically every 20ms.

\textbf{Thread Ripper} --- responsible for the deallocation of thread resources, including descriptors and stacks.

\textbf{Process Ripper} --- responsible for the deallocation of process resources, including the address space.

\subsection{Intrusive Data Structures}

Since threads frequently migrate between different linked lists and data structures, allocating and freeing list nodes for every operation introduces significant overhead. To address this, the system employs intrusive data structures \ref{subsec:intrusive}. Instead of allocating a node that contains a pointer to the object, the node structure is embedded directly within the object itself. Operations on the lists are performed by simply modifying the fields of the object.

\begin{cppcode}[caption={Intrusive Nodes}, label={lst:intrusiveNodes}]
template <class T, int kIntrusiveLevel>
struct IntrusiveListNode {
    T *next;
};

template <class T, int kIntrusiveLevel>
struct IntrusiveDoubleListNode {
    T *next;
    T *prev;
};

template <class T, class KeyT, int kIntrusiveLevel>
struct IntrusiveRbNode {
    enum class Color : u8 {
        kBlack = 0,
        kRed   = 1,
    };

    T *parent;
    union {
        struct {
            T *left;
            T *right;
        };
        T *child[2];
    };

    Color color;
    KeyT key;
};
\end{cppcode}

\subsection{Meta-Scheduler}

The scheduler architecture follows the \textbf{meta-scheduler} design pattern, which is common in modern operating systems. This approach relies on abstracting scheduling logic into Policies. Policies are primarily responsible for selecting the next task from the set of tasks they manage and reacting to thread behavior (e.g., punishing or rewarding threads with CPU time based on workload characteristics). The scheduler itself is responsible for all other operations, including:
\begin{itemize}
    \item Managing thread states.
    \item Transitioning threads to sleep.
    \item Waking up threads.
    \item Managing idle time.
    \item Configuring next timing events via the timing infrastructure.
    \item Blocking threads on wait queues.
    \item Releasing threads from wait queues.
\end{itemize}
A hierarchy exists between policies, ensuring that threads from higher-priority policies are selected before threads from lower-priority ones.

\subsection{Timing Model}

The system implements a tickless kernel architecture. Instead of relying on a periodic interrupt (the \textbf{Kernel Tick}) at a fixed frequency, the scheduler calculates precisely when the next timing event must occur. This approach improves efficiency and precision.

\subsection{Policies}

The following scheduling policies have been implemented:

\textbf{Round Robin Scheduling Policy} --- simple policy that iterates through a linked list from front to back. Threads are ordered based on their arrival time in the policy.

\textbf{Priority Queue Scheduling Policy} --- policy that orders threads based on kernel-assigned priorities. Priorities are capped at a range of 0-64 to enable the use of an \textbf{O(1)} priority queue, known as a \textbf{Bitmap Priority Queue}. This structure maintains an array of linked lists representing individual priorities. Additionally, a bitmask indicates the presence of tasks at specific priority levels. Lookup operations utilize bitwise instructions (counting leading/trailing zeros) to efficiently find the minimum or maximum priority value (\textbf{O(1)}).

\textbf{Multi-Level Feedback Queue (MLFQ) Scheduling Policy} --- policy that segregates threads into distinct queues according to their workload characteristics. The policy penalizes CPU-bound threads by demoting them to lower priorities, whereas I/O-bound threads are elevated to higher priorities, thereby favoring interactive performance. To prevent the starvation of low-priority tasks, a periodic reset mechanism promotes all threads to the highest priority queue every 100ms. Within each queue, threads are managed using \textbf{Red-Black trees}, sorted by a key that combines user-defined priority and aggregated CPU burst times (the amount of time a thread spends executing on the CPU before it either completes, requires I/O operations, or is interrupted by the operating system). This ensures that each thread in the given queue receives a fair share of CPU time.

\noindent These policies establish a hierarchy defined by the enumeration shown in Listing~\ref{lst:policyHierarchy}:

\begin{cppcode}[caption={Scheduling Policy Hierarchy}, label={lst:policyHierarchy}]
// PO > P1 > .. > P4
enum SchedulingPolicy {
    kUberTask_PQ_P0 = 0,
    kDrivers_PQ_P1,
    kUrgentTasks_PQ_P2,
    kNormalTasks_MLFQ_P3,
    kBackgroundTasks_RR_P4,
    kLast,
};
\end{cppcode}

Each policy is designated for a specific group of tasks:

\textbf{Uber Tasks} --- Tasks that must be executed as quickly as possible, such as emergency recovery actions or critical kernel state preservation (e.g., saving state before shutdown or terminating processes during out-of-memory conditions).

\textbf{Drivers} --- Driver tasks that require immediate execution to prevent device blocking and ensure smooth system operation (e.g., audio drivers or network interfaces).

\textbf{Urgent Tasks} --- Tasks that are less critical than drivers but more important than standard user tasks. These may include privileged user-space tasks that consume data from drivers.

\textbf{Normal Tasks} --- The most common group of tasks, comprising standard user-space programs such as graphical interfaces and data processing applications. Basic kernel workers, including those listed in the kernel workers section \ref{subsubsec:kworkers}, also belong to this category.

\textbf{Background Tasks} --- Tasks that are not time-critical and should only be executed when the system is otherwise idle. Examples include update checks or non-urgent cleanup operations.

\subsubsection{Policies Abstraction}
\label{subsec:policies}

The policy abstraction is shown in Listing~\ref{lst:policyStruct}:

\begin{cppcode}[caption={Policy Abstraction}, label={lst:policyStruct}]
struct Policy {
    struct {
        Thread *(*pick_next_task)(void *);

        void (*add_task)(void *, Thread *);
        void (*remove_task)(void *, Thread *);

        u64 (*get_preempt_time)(void *, Thread *);
        bool (*is_first_higher_priority)(void *, Thread *, Thread *);
        bool (*validate_flags)(void *, const ThreadFlags *);

        void (*on_thread_yield)(void *, Thread *);
        void (*on_periodic_update)(void *, u64 current_time_ns);
    } cbs;
    void *self;
};
\end{cppcode}

As shown, the primary operation is \textbf{pick\_next\_task}, alongside standard \textbf{add\_task} and \textbf{remove\_task} operations. Additionally, helper functions are provided for the scheduler and for statistics gathering.

\subsection{Sleeping}

To provide high-precision sleeping, the system relies on one-shot, precise timing events. Upon receiving a timer interrupt, the scheduler determines the timestamp for the next event based on the system state, which includes the current thread preemption time and the wake-up times of sleeping threads. Consequently, the scheduler must inspect the sleeping queue to process expired events. To ensure high performance and stable time complexity, an Intrusive Red-Black Tree was selected as the underlying data structure for the \textbf{Sleeping Queue}. The critical operations are \textbf{Insert}, \textbf{ExtractMin}, and \textbf{Remove}, all of which must have predictable execution times. The requirement for efficient arbitrary removal excluded binary heaps, while the need for stability excluded heaps with amortized complexity bounds. The Red-Black Tree satisfied all requirements and was already implemented for other system components, making it the optimal choice.

% ==================================================================

\chapter{User's Manual}

This section describes the usage of the AlkOS build environment and documents the public interfaces available to user space applications. It outlines the build and execution workflow, the integration of user programs, and the system call interface provided by the operating system.

\section{Building and Running the OS}

AlkOS provides a command-line utility script, \texttt{alkos\_cli.bash}, located in the \texttt{scripts/} directory. This script automates the setup of the development environment, the compilation of the kernel, and the execution of the operating system within an emulator. The build infrastructure is based on CMake and requires a Linux host system.

Automated dependency installation is officially supported for Arch Linux and Ubuntu distributions. Nevertheless, the build process is expected to function on other Linux distributions, provided that the required dependencies are installed manually. Reference installation dependencies for supported environments can be found in the \texttt{scripts/env/} directory.

The build and execution workflow consists of three sequential stages:

\begin{enumerate}
    \item \textbf{Environment Initialization:}
    Prior to compilation, a custom cross-compilation toolchain and all required system dependencies must be installed. This can be achieved by executing:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --install all
    \end{verbatim}
    Alternatively, to install only the cross-compiler toolchain, the following command may be used:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --install toolchain
    \end{verbatim}

    \item \textbf{Configuration:}
    During the configuration phase, build configuration and kernel feature flags are generated and stored in the \texttt{config/} directory. The default configuration can be created using:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --configure
    \end{verbatim}
    For customized configurations, the following script is provided:
    \begin{verbatim}
    ./scripts/config/configure.bash <platform> <build_type> [options...]
    \end{verbatim}
    Here, \texttt{<platform>} denotes the target architecture (e.g., \texttt{x86-64}), \texttt{<build\_type>} specifies either \texttt{Debug} or \texttt{Release}. A complete list of options can be obtained using the \texttt{\text{-}\text{-}help} flag.

    \item \textbf{Compilation and Execution:}
    In the final stage, the kernel and all registered user space applications are compiled, the root file system image and bootable ISO are generated, and the operating system is launched using the QEMU emulator. This process can be initiated with the command:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --run
    \end{verbatim}
\end{enumerate}

In the event of errors occurring at any stage of the process, the scripts may be executed with the \texttt{\text{-}\text{-}verbose} flag, which enables detailed logging output to facilitate debugging.

\section{Developing User Space Applications}

AlkOS supports user space applications written in C and C++. These applications are integrated into the operating system at build time and included directly in the generated root file system.

\subsection{Project Structure}

User space programs reside in the \texttt{userspace/programs/} directory. Each application is placed in its own subdirectory and must include all relevant source files along with a \texttt{CMakeLists.txt} configuration file. A minimal example of a user application is shown in Listing \ref{lst:minimalUserAppStructure}.

\begin{cppcode}[caption={Minimal User Application Structure}, label={lst:minimalUserAppStructure}]
// userspace/programs/my_app/main.cpp
#include <stdio.h>

extern "C" int main() {
    printf("Hello from AlkOS User Space!\n");
    return 0;
}
\end{cppcode}

\subsection{Build System Integration}

To include a user space application in the build process, it must be registered with the CMake-based build system. AlkOS provides the helper macro \texttt{alkos\_register\_userspace\_app}, which automatically configures compiler flags and links the application against the custom C and C++ standard libraries (\texttt{libc} and \texttt{libc++}). An example registration is shown in Listing \ref{lst:userAppCMakeRegistration}.

\begin{cmakecode}[caption={User Application Registration}, label={lst:userAppCMakeRegistration}]
# userspace/programs/my_app/CMakeLists.txt
alkos_find_sources(MY_APP_SOURCES)
alkos_register_userspace_app(my_app "${MY_APP_SOURCES}")
\end{cmakecode}

After registration, the application is automatically compiled, linked, and placed in the \texttt{/bin} directory of the root file system during the next build.

\section{C Standard Library}

Instead of relying on an external standard library implementation, AlkOS provides a custom C standard library tailored to its kernel interface.

\subsection{Standard C Support}

The library includes implementations of commonly used standard headers such as \texttt{<stdio.h>}, \texttt{<stdlib.h>}, and \texttt{<string.h>}. This enables the compilation of conventional C programs with minimal adaptation. Standard file operations, including \texttt{fopen}, \texttt{fread}, and \texttt{fwrite}, are fully supported and internally routed through the VFS.

\subsection{AlkOS-Specific Extensions}

Certain low-level system interactions are beyond the scope of the standard C library. To expose kernel-specific functionality, AlkOS provides additional interfaces via the \texttt{<alkos/calls.h>} header. These interfaces enable access to hardware abstraction, threading primitives, and system control mechanisms. The most important extensions include:

\begin{itemize}
    \item \textbf{Input and Output Subsystem (\texttt{alkos/sys/video.h}, \texttt{alkos/sys/input.h}):}
    The \texttt{GetVideoBufferInfo} function maps the framebuffer into the process address space, allowing direct graphical output. Keyboard input is accessed via \texttt{GetKeyState}, which enables polling of individual keys.

    \item \textbf{Process Management, Threading, and Timing (\texttt{alkos/sys/thread.h}, \texttt{alkos/sys/time.h}):}
    Thread lifecycle management is provided through \texttt{ThreadCreate}, \texttt{ThreadJoin}, and \texttt{ThreadDetach}. High-precision timing services are available via \texttt{NanoSleep} and \texttt{NanoSleepUntil}. Process-level control is facilitated by \texttt{Exec} and \texttt{ProcExit}.

    \item \textbf{Power Management (\texttt{alkos/sys/power.h}, \texttt{alkos/sys/proc.h}):}
    System power states can be controlled using the \texttt{Shutdown} and \texttt{Reboot} interfaces, which invoke the underlying ACPI mechanisms.

    \item \textbf{Extended File System Operations (\texttt{alkos/sys/fs/fs.h}):}
    While standard I/O functions support file access, directory enumeration, and metadata queries require OS-specific calls. The \texttt{ReadDirectory} function enables directory traversal, and \texttt{FileInfo} provides information about file system objects.
\end{itemize}

\section{Syscalls}

The system call interface represents the controlled boundary between user space applications executing in Ring 3 and the kernel executing in Ring 0. On the \texttt{x86-64} architecture, AlkOS utilizes the \texttt{int 0x80} software interrupt mechanism to invoke system calls.

\subsection{Calling Convention}

The calling convention is largely inspired by the System V x86-64 ABI, with modifications to accommodate system call semantics. The register usage is summarized in Table \ref{tab:syscall_abi}.

\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Register} & \textbf{Purpose} \\ \hline
\texttt{RAX} & System call number (input) / return value (output) \\ \hline
\texttt{RDI} & Argument 1 \\ \hline
\texttt{RSI} & Argument 2 \\ \hline
\texttt{RDX} & Argument 3 \\ \hline
\texttt{R10} & Argument 4 \\ \hline
\texttt{R8} & Argument 5 \\ \hline
\texttt{R9} & Argument 6 \\ \hline
\end{tabular}
\caption{System Call Register Mapping on x86-64}
\label{tab:syscall_abi}
\end{table}

\subsection{Provided System Services}

The available system calls are grouped into logical categories:

\paragraph{Process and Thread Management}
\begin{itemize}
    \item \texttt{exec(path)}: Loads and executes a program.
    \item \texttt{proc\_exit(status)} and \texttt{proc\_abort()}: Terminates the calling process.
    \item \texttt{kill(pid)} and \texttt{wait(pid)}: Manage inter-process control and synchronization.
    \item \texttt{thread\_create}, \texttt{thread\_exit}, \texttt{thread\_join}, \texttt{thread\_detach}: Thread management primitives.
    \item \texttt{get\_heap\_start()}: Returns the base address of the process heap.
\end{itemize}

\paragraph{File I/O and File System Management}
\begin{itemize}
    \item \texttt{open}, \texttt{close}, \texttt{read}, \texttt{write}, \texttt{seek}: File descriptor operations.
    \item \texttt{dup(fd)} and \texttt{dup\_to(fd, newfd)}: File descriptor duplication.
    \item \texttt{read\_directory} and \texttt{file\_info}: File system inspection.
    \item \texttt{create\_directory}, \texttt{delete\_file}, \texttt{move\_file}: File system modification operations.
\end{itemize}

\paragraph{Graphics and Input}
\begin{itemize}
    \item \texttt{create\_graphic\_session(info)}: Initializes a graphical context.
    \item \texttt{blit()}: Requests composition of the application backbuffer.
    \item \texttt{get\_key\_state(vk)}: Retrieves the state of a virtual key.
\end{itemize}

\paragraph{Time and Synchronization}
\begin{itemize}
    \item \texttt{nanosleep(ns)} and \texttt{nanosleep\_until(sys\_ns)}: Suspends execution for a defined duration.
    \item \texttt{get\_clock\_value}: Returns the current value of a specified clock.
    \item \texttt{get\_timezone}: Retrieves the system time zone configuration.
\end{itemize}

\paragraph{System Control and Debugging}
\begin{itemize}
    \item \texttt{power(action)}: Controls system power states.
    \item \texttt{panic(msg)}: Terminates execution and records a diagnostic message.
\end{itemize}


\section{Example Programs}
\label{sec::example_programs}

To validate the functionality of the kernel and its subsystems, a set of user space applications was developed. These programs serve as integration tests, verifying the interactions between the system call interface, the C standard library, drivers, and the kernel core. Each program targets specific subsystems and features of the operating system.

\subsection{Hello World}

The \texttt{hello\_world} program serves as the verification of the system call interface, standard I/O operations, and file system functionality.

\subsubsection{System Call Interface Verification}

The application calls \texttt{printf}, which internally buffers data and invokes the \texttt{write} system call. The successful appearance of text on the debug terminal confirms that we successfully transitioned from user space to kernel space and back, validating that the syscall dispatcher correctly routes requests to the appropriate kernel handlers (Listing~\ref{lst:hello_stdout}).

\begin{cppcode}[caption={Hello World Standard Output Test}, label={lst:hello_stdout}]
// userspace/programs/hello_world/main.cpp

extern "C" int main()
{
    printf(
        "\n----------------------------------\n"
        "Hello from User Space via Syscall!\n"
        "----------------------------------\n"
    );
    
    // ... file operations
    return 0;
}
\end{cppcode}

\subsubsection{File I/O Verification}

Beyond standard output, the program performs a sequence of file manipulations to test the underlying file system driver and the VFS layer. The application attempts to open, write to, seek within, and read from a file path \texttt{/docs/greet.txt}.

This sequence validates path resolution, file descriptor table management in the kernel, and the read/write logic of the file system driver (Listing~\ref{lst:hello_fileio}).

\begin{cppcode}[caption={File I/O Verification Logic}, label={lst:hello_fileio}]
    // Open an existing file with read/write permissions
    FILE *fp = fopen("/docs/greet.txt", "r+");
    if (!fp) {
        printf("Failed to open /docs/greet.txt for reading!\n");
        return 1;
    }

    // Test writing to the file system
    int written = fprintf(fp, "Hello AlkOS from User Space!");
    
    // Test Seeking
    if (fseek(fp, 0, SEEK_SET) != 0) {
        printf("Failed to seek to start of /docs/greet.txt!\n");
        fclose(fp);
        return 1;
    }

    // Test Reading back the data
    char buffer[128];
    size_t bytes_read = fread(buffer, 1, sizeof(buffer) - 1, fp);
    buffer[bytes_read] = '\0';

    printf("Read from file: '%s'\n", buffer);
    fclose(fp);
\end{cppcode}

\subsection{Graphics Test}

The \texttt{gui\_test} application demonstrates the capability of the kernel to provide user space processes with access to video hardware in a controlled manner. The resulting output is shown in Figure \ref{fig:gui_test}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{res/gui_test.png}
    \caption{The graphics test application rendering a dynamic pattern.}
    \label{fig:gui_test}
\end{figure}

\subsubsection{Video Session Initialization}

The application begins by invoking the \texttt{create\_graphic\_session} system call (Listing~\ref{lst:gui_init}). This request instructs the kernel's window manager to allocate a physical memory buffer acting as a backbuffer for the process. The kernel then maps this physical memory into the virtual address space of the process and returns a \texttt{GuiBufferInfo} structure.

\begin{cppcode}[caption={Initializing Graphical Session}, label={lst:gui_init}]
// userspace/programs/gui_test/main.cpp

extern "C" int main()
{
    GuiBufferInfo info = {};
    
    // Syscall to allocate and map framebuffer
    __platform_create_graphic_session(&info);

    if (info.buffer_ptr == nullptr) {
        return 1;
    }

    // Cast void* to raw pixel array
    u32 *fb = static_cast<u32 *>(info.buffer_ptr);
    const auto &fmt = info.format;
    
    // ... rendering loop
}
\end{cppcode}

\subsubsection{Direct Framebuffer Manipulation}

Once the session is established, the application treats the video buffer as a linear array of pixels. It implements a rendering loop that generates a moving pattern by calculating pixel colors based on coordinates and a time-variable offset (Listing~\ref{lst:gui_render}).

\begin{cppcode}[caption={Software Rendering Loop}, label={lst:gui_render}]
    while (true) {
        for (size_t y = 0; y < info.height; ++y) {
            for (size_t x = 0; x < info.width; ++x) {
                size_t index = (y * (info.pitch / 4)) + x;

                u8 r = (x + color_offset) & 0xFF;
                u8 g = (y + color_offset) & 0xFF;
                u8 b = (x + y) & 0xFF;

                // Pack pixel using format info provided by kernel
                u32 pixel = 0;
                pixel |= (static_cast<u32>(r) & ((1 << fmt.red_mask_size) - 1)) << fmt.red_pos;
                pixel |= (static_cast<u32>(g) & ((1 << fmt.green_mask_size) - 1)) << fmt.green_pos;
                pixel |= (static_cast<u32>(b) & ((1 << fmt.blue_mask_size) - 1)) << fmt.blue_pos;

                fb[index] = pixel;
            }
        }
        // ...
    }
\end{cppcode}

\subsubsection{Double Buffering and Synchronization}

To prevent screen tearing, AlkOS employs a double-buffering strategy. The user application writes to its private backbuffer. To display the frame, the application calls \texttt{blit()} (Listing~\ref{lst:gui_sync}). This system call triggers the kernel's window manager to copy the contents of the user's backbuffer to the physical video memory if the user's session is currently active.

\begin{cppcode}[caption={Frame Synchronization}, label={lst:gui_sync}]
    // Blit to screen
    __platform_blit();

    color_offset += 1;

    // Frame limiting
    NanoSleep(16'000'000); // ~60 FPS
\end{cppcode}

\subsection{Shell}

The Shell is a more complex user space program implemented, integrating the input subsystem, graphics subsystem, and process management. Its basic UI is shown in Figure \ref{fig:shell_welcome}.

\subsubsection{Graphical Console Subsystem}

The shell provides a text-based command-line interface. This is achieved through the \texttt{GraphicsConsole} class, which renders text onto a graphical surface using the PSF2 font format \cite{osdev-psf}. The console handles text rendering and cursor management.

\subsubsection{Command Line Interpreter}

The shell implements a Read-Eval-Print Loop (REPL). It reads input from \texttt{stdin}, which is connected to a kernel pipe fed by the keyboard driver. The \texttt{ProcessCommand} method processes the input string and dispatches execution (Listing~\ref{lst:shell_process}).

\begin{cppcode}[caption={Shell Command Processing}, label={lst:shell_process}]
// userspace/programs/shell/shell.cpp

void Shell::ProcessCommand()
{
    // ... string view parsing ...

    if (cmd == "help") {
        CmdHelp();
    } else if (cmd == "clear") {
        CmdClear();
    } else if (cmd == "ls") {
        CmdLs(args);
    } else if (cmd == "cat") {
        CmdCat(args);
    // ... other commands ...
    } else if (cmd == "exec") {
        CmdExec(args);
    } else if (cmd == "kill") {
        CmdKill(args);
    } else {
        console_.Write("Unknown command: ");
        console_.PutChar('\n');
    }
}
\end{cppcode}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{res/shell_welcome.png}
    \caption{The shell interface displaying the welcome message.}
    \label{fig:shell_welcome}
\end{figure}

\subsubsection{Built-in Commands}

The shell includes a suite of internal commands, which are handled directly within the \texttt{Shell::ProcessCommand} routine without spawning new processes.

\begin{itemize}
    \item \textbf{help}: Displays a list of available commands and their usage syntax (Figure \ref{fig:shell_help}).
    
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=\textwidth]{res/shell_help.png}
        \caption{The shell interface displaying the help message and available commands.}
        \label{fig:shell_help}
    \end{figure}
    
    \item \textbf{clear}: Resets the graphical console state. It invokes the \texttt{painter\_.Clear()} method to fill the framebuffer with the background color and resets the cursor position to $(0,0)$.
    
    \item \textbf{echo <text>}: Prints the provided text to the console.

    \item \textbf{shutdown / reboot}: These commands invoke the \texttt{power} system call with \texttt{kShutdown} or \texttt{kReboot} actions, triggering ACPI state transitions to power off or reset the physical machine.
\end{itemize}

\subsubsection{File System Utilities}

The shell maintains a current working directory state and provides utilities to navigate and inspect the file system.

\begin{itemize}
    \item \textbf{pwd}: Prints the full absolute path of the current working directory.
    
    \item \textbf{cd <path>}: Changes the current working directory. The shell resolves the provided path (handling relative paths and parent references like \texttt{..}) and verifies the destination exists and is a directory using the \texttt{file\_info} syscall before updating its internal state.

    \item \textbf{ls [path]}: Lists the contents of a directory. It calls the \texttt{read\_directory} syscall to retrieve directory entries. The output logic includes highlighting: directories are rendered in blue to visually distinguish them from regular files (Figure \ref{fig:shell_ls}).
    
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=\textwidth]{res/shell_ls.png}
        \caption{Execution of the \texttt{ls} command showing the root directory structure.}
        \label{fig:shell_ls}
    \end{figure}

    \item \textbf{cat <file>}: Displays the contents of a file. It uses standard C library functions (\texttt{fopen}, \texttt{fread}) to open the target file and print its content to the console. This command verifies the read functionality of the file system driver (Figure \ref{fig:shell_cat}).
    
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=\textwidth]{res/shell_cat.png}
        \caption{The \texttt{cat} command displaying the contents of a text file from the FAT16 partition.}
        \label{fig:shell_cat}
    \end{figure}
\end{itemize}

\subsubsection{Process Execution and Control}

The shell demonstrates the operating system's multitasking capabilities through commands that wrap process management system calls.

\begin{itemize}
    \item \textbf{exec <file> (or ./<file>)}: Loads and executes an ELF64 binary. The shell invokes the \texttt{exec} system call and immediately calls \texttt{wait(pid)}, blocking the shell's execution until the child process terminates.

    \item \textbf{exe\_async <file> (or ./<file> \&)}: The shell does not wait for the child process, allowing the user to continue issuing commands while the background process executes. This validates the scheduler's ability to timeshare the CPU between the interactive shell and compute-intensive background tasks.

    \item \textbf{kill <pid>}: Sends a termination signal to a specific process. This invokes the \texttt{kill} system call to forcibly terminate the target application and reclaim its resources.
\end{itemize}

\subsection{Doom}

To demonstrate the capability of the AlkOS user space and its standard library compatibility, we undertook the task of porting \textit{Doom}~\cite{idsoftware1993doom} (1993). In the realm of systems programming, getting Doom to run is often considered the de facto "Hello World" for operating systems, proving that the scheduler, memory management, file system, and video subsystem function cohesively. A screenshot of the port running on AlkOS is shown in Figure \ref{fig:doom_gameplay}.

We utilized \textbf{Doomgeneric}~\cite{doomgeneric}, a source port specifically designed to be portable. It strips away the complex, operating-system-specific dependencies (such as sound drivers, networking, or windowing) and exposes a minimal interface that the target OS must implement.

It is important to note that while the Doom engine has been open source since 1997 \cite{idsoftware1997doomsource}, the game data (levels, graphics, sounds) is stored in \textbf{.WAD} files, which remain under copyright. To execute the game on AlkOS, a valid \texttt{.WAD} file must be placed in the \texttt{/doom} directory of the rootfs overlay. This can be the \texttt{DOOM1.WAD} from the shareware version or the full \texttt{DOOM.WAD} from a retail copy. The engine supports games based on the same technology, such as \textit{Doom II} or \textit{Heretic}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{res/doom_gameplay.png}
    \caption{Doom gameplay on AlkOS running at 800x600 scaled resolution.}
    \label{fig:doom_gameplay}
\end{figure}

\subsubsection{The Doomgeneric Interface}

The integration of \textit{Doomgeneric} requires the implementation of a small set of functions defined in \texttt{doomgeneric.h}. These functions handle video output, input processing, and timing. The key functions that must be provided are:

\begin{itemize}
    \item \texttt{DG\_Init}: Initializes the video system and prepares input handling.
    \item \texttt{DG\_DrawFrame}: Called by the game loop when a frame is ready to be presented to the screen.
    \item \texttt{DG\_SleepMs}: Pauses execution to regulate the game loop timing.
    \item \texttt{DG\_GetTicksMs}: Returns the monotonic system time in milliseconds, used for game logic timing like delta time calculations.
    \item \texttt{DG\_GetKey}: Retrieves pending input events.
\end{itemize}

In our implementation (\texttt{doomgeneric\_alkos.c}), these functions wrap the AlkOS system calls. For instance, \texttt{DG\_GetTicksMs} utilizes \texttt{get\_clock\_value} to retrieve high-resolution timing, and \texttt{DG\_DrawFrame} invokes the \texttt{blit} syscall to trigger framebuffer updates.

\subsubsection{Video Output and Scaling}

The original resolution of Doom is $320 \times 200$ pixels. Modern framebuffers usually operate at significantly higher resolutions (e.g., $1024 \times 768$ or $1920 \times 1080$). Displaying a raw $320 \times 200$ image on such a screen would result in a tiny window. Consequently, software scaling is required.

\paragraph{Scaling Strategy}
A naive integer scaling approach (e.g., simply multiplying pixels by an integer factor) used in this port was not taking advantage of the available screen and resulted in wasted screen space. To address this, we restored and integrated legacy Doom scaling algorithms that support \textbf{Squash} and \textbf{Stretch} modes.

We implemented an automatic mode selection algorithm in \texttt{doomgeneric.c}. Upon initialization, the game inspects the dimensions of the framebuffer allocated by the window manager and calculates the optimal scaling factor ($1\times$ to $5\times$) and select mode (Stretch or Squash) to maximize screen usage:
\begin{itemize}
    \item \textbf{Stretch Mode}: Used when the vertical resolution allows for aspect ratio correction by stretching the height.
    \item \textbf{Squash Mode}: Used when horizontal compression is required to fit the aspect ratio.
    \item \textbf{Special Case}: Resolution $800 \times 600$ trigger optimized path ($3\times$ squash with $50\%$ pixel blending) to reduce visual artifacts.
\end{itemize}

\paragraph{Pixel Format}
The internal rendering engine of Doom operates in an 8-bit indexed color mode, utilizing a 256-color palette. However, the linear framebuffer provided by the kernel operates in a true color mode. To bridge this gap, we implemented a color conversion routine within the screen update loop. During the scaling process, each 8-bit pixel index is resolved against the current palette to obtain its RGB components. These components are then packed according to the specific red, green, and blue field positions defined by the framebuffer metadata. This ensures correct color reproduction regardless of the specific pixel format utilized by the underlying hardware.

\paragraph{Buffer Optimization}
\textit{Doomgeneric} renders the game to an internal buffer, and then we copy it again to the window manager's buffer. To maximize performance, we modified the \texttt{i\_video.c} rendering path to remove one layer of indirection. We allow the scaling routines to write directly into the framebuffer obtained via the \texttt{create\_graphic\_session} syscall. The scaling is performed in-place during the screen update phase.

\subsubsection{Input Event Mapping}

Doom expects a queue of input events (key presses and releases) using its own internal key codes. AlkOS, however, exposes a polling-based interface via the \texttt{GetKeyState} syscall, which returns the instantaneous state of a \texttt{VirtualKey} (keyboard layout agnostic key codes).

To bridge this gap, we implemented a translation layer in \texttt{doomgeneric\_alkos.c}. The system maintains an array of previous key states for all keys relevant to the game (movement, action, and menu keys). During every frame tick, the wrapper polls the current state of these keys using the kernel API. By comparing the current state with the previous state, we synthesize "Press" and "Release" events and push them into Doom's internal event queue.

We also utilized a lookup table to translate \texttt{VirtualKey} enum (e.g., \texttt{VK\_LeftCtrl}) into Doom's internal key codes (e.g., \texttt{KEY\_FIRE}), ensuring correct mapping for gameplay actions.

\subsubsection{Porting Process}

The process of porting \textit{Doom} revealed several shortcomings in the existing \texttt{libc} implementation, which required a subsequent extension of its functionality. Although the \textit{Doomgeneric} is designed to be largely operating-system-agnostic, portions of its codebase implicitly assume the presence of a runtime environment with at least partial POSIX compliance. In particular, commonly used facilities such as string manipulation routines and directory handling interfaces were expected to be available. The following sections outline the key areas where the \texttt{libc} was enhanced to meet these requirements.

\paragraph{Memory Management}
Doom relies heavily on dynamic memory allocation for level geometry, textures, and game objects (\texttt{Z\_Malloc}). Our \texttt{libc} initially lacked a heap allocator. We implemented a standard \texttt{malloc}, \texttt{free}, \texttt{calloc}, and \texttt{realloc} suite based on a linked-list memory control block strategy \cite{malloc-impl}. This required adding a new system call, \texttt{get\_heap\_start}, which allows the user space allocator to locate the valid heap region assigned by the kernel's VMM.

\paragraph{String Manipulation}
The Doom engine, particularly the WAD management system, relies on case-insensitive string comparisons to locate resources (e.g., finding "doom.wad" when the file is named "DOOM.WAD"). We implemented POSIX-compliant \texttt{strcasecmp} and \texttt{strncasecmp} functions to support this behavior.

\paragraph{Standard I/O}
To support loading data from the file system, we expanded the \texttt{stdio} implementation to include \texttt{fopen}, \texttt{fread}, \texttt{fseek}, and \texttt{ftell}, backed by the underlying syscalls (\texttt{open}, \texttt{read}, \texttt{seek}). This allowed the Doom engine to seamlessly load WAD files from the root file system using standard I/O semantics. Also, \texttt{create\_directory} needed to be implemented to create the \texttt{saves/} directory for saving game progress. \\

By addressing these requirements, the Doom port served not only as a graphical demo but as a comprehensive validation test for the kernel's memory management, file system, and standard library implementations.

\chapter{Results}
\label{sec:results}

This chapter summarizes the work accomplished and the assumptions made during the development process. Furthermore, it provides a critical analysis of the implemented solution.

\section{Achieved Functionalities}

We successfully fulfilled all functional requirements defined in Section \ref{subsec:scope}, in addition to implementing several supplementary features. The following list details all functionalities achieved:

\begin{itemize} 
    \item \textbf{Hardware Abstraction Layer (HAL)} -- An architectural interface decoupling high-level kernel logic from platform-specific hardware instructions.
    \item \textbf{Custom Bootloader} -- A multi-stage loader handling the transition from Multiboot2 to a 64-bit higher-half kernel environment.
    \item \textbf{Physical Memory Management} -- A robust allocation system combining a Buddy Allocator for pages and a Slab Allocator for kernel objects.
    \item \textbf{Virtual Memory Management} -- A comprehensive VMM supporting recursive page table mapping, higher-half mapping, and kernel space synchronization.
    \item \textbf{Lazy Allocation} -- A demand-paging mechanism that allocates physical memory only when accessed, utilizing page fault handling.
    \item \textbf{Tickless Kernel Architecture} -- A dynamic timing subsystem that schedules interrupts only when necessary, improving CPU efficiency.
    \item \textbf{Advanced Scheduling Policies} -- Implementation of multiple algorithms, including Round Robin, Priority Queue, and Multi-Level Feedback Queue (MLFQ).
    \item \textbf{Process \& Thread Management} -- Full lifecycle management including creation, execution, detaching, joining, and termination of tasks.
    \item \textbf{Wait Queues} -- A synchronization primitive allowing threads to block efficiently until specific conditions or events occur.
    \item \textbf{System Call Interface} -- A dispatch mechanism utilizing \texttt{int 0x80} instruction with a wide array of implemented services.
    \item \textbf{Standard C Library (libc)} -- A custom implementation of the C standard library (stdio, stdlib, string, math) tailored for the kernel environment.
    \item \textbf{Virtual File System (VFS)} -- An abstraction layer supporting mount points, file descriptors, and generic I/O operations.
    \item \textbf{FAT File System Support} -- Full read/write drivers for FAT12, FAT16, and FAT32 file systems.
    \item \textbf{Initial Ramdisk (Initrd)} -- Support for loading a temporary root file system from memory during the boot process.
    \item \textbf{ELF64 Loader} -- A static parser capable of validating, loading, and relocating standard 64-bit executables.
    \item \textbf{User Space Shell} -- An interactive command-line environment with built-in commands and process execution capabilities.
    \item \textbf{Graphical Output} -- A linear framebuffer driver supporting double-buffering and direct pixel manipulation.
    \item \textbf{Window Manager} -- A session-based system allowing switching between different graphical applications.
    \item \textbf{Doom Port} -- A successful port of the Doom game engine, validating the stability of the kernel's subsystems.
    \item \textbf{ACPI Integration} -- A subsystem for parsing ACPI tables to discover hardware topology and manage system power states.
    \item \textbf{In-Kernel Testing Framework} -- A custom unit-testing suite running directly on bare metal to verify kernel logic.
    \item \textbf{Build Infrastructure} -- A fully automated CMake-based pipeline capable of bootstrapping a custom cross-compilation toolchain.
    \item \textbf{Scripts} -- A collection of scripts that streamline common development tasks, manage build configurations, and automate the environment setup.
\end{itemize}

\section{Limitations and Assumptions}

TODO: KRYCZKA DODAJ SWOJE

Developing an operating system kernel is an inherently complex task, further complicated by the intricacies of hardware architectures. To maintain a steady development pace and avoid getting stalled by exhaustive driver implementation or edge-case handling, several strategic assumptions were made. Consequently, for AlkOS to function correctly, the underlying hardware must meet the following requirements:

\begin{itemize} 
    \item \textbf{64-bit Architecture} -- The system supports only 64-bit instructions. Compatibility libraries for 32-bit applications are not provided. 
    \item \textbf{AVX2 Support} -- The system requires AVX2 support. Implementation was restricted to this specific instruction set to avoid the complexity of supporting every possible vector extension. 
    \item \textbf{SWAPGS Instruction} -- The architecture must support the \texttt{swapgs} instruction, which is utilized to efficiently swap the GS register base address for Core Local Storage management. 
    \item \textbf{Invariant TSC} -- To avoid implementing a driver for every possible x86-64 timer, the system relies on the Time Stamp Counter (TSC) as the primary clock source. The hardware must support an invariant TSC to ensure timing consistency regardless of frequency scaling.
    \item \textbf{HPET} -- The High Precision Event Timer (HPET) must be available, as it serves as the reference source for calibrating both the TSC and the LAPIC Timer. 
    \item \textbf{APIC} -- Although SMP is not fully enabled, the system architecture was designed with SMP compatibility in mind, necessitating the presence of an Advanced Programmable Interrupt Controller (APIC). 
    \item \textbf{ACPI} -- It is assumed that device discovery is performed via the Advanced Configuration and Power Interface (ACPI), a standard supported by the majority of modern platforms. 
    \item \textbf{PS/2 Keyboard} -- Input is handled via a PS/2 keyboard controller. A USB stack was not implemented due to the significant development effort required. 
    \item \textbf{FAT12/16/32 Support} -- The system supports the FAT12, FAT16, and FAT32 file system variants. Long file name (LFN) support is not implemented; filenames are therefore limited to the 8.3 format.
    \item \textbf{Ramdisk} -- The system is bootstrapped using an in-memory file system mounted as the initial root file system. Loading a root file system from persistent storage is not supported, and thus, no permanent modifications are possible. The expected file system is hardcoded to FAT16.
    \item \textbf{ELF64} -- User space applications must be compiled as ELF64 binaries. Dynamic linking is not supported. In particular, binaries containing a \texttt{PT\_INTERP} segment \cite{elf-spec-dynamic} or relying on shared libraries cannot be executed.
\end{itemize}

\section{Future Work}

TODO: KRYCZKA DODAJ SWOJE

The current solution can be extended and improved in several areas: 
\begin{itemize} 
    \item \textbf{SMP Support} -- As most components were designed with multicore execution in mind, implementing SMP support is a feasible next step. This would involve reinforcing locking schemes, implementing core boot-up sequences, inter-core communication, and a scheduler load balancer. 
    \item \textbf{CPU Frequency Scaling} -- Implement a mechanism to dynamically adjust the CPU frequency based on system load and thermal conditions.
    \item \textbf{Relocatable Code and Dynamic Libraries} -- Currently, the system supports only statically linked binaries. Future work could include a dynamic linker to support shared libraries. 
    \item \textbf{General Driver Interface} -- Establishing a standardized driver model to facilitate the easy addition of new devices. 
    \item \textbf{Refining Scheduling Algorithms} -- Improving the scheduler to enhance fairness and reduce latency. 
    \item \textbf{Completing Libc Implementation} -- Extending the standard library to support a wider range of third-party applications. 
    \item \textbf{Task Synchronization Primitives} -- Implementing full support for mutexes, semaphores, and condition variables.
    \item \textbf{Networking Stack} -- Adding a network layer to support TCP/IP communication.
    \item \textbf{USB Controller Driver} -- Implementing a USB controller driver to support USB devices.
    \item \textbf{Physical Storage Drivers} -- Replacing the RAM disk with drivers for physical storage media (e.g., AHCI/NVMe). 
    \item \textbf{Abstract Filesystems} -- Add pseudo filesystems (e.g., \texttt{procfs}, \texttt{sysfs}) for extensible system interfaces.
    \item \textbf{File System Caching} -- Introduce caching to improve file system performance.
    \item \textbf{File System Permissions} -- Implement file and directory permissions for security and access control.
    \item \textbf{Fast System Call} -- Replace the current interrupt-based syscall mechanism with the \texttt{syscall}/\texttt{sysret} instructions to reduce overhead and improve kernel entry performance.
    \item \textbf{Syscall Restriction} -- Implement a mechanism to restrict access to system calls based on process privileges or roles.
    \item \textbf{Compositing Window Manager} -- Implementing a compositing window manager to support a window-based graphical user interface.
\end{itemize}

\section{Tests}

To ensure the high quality and stability required for a project of this magnitude, we employed two primary testing methodologies throughout the development process:

\begin{itemize}
    \item \textbf{Unit Testing} --- We implemented a comprehensive suite of unit tests within the kernel to verify the correctness of the custom C/C++ standard libraries (\texttt{libc}/\texttt{libc++}) and internal kernel components.
    \item \textbf{Regression Testing} --- We utilized Continuous Integration (CI) pipelines to automatically parse kernel logs, ensuring that no warnings or fatal errors were introduced during development.
\end{itemize}

Finally, with everything implemented, we validated the system manually by executing the suite of example programs described in Section~\ref{sec::example_programs}.

\section{Conclusion}

Summing up, we successfully fulfilled the project's initial requirements and significantly extended its scope with multiple additional features. Subjectively, we are proud of the achievement, particularly the final shape of the system and the effort invested. We surpassed our initial expectations and accomplished our primary goal: gaining a deep, practical understanding of the complex concepts involved in operating system development.

Objectively, the project delivered more than anticipated. The final codebase consists of approximately 700 files and over 80,000 lines of code. This achievement required processing a vast amount of technical literature, including hundreds of pages of the Intel Software Developer's Manual, OSDev documentation, hardware specifications, and the source code of established systems like Linux or Minix.

However, a critical assessment reveals that the execution was not flawless, and we encountered multiple obstacles along the way. We were compelled to refactor thousands of lines of code due to technical debt or an initial lack of understanding regarding specific mechanisms. For instance, the build system underwent at least two complete rewrites and still requires further refinement. Furthermore, early design decisions regarding project structure and header inclusion patterns led to circular dependency issues that had to be painstakingly resolved.

Regarding the division of labor, we believe the workload was distributed effectively. The inherent complexity of the project ensured that there were always more problems to solve than available contributors, keeping everyone fully engaged. In retrospect, communication during the architectural design phase could have been better to avoid some of the aforementioned structural issues. Finally, while we likely allocated too much time to the implementation of the C and C++ standard libraries, this process proved to be extremely educational and significantly expanded our horizons anyway.

% ----------- BIBLIOGRAPHY ---------

\printbibliography[heading=bibintoc]

\pagenumbering{gobble}
\thispagestyle{empty}

% --------- LIST OF SYMBOLS AND ABBREVIATIONS ------
\chapter*{List of symbols and abbreviations}

\begin{supertabular}{cl}
ABI & Application Binary Interface \\
ACPI & Advanced Configuration and Power Interface \\
AES-NI & Advanced Encryption Standard New Instructions \\
AHCI & Advanced Host Controller Interface \\
API & Application Programming Interface \\
APIC & Advanced Programmable Interrupt Controller \\
ASLR & Address Space Layout Randomization \\
AVX & Advanced Vector Extensions \\
BIOS & Basic Input/Output System \\
BSD & Berkeley Software Distribution \\
BSS & Block Starting Symbol \\
BST & Binary Search Tree \\
CFS & Completely Fair Scheduler \\
CI & Continuous Integration \\
CLS & Core Local Storage \\
COW & Copy On Write \\
CPU & Central Processing Unit \\
CRTP & Curiously Recurring Template Pattern \\
DDR5 & Double Data Rate 5 \\
DMA & Direct Memory Access \\
DRAM & Dynamic Random Access Memory \\
DTB & Device Tree Blob \\
EEVDF & Earliest Eligible Virtual Deadline First \\
ELF & Executable and Linkable Format \\
eMMC & Embedded MultiMediaCard \\
FAT & File Allocation Table \\
FHS & Filesystem Hierarchy Standard \\
FLI & First-Level Index \\
FPU & Floating Point Unit \\
GCC & GNU Compiler Collection \\
GDB & GNU Debugger \\
GDT & Global Descriptor Table \\
GPU & Graphics Processing Unit \\
GRUB & GRand Unified Bootloader \\
HAL & Hardware Abstraction Layer \\
HPET & High Precision Event Timer \\
IDT & Interrupt Descriptor Table \\
IOAPIC & I/O Advanced Programmable Interrupt Controller \\
IPC & Inter-Process Communication \\
ISA & Instruction Set Architecture \\
ISO & International Organization for Standardization \\
ISR & Interrupt Service Routine \\
JSON & JavaScript Object Notation \\
LAPIC & Local Advanced Programmable Interrupt Controller \\
LBA & Linear Block Addressing \\
LFN & Long File Name \\
LIT & Logical Interrupt Table \\
LKM & Loadable Kernel Module \\
MD & Machine Dependent \\
MI & Machine Independent \\
MLFQ & Multi-Level Feedback Queue \\
MMC & MultiMediaCard \\
MMIO & Memory-Mapped I/O \\
MMU & Memory Management Unit \\
NIC & Network Interface Controller \\
NUMA & Non-Uniform Memory Access \\
NVMe & Non-Volatile Memory Express \\
OS & Operating System \\
PA & Physical Address \\
PC & Personal Computer \\
PCI & Peripheral Component Interconnect \\
PCID & Process-Context Identifier \\
PCIe & Peripheral Component Interconnect Express \\
PCP & Per-CPU Pagesets \\
PD & Page Directory \\
PDP & Page Directory Pointer \\
PFN & Physical Frame Number \\
PIC & Programmable Interrupt Controller \\
PID & Process IDentifier \\
PIE & Position Independent Executable \\
PIT & Programmable Interval Timer \\
PLL & Phase-Locked Loop \\
PML4 & Page Map Level 4 \\
PMM & Physical Memory Manager \\
POSIX & Portable Operating System Interface \\
PS/2 & Personal System/2 \\
PT & Page Table \\
RAM & Random Access Memory \\
RAII & Resource Acquisition Is Initialization \\
REPL & Read-Eval-Print Loop \\
ROM & Read-Only Memory \\
RR & Round Robin \\
RTC & Real-Time Clock \\
RTOS & Real-Time Operating System \\
SATA & Serial Advanced Technology Attachment \\
SD & Secure Digital \\
SIMD & Single Instruction, Multiple Data \\
SLI & Second-Level Index \\
SMAP & Supervisor Mode Access Prevention \\
SMEP & Supervisor Mode Execution Prevention \\
SMP & Symmetric MultiProcessing \\
SoC & System on Chip \\
SRAM & Static Random Access Memory \\
SSD & Solid State Drive \\
SSE & Streaming SIMD Extensions \\
SSOT & Single Source Of Truth \\
TID & Thread IDentifier \\
TLB & Translation Lookaside Buffer \\
TLSF & Two-Level Segregated Fit \\
TPU & Tensor Processing Unit \\
TOML & Tom's Obvious, Minimal Language \\
TSC & TimeStamp Counter \\
TSS & Task State Segment \\
UEFI & Unified Extensible Firmware Interface \\
USB & Universal Serial Bus \\
VA & Virtual Address \\
vDSO & Virtual Dynamic Shared Object \\
VFS & Virtual File System \\
VGA & Video Graphics Array \\
VMM & Virtual Memory Manager \\
VPN & Virtual Page Number \\
WAD & Where's All the Data \\
XIP & eXecute In Place \\
YAML & YAML Ain't Markup Language \\
\end{supertabular}
\\
\thispagestyle{empty}


% ----------  LIST OF FIGURES ------------
\listoffigures
\thispagestyle{empty}

% -----------  LIST OF TABLES ------------
\renewcommand{\listtablename}{List of Tables}
\listoftables
\thispagestyle{empty}

\end{document}

% ==================================================================
% TODOS:
% - wytlumaczyc na starcie co zakladamy etc i dlaczego np segmenty
