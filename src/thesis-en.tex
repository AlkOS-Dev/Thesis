\documentclass[a4paper,9pt,twoside]{report}

% ----------------------   PREAMBLE PART ------------------------------

% ------------------------ ENCODING & LANGUAGES ----------------------

\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage[polish, english]{babel}


\usepackage{amsmath, amsfonts, amsthm, latexsym}

\usepackage[final]{pdfpages}
\usepackage[backend=biber, style=numeric, sorting=none]{biblatex}
\addbibresource{references.bib}


\usepackage{commath}

\usepackage[hidelinks]{hyperref}

\usepackage[inkscapepath=../output/svg/]{svg}


% ---------------- MARGINS, INDENTATION, LINESPREAD ------------------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}


\linespread{1.5}
\allowdisplaybreaks

\usepackage{indentfirst}
\setlength{\parindent}{5mm}


%---------------- RUNNING HEAD - CHAPTER NAMES, PAGE NUMBERS ETC. -------------------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[LE,RO]{\thepage} 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}

\renewcommand{\headrulewidth}{0 pt}


\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[LE,RO]{\thepage}
  
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.0pt}
}

%---------------- code listings -------------------

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstdefinestyle{cppstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=C++,
    morekeywords={constexpr, nullptr, size_t, uint64_t}
}
\lstset{style=cppstyle}
\renewcommand{\lstlistingname}{C++ Code Snippet}

\lstdefinestyle{nasmstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=[x86masm]Assembler,
    morekeywords={rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8, r9, r10, r11, r12, r13, r14, r15,
                  eax, ebx, ecx, edx, esi, edi, ebp, esp,
                  cr0, cr2, cr3,
                  mov, push, pop, call, ret, int, iretq, jmp, je, jne, jg, jl, cmp, test,
                  add, sub, mul, div, inc, dec, xor, or, and,
                  lidt, lgdt, sti, cli, hlt,
                  section, global, extern, db, dw, dd, dq, resb, resw, resd, resq,
                  macro, endmacro, \%define}
}

\lstdefinestyle{cmakestyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=CMake,
    morekeywords={project, add_executable, add_library, target_link_libraries, set, include_directories,
                  cmake_minimum_required, file, macro, endmacro, foreach, endforeach, if, endif, else,
                  find_package, add_custom_command, add_custom_target, install, alkos_find_sources, alkos_register_userspace_app}
}

% --------------------------- DRAWING ---------------------

\usepackage{tikz}
\usetikzlibrary{fit, backgrounds, shapes, arrows.meta, positioning, calc, chains}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{svg}
\usepackage{graphicx}

% --------------------------- CHAPTER HEADERS ---------------------

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 

    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% ----------------------- TABLE OF CONTENTS SETUP ---------------------------

\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}
  [0pt]
  {}
  {\bfseries \thecontentslabel.\quad}
  {\bfseries}
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% ---------------------- TABLES AD FIGURES NUMBERING ----------------------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}


% ------------- DEFINING ENVIRONMENTS FOR THEOREMS, DEFINITIONS ETC. ---------------

\makeatletter
\newtheoremstyle{definition}
{3ex}
{3ex}
{\upshape}
{}
{\bfseries}
{.}
{.5em}
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% --------------------- END OF PREAMBLE PART (MOSTLY) --------------------------





% -------------------------- USER SETTINGS ---------------------------

\newcommand{\tytul}{Funkcjonalne jądro systemu operacyjnego: AlkOS}
\renewcommand{\title}{From Bare Metal to a Functional Kernel: The AlkOS Operating System}
\newcommand{\type}{Engineer}
\newcommand{\supervisor}{mgr inż. Paweł Sobótka}



\begin{document}
\sloppy
\selectlanguage{english}

\includepdf[pages=-]{titlepage-en}

\null\thispagestyle{empty}\newpage

% ------------------ PAGE WITH SIGNATURES --------------------------------

%\thispagestyle{empty}\newpage
%\null
%
%\vfill
%
%\begin{center}
%\begin{tabular}[t]{ccc}
%............................................. & \hspace*{100pt} & .............................................\\
%supervisor's signature & \hspace*{100pt} & author's signature
%\end{tabular}
%\end{center}
%


% ---------------------------- ABSTRACT -----------------------------

{  \fontsize{12}{14} \selectfont
\begin{abstract}

\begin{center}
\title
\end{center}

TODO

\end{abstract}
}

\null\thispagestyle{empty}\newpage

%% --------------------------- DECLARATIONS ------------------------------------
%
%%
%%	IT IS NECESSARY OT ATTACH FILLED-OUT AUTORSHIP DEECLRATION. SCAN (IN PDF FORMAT) NEEDS TO BE PLACED IN scans FOLDER AND IT SHOULD BE CALLED, FOR EXAMPLE, DECLARATION_OF_AUTORSHIP.PDF. IF THE FILENAME OR FILEPATH IS DIFFERENT, THE FILEPATH IN THE NEXT COMMAND HAS TO BE ADJUSTED ACCORDINGLY.
%%
%%	command attacging the declarations of autorship
%%
%\includepdf[pages=-]{scans/declaration-of-autorship}
%\null\thispagestyle{empty}\newpage
%
%% optional declaration
%%
%%	command attaching the declaataration on granting a license
%%
%\includepdf[pages=-]{scans/declaration-on-granting-a-license}
%%
%%	.tex corresponding to the above PDF files are present in the 3. declarations folder 
%
\null\thispagestyle{empty}\newpage
% ------------------- TABLE OF CONTENTS ---------------------
\selectlanguage{english}
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\newpage % IF YOU HAVE EVEN QUANTITY OD PAGES OF TOC, THEN REMOVE IT OR ADD \null\newpage FOR DOUBLE BLANK PAGE BEFORE INTRODUCTION


% -------------------- THE BODY OF THE THESIS --------------------------------

\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11}


\chapter{Introduction}
\markboth{}{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\section{Theoretical Background} 
TODO KTOKOLWIEK

\subsubsection{Interrupts}
\subsubsection{Kernel Tick}
\subsubsection{Exceptions}
\subsubsection{Virtualization}
\subsubsection{Address Space}
\subsubsection{Context Switch}

\section{Scope of the Thesis} 
TODO KTOKOLWIEK
\section{Limitations and Assumptions}

\subsection{Minimal requirements}
\begin{itemize}
  \item avx
  \item osxsave
  \item swapgs
  \item invariant tsc
  \item HPET
  \item APIC
  \item acpi
  \item 64bit LAPIC
  \item mmu
\end{itemize}

\subsection{Known Limitations}
\label{subsec:limitations}

\begin{itemize}
  \item max process name = 128
  \item max processes = 4096
  \item max threads = 8192
\end{itemize}

Functionalities:
\begin{itemize}
  \item aaa
\end{itemize}

\section{Achieved Functionalities} 
TODO KTOKOLWIEK
\section{Work Division -- Code} 
TODO KTOKOLWIEK
\section{Work Division -- Thesis}
TODO KTOKOLWIEK

% ==================================================================

\chapter{Creating an Operating System from Scratch}

Here we will briefly describe in proper sequence what are the most important parts of the kernel on which most of other code will highly depend on. This sequence is build on our experience and meterials we read during the development so it is not the ideal way of things, but our view we propose. Following the providing sequnece should be an easy path to follow instead of discovering everything again. We will focus only on monolithic-kernel design where every module resides in same address space - kernel address space.

\section{Host Environment Preparation}

Before proceeding to writing actual kernel code we must first do some preparation on our development tools and development environemnts. Below we can find brief description of needed things.

\subsection{Cross-Compilation Toolchain}
TODO KTOKOLWIEK
\subsection{Building Machinery}
TODO KTOKOLWIEK
\subsection{Emulation}
TODO KTOKOLWIEK

\section{General Design Considerations}
TODO KTOKOLWIEK

\subsection{Intrusive Data Structures}
TODO KTOKOLWIEK

\subsection{Ref-counting}
TODO KTOKOLWIEK

\subsection{Archtiecture Abstraction}
TODO KTOKOLWIEK

\subsection{Directory Layout}
TODO KTOKOLWIEK

\subsection{Including Good Practices}
TODO KTOKOLWIEK

\section{Target Environment}
TODO KTOKOLWIEK

\subsection{Implementation of Libc and Libc++}
TODO KTOKOLWIEK

\subsection{Bootloader}
\label{subsec:theory_bootloader}

The process of bringing a computer from a powered-off state to a fully functional operating system is governed by a rigid chain of physical and logical constraints. At the hardware level, the Central Processing Unit (CPU) functions as a complex state machine. Upon the application of power or a reset signal, the CPU resets its internal registers to default values and sets the Instruction Pointer to a specific, hardcoded physical address known as the \textit{Reset Vector} \cite{IntelManual-Reset}.

\subsubsection{The Memory Paradox and Storage}
A fundamental challenge in this sequence is the source of the initial instructions. The standard Random Access Memory (RAM), which serves as the primary workspace for modern operating systems, is volatile. It requires active electrical flow to maintain its state. When the system is powered off, the state is lost; upon power-up, the memory cells contain random garbage data. Consequently, the CPU cannot fetch valid instructions from standard RAM immediately after a reset.

To resolve this, hardware architects map the Reset Vector address to a non-volatile memory region, typically Flash Memory or Read-Only Memory (ROM), which retains data without power.

\subsubsection{Embedded vs. Complex Architectures}
In simple embedded architectures (e.g., microcontrollers used in automotive braking systems or household appliances), the entire application code is often stored in this non-volatile memory. The memory controller maps this storage directly into the CPU's addressable space. This technique, known as \textbf{Execute In Place (XIP)}, allows the CPU to fetch and execute the developer's code from the very first clock cycle \cite{ARM-CortexM4-Generic-User-Guide}. The developer "owns" the machine from the first nanosecond.

In contrast, more complex architectures (such as ARM-based smartphones or single-board computers like the Raspberry Pi) often store the main operating system on external, complex storage media like SD cards or eMMC chips. The CPU cannot simply memory-map an SD card; it requires a sophisticated software driver to communicate with the storage controller. To bridge this gap, manufacturers embed a tiny, immutable piece of software called the \textbf{BootROM} directly into the silicon. This code initializes the minimal required hardware (often internal SRAM) and loads a secondary bootloader from the external storage into that SRAM, which in turn loads the main software.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    % Standard block style
    block/.style={
        rectangle, 
        draw=black!70, 
        rounded corners=3pt, 
        minimum width=2.2cm, 
        minimum height=1cm, 
        align=center, 
        fill=white,
        font=\small
    },
    % Style for the "Silicon" container
    soc/.style={
        rectangle,
        draw=black!40,
        dashed,
        fill=gray!5,
        rounded corners=5pt,
        inner sep=0.5cm
    },
    % Arrow styles
    arrow/.style={-Latex, thick, color=black!80},
    data/.style={-Latex, thick, dashed, color=black!80},
    % Label style for arrows
    lbl/.style={
        font=\footnotesize\bfseries, 
        fill=white, 
        inner sep=1pt,
        text=black!80
    }
]

% =========================================================
% LEFT SIDE: SIMPLE XIP
% =========================================================

\node (cpu1) [block, fill=blue!10] {CPU};
\node (flash) [block, below=1.5cm of cpu1, fill=orange!10] {NOR Flash\\(Memory Mapped)};
\node (label1) [above=0.1cm of cpu1, font=\bfseries] {Simple (XIP)};

% Arrow
\draw[arrow] (cpu1) -- node[midway, right, font=\footnotesize] {Direct Fetch} (flash);


% =========================================================
% RIGHT SIDE: COMPLEX BOOT
% =========================================================

% We place CPU2 to the right
\node (cpu2) [block, right=6cm of cpu1, fill=blue!10] {CPU};

% Define relative positions for BootROM and SRAM relative to CPU2
\node (bootrom) [block, below left=1.2cm and -0.5cm of cpu2, fill=gray!20] {BootROM\\(Immutable)};
\node (sram) [block, below right=1.2cm and -0.5cm of cpu2, fill=green!10] {Internal\\SRAM};

% Draw the SoC Boundary around them
\begin{scope}[on background layer]
    \node (soc_box) [soc, fit=(cpu2) (bootrom) (sram), label={[anchor=south west, inner sep=5pt]north west:\tiny System on Chip (SoC)}] {};
\end{scope}

% External Storage below the SoC
\node (sdcard) [block, below=1.0cm of soc_box, fill=orange!10] {SD Card / Disk\\(External)};
\node (label2) [above=0.5cm of soc_box, font=\bfseries] {Complex (Bootload)};


% =========================================================
% ARROWS & FLOW (Complex)
% =========================================================

% 1. Power On -> BootROM
\draw[arrow] (cpu2) -- node[lbl, pos=0.6] {1. Init} (bootrom);

% 2. BootROM -> External
\draw[arrow] (bootrom) -- node[lbl, pos=0.4] {2. Load} (sdcard);

% 3. External -> SRAM (Data copy)
\draw[data] (sdcard) -- node[lbl, pos=0.4] {3. Copy} (sram);

% 4. CPU -> SRAM (Execution)
\draw[arrow] (cpu2) -- node[lbl, pos=0.6] {4. Jump} (sram);

\end{tikzpicture}
\caption{Comparison of Boot Architectures}
\label{fig:boot_methods}
\end{figure}

On modern "heavy" machines, such as x86-64 workstations, the initialization process is exponentially more complex. The main CPU is fragile and dependent on a specific environment to function. Before the main cores can execute a single instruction, the hardware requires:
\begin{enumerate}
    \item \textbf{Power Sequencing:} Multiple voltage rails (Vcore, VccSA, VccIO) must be brought up in a specific order with millisecond-precision timing.
    \item \textbf{Clock Stabilization:} Phase-Locked Loops (PLLs) must be tuned and stabilized to generate the gigahertz-range frequencies required by the cores.
    \item \textbf{DRAM Training:} Modern DDR4/DDR5 memory requires a complex calibration process to align signal timing before it becomes usable.
\end{enumerate}
\cite{Intel-Datasheet-Vol1}

To manage this, modern chipsets often include a smaller, dedicated processor (e.g., the Intel Management Engine or AMD Platform Security Processor) that starts before the main CPU. This co-processor initializes the platform hardware to a state where the main CPU can begin execution.

\subsubsection{The Chain of Trust and Abstraction}
\label{subsubsec:chain_of_trust_and_abstraction}
By the time a modern Operating System kernel begins execution, it is likely the fourth or fifth program in the boot chain. The entity responsible for defining the interface between the hardware and the OS is the \textbf{System Firmware} \cite{UEFI-Base-Spec, UEFI-PI-Spec}.

The firmware's responsibility is to abstract the diverse implementations of different motherboards (e.g., how the disk controller is wired) and provide a mechanism to load an OS from a disk into RAM. However, relying solely on firmware is often insufficient for a portable operating system:
\begin{itemize}
    \item \textbf{Inconsistent State:} Different firmware implementations may leave the CPU in varying states (e.g., different interrupt configurations or privilege modes).
    \item \textbf{Feature Limitations:} Firmware is designed to be simple and compatible, often leaving the CPU in a conservative, low-feature mode with caches or advanced vector units disabled.
    \item \textbf{Interface Variance:} The method used to retrieve a memory map or video configuration can vary wildly between hardware generations.
\end{itemize}

To solve this, a \textbf{Third-Party Bootloader} is often utilized. This program acts as a "Normalizer" \cite{Limine-Spec}. It knows how to talk to various firmware types and storage devices. Its job is to abstract away the firmware differences, load the kernel file into memory, and pass control to the OS in a unified, predictable manner.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1cm,
    stage/.style={rectangle, draw, fill=white, text width=4cm, align=center, minimum height=1cm},
    arrow/.style={-Latex, thick}
]

\node (power) [stage, fill=gray!10] {\textbf{Hardware Power-On}\\(Reset Vector)};
\node (firmware) [stage, below=of power] {\textbf{System Firmware}\\(BIOS / UEFI)};
\node (loader) [stage, below=of firmware] {\textbf{Bootloader}\\(GRUB / Limine)};
\node (trampoline) [stage, below=of loader] {\textbf{OS Trampoline}\\(Arch Specific)};
\node (kernel) [stage, below=of trampoline, fill=gray!30] {\textbf{Kernel Main}\\(Arch Agnostic)};

\draw[arrow] (power) -- node[right, font=\footnotesize] {Init Platform} (firmware);
\draw[arrow] (firmware) -- node[right, font=\footnotesize] {Load from Disk} (loader);
\draw[arrow] (loader) -- node[right, font=\footnotesize] {Normalize State} (trampoline);
\draw[arrow] (trampoline) -- node[right, font=\footnotesize] {Enable Features (AVX, Paging)} (kernel);

\end{tikzpicture}
\caption{Typical Boot Chain for x86\_64}
\label{fig:boot_chain}
\end{figure}

\subsubsection{The OS-Level Trampoline}
Even with a standardized bootloader, the kernel cannot assume full control immediately. A generic bootloader cannot know the specific internal requirements of the OS. For instance:
\begin{itemize}
    \item The kernel may typically require memory to be mapped to a specific virtual address range (e.g., the higher half).
    \item Specific hardware features (like Floating Point Units or Virtualization Extensions) are often disabled by default to save power and must be explicitly enabled.
    \item The OS must define its own memory protection structures to enforce its specific security model.
\end{itemize}

Therefore, a robust operating system must implement its own initialization stage, effectively an \textbf{OS-Level Bootloader} or "Trampoline." This architecture-specific code is responsible for taking the machine from the normalized state provided by the external bootloader, enabling the specific CPU features required by the kernel, and establishing the runtime environment before passing control to the architecture-agnostic kernel main function.

\section{Memory Preloading and Discovery}
\label{sec:mem_discovery}
One of the first and most critical responsibilities of a kernel during the bootstrap phase is to establish an authoritative map of the system's physical memory. Unlike user-space applications, which simply request memory from the operating system via system calls (e.g., \texttt{malloc} or \texttt{mmap}), the kernel is the manager responsible for fulfilling those requests. Upon entry, the kernel does not know how much RAM is available, where it is located, or which memory ranges are reserved for hardware mapped I/O (MMIO).

This discovery process is not standardized. It is strictly coupled to the target architecture, the silicon vendor, and the residing firmware. Depending on the platform complexity, the kernel may acquire the memory map through one of three primary mechanisms: static definition, firmware interrogation, or hardware description structures.

\subsection{Static Definition}

On strictly embedded architectures (e.g., ARM Cortex-M or AVR), the physical memory layout is immutable. The location and size of SRAM banks, Flash storage, and peripheral registers are defined by the silicon vendor and do not change. In these environments, runtime discovery is redundant.

The memory map is hardcoded directly into the kernel's source code or linker scripts, matching the specific System-on-Chip (SoC) datasheet \cite{ARM-CortexM4-Generic-User-Guide}. The developer explicitly defines the boundary between kernel code, stack, and heap. As illustrated in Figure \ref{fig:cortex_m4_memory}, the address space is rigid; the kernel assumes ownership of specific addresses immediately upon reset without querying external entities.

In this context, the Operating System does not "discover" memory. The kernel code assumes these addresses are valid from the first instruction. For example, a Cortex-M4 kernel may be hardcoded to expect code at 0x00000000 and RAM at 0x20000000. If the software is flashed onto a different chip variant, it will simply fault; flexibility is sacrificed for minimizing initialization overhead.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    % Main memory block style
    memblock/.style={
        rectangle, 
        draw=black, 
        thick,
        minimum width=4.0cm, 
        align=center, 
        anchor=south,
        outer sep=0pt
    },
    % Left-side exploded box style
    sideblock/.style={
        rectangle,
        draw=black,
        minimum width=3.8cm,
        text width=3.5cm,
        align=center,
        font=\sffamily\scriptsize,
        fill=white
    },
    % Address label style
    addr/.style={
        font=\ttfamily\scriptsize
    }
]

% ==========================================
% 1. DRAW MAIN MEMORY STACK
% ==========================================

% Code
\node[memblock, minimum height=1.5cm] (code) at (0,0) {Code\\ \scriptsize 0.5GB};

% SRAM
\node[memblock, minimum height=1.5cm, above=0cm of code] (sram) {SRAM\\ \scriptsize 0.5GB};

% Peripheral
\node[memblock, minimum height=1.5cm, above=0cm of sram] (periph) {Peripheral\\ \scriptsize 0.5GB};

% External RAM
\node[memblock, minimum height=2.0cm, above=0cm of periph] (extram) {External RAM\\ \scriptsize 1.0GB};

% External Device
\node[memblock, minimum height=2.0cm, above=0cm of extram] (extdev) {External Device\\ \scriptsize 1.0GB};

% PPB
\node[memblock, minimum height=0.6cm, above=0cm of extdev] (ppb) {Private peripheral\\bus \scriptsize 1.0MB};

% Vendor
\node[memblock, minimum height=1.0cm, above=0cm of ppb] (vendor) {Vendor-specific\\memory \scriptsize 511MB};


% ==========================================
% 2. ADDRESS LABELS (RIGHT SIDE)
% ==========================================
\node[addr, anchor=south west] at (vendor.north east) {0xFFFFFFFF};
\node[addr, anchor=south west] at (vendor.south east) {0xE0100000};
\node[addr, anchor=south west] at (ppb.south east)    {0xE0000000};
\node[addr, anchor=south west] at (extdev.south east) {0xA0000000};
\node[addr, anchor=south west] at (extram.south east) {0x60000000};
\node[addr, anchor=south west] at (periph.south east) {0x40000000};
\node[addr, anchor=south west] at (sram.south east)   {0x20000000};
\node[addr, anchor=south west] at (code.south east)   {0x00000000};


% ==========================================
% 3. LEFT SIDE BIT-BANDING (MANUAL PLACEMENT)
% ==========================================

% --- SRAM GROUP ---
% Define the target point (Base of SRAM)
\coordinate (sram_target) at (sram.south west);

% 1MB Region Box (Level with SRAM base)
\node[sideblock, anchor=east] (sram_region) at (-2.5, 1.2) {1MB Bit band region};
% Address for 1MB Region
\node[addr, anchor=east] at (sram_region.south west) {0x20000000};
\node[addr, anchor=east] at (sram_region.north west) {0x200FFFFF};

% Alias Box (Floated above)
\node[sideblock, anchor=east, minimum height=0.8cm] (sram_alias) at (-2.5, 2.5) {32MB Bit band alias};
% Address for Alias
\node[addr, anchor=east] at (sram_alias.south west) {0x22000000};
\node[addr, anchor=east] at (sram_alias.north west) {0x23FFFFFF};

% Connect lines for SRAM
\draw (sram_alias.north east) -- (sram_target);
\draw (sram_alias.south east) -- (sram_target);
\draw (sram_region.north east) -- (sram_target);
\draw (sram_region.south east) -- (sram_target);


% --- PERIPHERAL GROUP ---
% Define the target point (Base of Peripheral)
\coordinate (periph_target) at (periph.south west);

% 1MB Region Box (Level with Peripheral base)
\node[sideblock, anchor=east] (periph_region) at (-2.5, 4.2) {1MB Bit band region};
% Address for 1MB Region
\node[addr, anchor=east] at (periph_region.south west) {0x40000000};
\node[addr, anchor=east] at (periph_region.north west) {0x400FFFFF};

% Alias Box (Floated above)
\node[sideblock, anchor=east, minimum height=0.8cm] (periph_alias) at (-2.5, 5.5) {32MB Bit band alias};
% Address for Alias
\node[addr, anchor=east] at (periph_alias.south west) {0x42000000};
\node[addr, anchor=east] at (periph_alias.north west) {0x43FFFFFF};

% Connect lines for Peripheral
\draw (periph_alias.north east) -- (periph_target);
\draw (periph_alias.south east) -- (periph_target);
\draw (periph_region.north east) -- (periph_target);
\draw (periph_region.south east) -- (periph_target);

\end{tikzpicture}
\caption{Cortex-M4 Memory Map with Bit-banding regions (Adapted from \cite{ARM-CortexM4-Generic-User-Guide})}
\label{fig:cortex_m4_memory}
\end{figure}

\subsection{Flattened Device Tree (DTB)}

To allow a single kernel binary to support multiple board configurations without hardcoding, architectures such as ARM64 and RISC-V utilize the \textbf{Flattened Device Tree}. The bootloader passes a pointer to a binary structure (the DTB Blob) which describes the hardware topology, including physical memory ranges and reserved regions.

The DTB format encodes the device tree into a linear, pointerless data structure. It consists of a fixed-size header followed by three variable-sized blocks:
\begin{enumerate}
    \item \textbf{Memory Reservation Block}: Lists physical memory ranges that the kernel must not overwrite (e.g., firmware runtime data).
    \item \textbf{Structure Block}: Describes the device nodes and properties in a linear tree format using token-based tags.
    \item \textbf{Strings Block}: A pool of null-terminated property names referenced by offset.
\end{enumerate}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    block/.style={
        rectangle, 
        draw=black, 
        thick,
        minimum width=5cm, 
        minimum height=0.8cm, 
        align=center, 
        fill=white
    },
    arrow/.style={-Latex, thick, color=black!70}
]

\node[block, fill=gray!10] (header) {\textbf{Header}\\ \scriptsize (Magic, Totalsize, Offsets)};
\node[block, below=0.1cm of header] (reserve) {Memory Reservation Block};
\node[block, below=0.1cm of reserve] (struct) {Structure Block\\ \scriptsize (Nodes \& Properties)};
\node[block, below=0.1cm of struct] (strings) {Strings Block\\ \scriptsize (Property Names)};

% Pointers
\draw[arrow] (header.east) -- ++(0.5,0) |- node[pos=0.7, right, font=\scriptsize] {off\_mem\_rsvmap} (reserve.east);
\draw[arrow] (header.east) -- ++(0.8,0) |- node[pos=0.7, right, font=\scriptsize] {off\_dt\_struct} (struct.east);
\draw[arrow] (header.east) -- ++(1.1,0) |- node[pos=0.7, right, font=\scriptsize] {off\_dt\_strings} (strings.east);

\end{tikzpicture}
\caption{Structure of a Flattened Device Tree Blob}
\label{fig:dtb_structure}
\end{figure}
\todo[inline]{Probably delete this, unless it can be extended or modified to bring more value}

The kernel parses this blob at boot to discover available RAM. Notably, the DTB standard mandates \textbf{Big-Endian} byte ordering. On Little-Endian architectures like x86-64, the kernel must perform byte-swapping when parsing these structures.

\cite{Devicetree-Spec}

\subsection{Firmware Interrogation}

On general-purpose platforms (x86-64), the hardware is modular. The kernel cannot predict the amount of installed RAM or the physical address map. In this scenario, the kernel must query the system firmware directly. This introduces a dependency on the firmware interface:
\begin{itemize}
    \item \textbf{Legacy BIOS:} Requires invoking interrupt vectors (e.g., \texttt{INT 0x15, EAX=0xE820}) to retrieve a list of memory ranges.
    \item \textbf{UEFI:} Requires calling specific boot services (\texttt{GetMemoryMap}) to retrieve descriptors of physical pages and their attributes.
\end{itemize}

\cite{UEFI-Base-Spec}

\subsection{Hardware Abstraction}
How does the kernel handle such diverse set of methods of querying the memory? This is a part of a bigger topic - namely Hardware / Architecture Abstraction - and wil be discussed in more detail in \todo[inline]{add label to hardware abstraction layer / problem}

\subsection{Discovering and Enabling CPU Features}
\label{subsec:cpu_discovery}

Modern Central Processing Units (CPUs) are not monolithic entities with a fixed feature set. Instead, they represent an accumulation of decades of architectural extensions. A generic x86-64 processor guarantees a baseline instruction set (User-level ISA), but specific capabilities regarding vectorization (AVX, AVX-512), cryptography (AES-NI), security (SMEP, SMAP), and performance (PCID, TSC-Deadline) vary significantly between processor generations and manufacturers.
\cite{Intel-AVX}

An operating system kernel cannot blindly execute advanced instructions. Doing so on hardware that lacks support results in an \textit{Invalid Opcode} exception, causing a kernel panic. Therefore, a robust kernel must perform a feature discovery handshake during the early initialization phase.

\subsubsection{Feature Identification}
On the x86 architecture, this discovery is performed via the \texttt{CPUID} instruction. This instruction acts as a query interface where the software loads a leaf index into the \texttt{EAX} register (and optionally a subleaf in \texttt{ECX}) and executes \texttt{CPUID}. The processor returns feature bitmaps and vendor information in the general-purpose registers (\texttt{EAX}, \texttt{EBX}, \texttt{ECX}, \texttt{EDX}).
\cite{Intel-CPUID}

While x86 relies on this dynamic instruction-based discovery, other architectures employ different strategies:
\begin{itemize}
    \item \textbf{ARM64 (AArch64)} utilizes special system registers (e.g., \texttt{ID\_AA64PFR0\_EL1}) that the kernel reads to determine support for floating-point units or cryptographic extensions.
    \item \textbf{RISC-V} typically employs the Device Tree Blob (DTB) or the \texttt{misa} (Machine ISA) Control and Status Register to inform the kernel about supported standard extensions (e.g., Atomics, Floats).
\end{itemize}
\cite{ARM-Arch-Ref, RISCV-Priv-Spec}

\subsubsection{Feature Enablement}
Identifying that a feature exists is often insufficient; the kernel must explicitly enable it. To maintain backward compatibility and minimize power consumption, processors often boot with advanced features disabled.

A prime example is the Floating Point Unit (FPU) and Vector Extensions (SSE/AVX). On x86-64, even if \texttt{CPUID} reports that AVX is supported, attempting to execute a \texttt{VMOVDQA} instruction will fault unless the kernel has:
\begin{enumerate}
    \item Enabled the FPU by clearing the Emulation bit in Control Register \texttt{CR0}.
    \item Enabled SSE by setting the \texttt{OSFXSR} bit in \texttt{CR4}.
    \item Enabled XSAVE/XRSTOR support by setting the \texttt{OSXSAVE} bit in \texttt{CR4}.
    \item Explicitly enabled AVX state saving in the Extended Control Register (\texttt{XCR0}).
\end{enumerate}
\cite{Intel-ControlRegisters}

This enabling phase is critical not only for allowing instruction execution but also for the scheduler. The operating system must know the size of the processor's register state (Context) to correctly save and restore threads during context switches. If AVX-512 is enabled, the context size increases significantly compared to standard SSE, impacting memory usage and context switch latency.

\cite{Intel-XSAVE}

\subsection{Establishment of Basic Communication}

One of the primary objectives when initializing code on a target architecture is to establish an external communication channel. In an emulation environment, this is often achieved by interacting with the emulator's framework (e.g., QEMU utilizes a serial port that can be attached to a Linux shell session). On physical hardware, the developer may need to render fonts on a screen (e.g., using the VGA standard on AMD64 desktop platforms) or implement a basic network stack. The preferred method should support bidirectional communication during the early development stages to facilitate testing and provide input to the kernel. This functionality is primarily required for debugging and testing and as development progresses, it is advisable to disable this communication and associated debug traces via compilation flags. It is crucial to strictly separate debug-only communication from standard output devices, as the former must be disabled in release builds to ensure performance and security.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{res/debug-output.png}
    \caption{QEMU serial-port Communication}
    \label{fig:qemu_comms}
\end{figure}

As shown in Figure \ref{fig:qemu_comms}, alongside typical output to the screen and keyboard input, the kernel transmits text to the QEMU serial port, which is then streamed to the host shell. This approach enables host-side scripts to parse logs, detect bugs or failures, and allow manual inspection of the system state immediately preceding a crash.

\subsection{Enabling Interrupts and Exceptions}
\label{subsec:os-tutorial-interrupts}

Prior to the implementation of memory management (Section \ref{subsec:os_tutorial_mem}), it is essential to establish a basic interrupt handling mechanism. On most platforms, the exception system relies entirely on interrupt mappings. Consequently, without a functional interrupts, the kernel is unable to display debug information on the designated communication device when an error occurs.

Exception handlers should output a descriptive message detailing the failure. This message must include the CPU state, the source of the exception, the instruction pointer where the error occurred, and, if applicable, an error code explaining the cause. An example of a kernel dump following an exception is presented below:

\begin{lstlisting}[caption={Example Kernel Panic on Exception}, label={lst:kernel_panic}]
    [ KERNEL PANIC ] Received exception: 0 (Divide Error (#DE))
    And error: 0
    At instruction address: 0xffffffff8021b856
    rip:                    0xffffffff8021b856
    rflags:                 0x0000000000010246
    rsp:                    0xffffffff8138dc20
    rax:                    0x0000000000000009
    rbx:                    0x000000000006f000
    rcx:                    0x0000000000000000
    rdx:                    0x0000000000000000
    rsi:                    0xffffffff803648f3
    rdi:                    0xffffffff80818c28
    rbp:                    0xffffffff8138dc40
    r8:                     0xffffffff80305d00
    r9:                     0xffffffff80305d59
    r10:                    0xffffffff8138df10
    r11:                    0xffffffff802fdfaf
    r12:                    0x0000000000000010
    r13:                    0x0000000000180000
    r14:                    0x0000000001a91000
    r15:                    0x0000000000000800
    
    RFLAGS:                 0x0000000000010246
\end{lstlisting}

Before initializing Memory Management, exception handling is the primary function required from the interrupt system. At this stage, there is typically no use case for interrupt-driven devices, as the consumers (processes) have not yet been booted, and multicore operations (which require memory for bookkeeping structures) are not active. However, the design of the interrupt subsystem must account for future extensibility and requirements.

\subsubsection{Design Considerations}

It is important to note that low-level, hardware-compliant interrupt handlers differ significantly from standard compiled functions. They often require specific entry and exit instructions, making the runtime swapping of these functions difficult. A robust solution is to create a low-level assembly wrapper that efficiently performs all architecture-dependent operations before invoking a high-level function responsible for the kernel's logic. However, this approach has limitations. The high-level function is typically hardcoded into the low-level handler, preventing runtime reconfiguration.

To address this, a hardware abstraction layer is required to serve as an intermediary between the architecture-specific wrapper and the high-level kernel logic. This abstraction should manage the low-level tables and facilitate the swapping of handlers at runtime. This can be implemented using an array of abstract handlers mapped to hardware interrupts. Furthermore, devices are mapped to hardware interrupts dynamically during runtime based on user configuration and hardware discovery, necessitating the ability to swap drivers and logic dynamically. Without this modularity, the system would require multiple stages of interrupt initialization, multiple architecture-specific handlers, or complex handlers that query the kernel state to determine valid operations (e.g., a page fault handler checking for virtualization support each time is invoked).

It is also critical to design handlers to perform minimal work. This precludes operations such as waiting, sleeping, or extensive tracing. If device handling requires complex logic or a state change, the handler should invoke the scheduler to perform a context switch upon exiting the interrupt. Executing complex logic within the interrupt context blocks other interrupts and may lead to data loss. Consequently, the interrupt abstraction layer must also support task switching.

When implementing this abstraction, three distinct classes of interrupts can be identified:

\begin{itemize}
    \item \textbf{Exceptions} -- Generated by the CPU to indicate specific conditions requiring immediate attention, such as Page Faults, Division by Zero, or invalid instruction operands.
    \item \textbf{Hardware Interrupts} -- Generated by external devices (e.g., Timers, Keyboard, Disk Controller) to communicate efficiently with the kernel without the need for polling.
    \item \textbf{Software Interrupts} -- Initiated by software instructions. For example, on the x86-64 architecture, the instruction \texttt{INT 0x80} triggers an interrupt with the vector number \texttt{0x80}.
\end{itemize}

Architecture-specific details must also be considered. for example, the AMD64 architecture utilizes the legacy PIC \cite{osdev-pic} and the improved, SMP-supporting APIC \cite{osdev-apic}. Finally, to enable Symmetric Multiprocessing (SMP) and utilize multiple cores, the interrupt mechanism serves as the primary method of inter-core communication.

\subsection{Tracing System}

As discussed in Section \ref{subsec:os-tutorial-interrupts}, direct tracing inside interrupt handlers is generally discouraged due to latency concerns. However, in a general context, kernel logs and debug messages must be preserved to a file or terminal. The challenge is that writing to a file or physical device incurs significant latency, while system code must execute as rapidly as possible. To resolve this, the tracing framework must be robust enough to operate in a concurrent environment (handling interrupts and SMP) while decoupling the generation of traces from their output. This can be achieved by allocating a large circular buffer for messages, which is then asynchronously flushed to the output device by a dedicated, low-priority task. Furthermore, to facilitate effective debugging in a multi-threaded and multi-core environment, each log entry should automatically capture essential context metadata, specifically the high-precision timestamp, the current Core ID, and the Process ID.

This goes about the general design, but during initial development system there is no need to have full tracing system, simple print with formatting is enough to proceed.

\subsection{Testing framework}

Once we have two-way communication established with external system and some tracing capabilities enabled it probably good idea to implement some protocol allowing for precise testing implemented code inside the target system and validate the results by external one. Such framework might be utilized to implement regression tests and unit tests running each time we create an PR to trace failures. The test of the code (even data structures) must be condacted on target device as the code might work on some host machine operating with linux but fails locally, because of disabled futures, wrongly implemented system calls, inproper memory management or wrongly generated assembly code. The design should isolate the tests runs so that previous failed tests does not affect new ones. Ideally it could be done by rebooting the system into testing module after each test, but this might be costfull and take long time, but on the other hand false positives might create real loss of time for developers. So test framework must be designed to derive reliable and repeatable results.

\subsection{Memory Management}
\label{subsec:os_tutorial_mem}
TODO: KRYCZKA

\subsubsection{Physical Memory Management}
TODO: KRYCZKA
\subsubsection{Virtual Memory Management}
TODO: KRYCZKA
\subsubsection{Virtual Address Space}
TODO: KRYCZKA
\subsubsection{Kernel Space Allocation API}
TODO: KRYCZKA

\subsection{Discovering External Devices and System Capabilities}

Finally when we have avaialbe fully memory management subsystem we can start discovering the external devices and implementing various subsystems to handle capabilities like storage devices, input devices or networking devices. We will not be focusing on all those as there is no need to support them all to get a minimal working kernel. We will cover only the absolute minimal list of devices we have to support to get working scheduler and userspace programs. 

\subsubsection{Discovery and Abstraction (Drivers)}

The discovery of devices differs between architectures and one must verify the solutions implemented on desired one. For example in most desktop amd64 archtiectures you need to use ACPI \cite{ACPI-spec} subsystem to enumarate some devices like (CPUs, apics, timers on non-pci devices) and afterwards PCI discovery \cite{osdev-pci} must be done to detect devices operating on PCI lines. Obviously this must be done by archtiecture code and the lower level must provide abstractions to the independent kernel code, which it properly understands. Example abstraction of device may look like this:

\begin{lstlisting}[caption={Event Clock Driver Structure}, label={lst:eventClockDriver-tutorial}]
    struct alignas(arch::kCacheLineSizeBytes) EventClockRegistryEntry : data_structures::RegistryEntry {
        /* Clock numbers */
        u64 min_next_event_time_ns;  // Minimum time for the next event in nanoseconds
    
        /* Clock specific data */
        EventClockFlags flags;     // Features of the event clock, e.g., core-local
        CoreMask supported_cores;  // Cores that support this event clock
    
        /* infra data */
        u64 next_event_time_ns;  // Time for the next event in nanoseconds
        EventClockState state;   // Current state of the clock
    
        /* Driver data */
        void *own_data;  // Pointer to the clock's own data, used for callback
    
        /* callbacks */
        struct callbacks {
            // Callback to set next event time
            u32 (*next_event)(EventClockRegistryEntry *, u64);  
            // Callback to set clock state
            u32 (*set_oneshot)(EventClockRegistryEntry *);
            // Callback to set clock state
            u32 (*set_periodic)(EventClockRegistryEntry *);     
            void (*on_entry)(EventClockRegistryEntry *);        // optional
            void (*on_exit)(EventClockRegistryEntry *);         // optional
        } cbs;
    };
\end{lstlisting}

It should define crucial operations for the device type and data describing functionality of the device, allowing to pick proper one by the user or automatically by some algorithms inside the kernel.

\subsubsection{Core Local Storage}

If we are implementing a SMP kernel it will be very handy to prepare infrastructure to acces data local to the core we running as soon as possible to prevent big refactorings in the future. Note that it must be done after initializing the memory and discvoering the CPU topology as we have to allocate the structure for each core individually. In future there will be many structure allocated per-core as to prevent locking and boost performance critical sections and obviously we will have some data that strictly correlated with the core like currently running thread and more so it is not only an optimisation but required step to implement the scheduler. Archtiecture must provide us some mechanism to proceed further for example core unique indexes accessed by some instruction.

\begin{lstlisting}[caption={Example Core Local Usage}, label={lst:core-local}]
    void cdecl_UpdateTcbOnSyscallEntry()
    {
        const auto thread = hardware::GetCoreLocalTcb();
        const u64 t       = TimingModule::Get().GetSystemTime().ReadLifeTimeNs();
        thread->user_time_ns += t - thread->timestamp;
        thread->timestamp = t;
        thread->num_syscalls++;
    };
\end{lstlisting}

\subsubsection{Timing Design}

Before talking about timing devices one must decide what type of kernel you want to implement we have to choose wheter the architure is \texttt{tickless} or \texttt{ticking}. Ticking kernels have some global constant denoting how frequently timing interrupts are scheduled those values are in granularity of milliseconds as more frequent would render the device unusable. On the other hand we have Tickless kernels which does not define periodic interrupts and schedules next timing event based on the needs (sleeping schedule and time slice of current running process). Note that ticking kernels also need to run timing interrupts with bigger granularity if support for nanosleep is needed \ref{subsec:linux-timing}.

\subsubsection{Clocks}

First type of device we will surely need to proceed further as simple clocks capable of counting how much time passed since the start of the system, mainly to measure statistics and keep track of time. For example to support nanosleep we will later need them to place tasks somewhere on the timeline and based on that schedule interrupts for high-precision sleep operations. Without those capabilities we would have to each time we pick tasks to wake up iterate over all sleeping tasks and decrement sleep time left by last interrupt scheduled time. Such approach would have complexity of O(n), what is unacceptable solution for waking up tasks as interrupts must be as fast as possible and timeline is used to sort the tasks in some high performance priority queue data structure (red-black tree is a good choice as it support O(logn) removal in contrast to other implementations and guarantes high stability of the operations). For example on amd64 we can use TSC\cite{IntelManual-TSC} clock for reading how much time passed since the start of the system (sufficient for scheduler) and RTC clock to read real-world time displayed to the user. If multiple timers will be available the kernel should pick the one with best performance-accuracy ratio.

\subsubsection{Event Clocks}

At this stage we should decide about approach we wanna take on designing the timing events logic. This decision depends on architecture we wanna support and capabilities we want to expose to the users.

First question we have to answer is that what granularity of sleep mechanisms you want to support. First simple answer will be that the maximal precision of sleep is around milliseconds (1e-3 of second). Possibly every architecture available on the market will allow to imeplemnt such mechanism, as periodic interrupts with minimal granularity (e.g. each 1ms interrupt fired) are enough and most device will allow that. Otherwise to implement high precision ns/us level sleep we will need timer devices supporting fast reprogramable oneshot interrupt (e.g. LAPIC Timer on amd64) as it is not possible at all to handle periodic interrupt each us. We must be precise and schedule interrupts as needed.

Another important factor we have to take in account is that do you want to support multiple cores? If yes it is preferable to pick timing devices that resides locally on the cores and allows to run independently from other cores allowing to maximize the performance. And on most architectures such devices will be available (e.g. LAPIC Timer on amd64), only some older SMP architectures or exotic embedded ones will be lacking those. In that case the kernel must use some workaround as global timer and use interrupt driven inter core communication to simulate local interrupts.

Based on that we will have to pick a proper timing device to use. If you want to implement only single archtiecture you can pick suitable option and stick with it. In reality if you want to support multiple ones, you will probably end up implementing each of them and adjust the usage based on compiled archtiecture and device capabilities.

Example devices availability scheme by architecture may look like: 
\begin{itemize}
\item SMP fast local one-shot timer -> core local interrupt system + nanosleep mechanism available
\item SMP local periodic only timer -> core local interrupt system + low precision msleep only available
\item SMP non local one-shot timer  -> owner core of timer interrupts propagates the interrupt to target cores + nanosleep
\item SMP non local periodic only timer -> owner core of timer interrupts propagates the interrupt to target cores + low precision msleep only available
\end{itemize}

Most newer arhcitectures will support both local and high precision timers. For example nn amd64 if we work with SMP we have guaranted to have local LAPIC Timers\cite{IntelManual-APIC} (high precision, one shot), what simplifies life a lot.

\subsubsection{Keyboard}
TODO: JAKUB

\subsection{File Systems}
TODO: ADAM
\subsection{Screen Drawing}
TODO: KRYCZKA
\subsection{Scheduling}
TODO: JAKUB

\subsubsection{Process}
TODO: JAKUB
\subsubsection{Thread}
TODO: JAKUB
\subsubsection{Meta-Scheduler}
TODO: JAKUB
\subsubsection{Policies}
TODO: JAKUB
\subsubsection{Context Switch}
TODO: JAKUB
\subsubsection{Context Conversion}
TODO: JAKUB

\subsection{User Space}
TODO: JAKUB

\subsubsection{Syscalls}
TODO: ADAM
\subsubsection{Libc System Headers}
TODO: ADAM
\subsubsection{Conversion to User Space}
TODO: JAKUB

\section{Host Environment -- Working OS}

\subsection{Rootfs}
TODO: ADAM
\subsection{Compiling Userspace Programs}
TODO: ADAM

% ==================================================================

\chapter{Analysis of Existing Solutions}

\section{Linux}

The Linux operating system family is one of the most widely used globally, alongside Windows. The Linux kernel relies on a monolithic architecture. This implies that the kernel has a broad range of responsibilities, while user space contains applications that rely entirely on system capabilities. In practice, all code responsible for memory management, scheduling, interrupt handling, and device drivers resides within the kernel address space. This approach presents various advantages and disadvantages.

\noindent Advantages:
\begin{itemize}
    \item \textbf{Zero-copy and memory efficiency} -- As all components reside in a single address space, data does not need to be copied between distinct address spaces. Passing a pointer is sufficient to transfer data between modules.
    \item \textbf{Minimized context switching} -- Within the single kernel address space, invoking functions from different modules is a direct function call, avoiding the overhead of context switching required by microkernels to access driver services.
    \item \textbf{Simplified source management} -- A single binary structure allows for a unified build scheme. Developing numerous separately compiled services can easily lead to inconsistencies in behavior or data models.
    \item \textbf{Ease of implementation} -- This architecture is often easier to implement for small to mid-sized projects.
\end{itemize}

\noindent Disadvantages:
\begin{itemize}
    \item \textbf{Stability} -- A bug in a single driver, even for an unused device, can compromise the stability of the entire system.
    \item \textbf{Security} -- Vulnerabilities in drivers can potentially expose the kernel address space to malicious user-space execution.
    \item \textbf{Scalability challenges} -- For large-scale projects, strict standards are necessary. Without them, the code base can easily become unmaintainable due to complex webs of references and dependencies.
\end{itemize}

To enhance extensibility and avoid the need for recompilation when adding drivers, Linux employs Loadable Kernel Modules (LKMs) \cite{linux_lkm}. These allow the kernel functionality to be extended dynamically via well-defined interfaces during runtime.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tikzpicture}
            \tikzstyle{every node}=[font=\large]
            \draw [fill={rgb,255:red,242; green,242; blue,242}, thick] (0,0) rectangle (14, 8);
            \node [anchor=north west, font=\bfseries\Large] at (0.3, 7.7) {Kernel Address Space};
            \draw [fill=white, thick, rounded corners=3.0] (0.5, 4.5) rectangle (13.5, 6.5);
            \node [font=\Large, align=center] at (7, 5.5) {Base Linux Kernel Components};
            \tikzstyle{mod}=[draw, fill=white, thick, rounded corners=3.0, minimum width=2cm, minimum height=1.3cm, font=\small]
    
            \node[mod] at (1.5, 3.2) {vfat};
            \node[mod] at (3.7, 3.2) {fat};
            \node[mod] at (5.9, 3.2) {realtek};
            \node[mod] at (8.1, 3.2) {pcspkr};
            \node[mod] at (10.3, 3.2) {kvm-intel};
            \node[mod] at (12.5, 3.2) {kvm};
    
            \node[mod] at (1.5, 1.5) {nvme};
            \node[mod] at (3.7, 1.5) {nvidia};
            \node[mod] at (5.9, 1.5) {rfkill};
            \node[mod] at (8.1, 1.5) {nf\_tables};
            \node[mod] at (10.3, 1.5) {snd};
            \node[mod] at (12.5, 1.5) {\Huge \dots};
    
        \end{tikzpicture}
    }
    \caption{Linux Monolithic Architecture with Loadable Kernel Modules}
    \label{fig:linux_lkm}
\end{figure}

\subsection{Timing}
\label{subsec:linux-timing}

Historically, Linux utilized a strictly periodic tick (defined by hardware), configuring hardware timers to generate interrupts at a fixed frequency (e.g., 100 Hz or 1000 Hz). While this design was straightforward, it imposed limitations on sleep granularity and power efficiency. To address these issues, Linux introduced High Resolution Timers (\texttt{hrtimers}), which allow the kernel to schedule interrupts with nanosecond precision based on immediate needs rather than a fixed cadence. Despite the capabilities of high-resolution timers, the concept of a periodic tick is maintained within the kernel for performance reasons and architectural legacy, by simulating the tick with high precision framework.

This hight precision framework has one major drawback - it can get slow for big number of events as it based on priority queue with O(logn) complexities. Relying exclusively on high-precision mechanisms for every timing event would introduce unacceptable overhead in scenarios involving a massive number of active timers. To mitigate this, Linux maintains the \texttt{jiffies} counter, which increments at the frequency of the system tick. This counter drives the Timer Wheel mechanism \cite{linux_timer_wheel}, a highly efficient algorithm designed for managing low-precision timeouts. The timer wheel offers O(1) complexity for insertion and expiration, making it ideal for subsystems that require the management of thousands or even hundreds of thousands of concurrent events where nanosecond precision is unnecessary. A prime example is the networking stack, which must track timeouts for tens of thousands of open TCP connections. Utilizing high-resolution timers for such a volume would be computationally impossible. The \texttt{jiffies}-based timer wheel allows the system to handle these massive quantities of events with minimal CPU overhead. Consequently, standard API calls such as \texttt{msleep} continue to rely on this coarse-grained, high-performance infrastructure.

\subsection{The Process and Thread Model}
Unlike many other operating systems that distinguish strictly between processes (containers of resources) and threads (units of execution), Linux treats them almost identically. The core data structure is the \texttt{task\_struct} \cite{linux_task}.
\begin{itemize}
    \item A \textbf{Process} is a \texttt{task\_struct} with a unique memory map and file descriptor table.
    \item A \textbf{Thread} is simply a \texttt{task\_struct} created via the \texttt{clone()} system call with flags such as \texttt{CLONE\_VM} and \texttt{CLONE\_FILES}, causing it to share the address space and resources with its parent.
\end{itemize}

\subsection{Scheduler}

Linux implements several scheduling policies \cite{linux_policies}\cite{linux_cfs} operating on task priorities. There are two major classes: \textbf{Real Time}, which operates on priorities 1-99, and \textbf{Fair}, which operates on priority 0. Tasks with higher priorities preempt those with lower priorities. Linux provides six policy classes:

\begin{itemize}
    \item \texttt{SCHED\_DEADLINE} -- Takes precedence over any other policy and provides real-time capabilities.
    \item \texttt{SCHED\_FIFO} -- A real-time policy where a task runs until it yields control or is preempted by a higher-priority task.
    \item \texttt{SCHED\_RR} (Round Robin) -- A real-time policy where each task is assigned a time slice. Upon exhaustion, the task is moved to the end of the queue.
    \item \texttt{SCHED\_OTHER} -- The standard policy for the majority of non-real-time tasks.
    \item \texttt{SCHED\_BATCH} -- Designed for non-interactive tasks that run for longer periods without context switches, tolerating longer scheduling latencies.
    \item \texttt{SCHED\_IDLE} -- Used for very low priority tasks that run only when the system is otherwise idle.
\end{itemize}

From version 2.6.23 \cite{linux_cfs} up to 6.6 \cite{linux_eevdf}, the \texttt{CFS} (Completely Fair Scheduler) was the default scheduler for the \textbf{Fair} class. Starting with version 6.6, the \texttt{EEVDF} (Earliest Eligible Virtual Deadline First) scheduler began replacing CFS.

\subsubsection{Completely Fair Scheduler}
The Linux documentation summarizes the design of the \textbf{CFS} as follows:

\begin{quote}
80\% of CFS's design can be summed up in a single sentence: CFS basically models an “ideal, precise multi-tasking CPU” on real hardware.

“Ideal multi-tasking CPU” is a (non-existent :-)) CPU that has 100\% physical power and which can run each task at precise equal speed, in parallel, each at 1/nr\_running speed. For example: if there are 2 tasks running, then it runs each at 50\% physical power --- i.e., actually in parallel.
    
On real hardware, we can run only a single task at once, so we have to introduce the concept of “virtual runtime.” The virtual runtime of a task specifies when its next timeslice would start execution on the ideal multi-tasking CPU described above. In practice, the virtual runtime of a task is its actual runtime normalized to the total number of running tasks.
\cite{linux_cfs}
\end{quote}

The implementation uses a red-black tree sorted by virtual runtime, representing the execution timeline. The next task selected is the leftmost node in the tree (the task with the least spent execution time) and runs until next another taks becomes the leftmost + some granularity to prevent over-scheduling.

\subsubsection{Earliest Eligible Virtual Deadline First Scheduler}

This algorithm is becoming the new standard for Linux scheduling, addressing shortcomings of the previous \textbf{CFS} implementation. The new approach functions similarly to CFS but operates on deadlines rather than accumulated runtime \cite{linux_eevdf}. This modification allows for better prioritization of latency-sensitive tasks.

\subsection{Memory Management}
\label{subsec:linux_mem}

Linux features centralized memory management with various APIs, including \texttt{kmalloc}, \texttt{kzalloc}, \texttt{vmalloc}, and \texttt{kvmalloc} \cite{linux_mem_interfaces}.

\subsubsection{Physical Memory}
As an architecture-independent kernel, Linux abstracts hardware details. Physical memory is partitioned into zones, each serving a distinct purpose \cite{linux_mem_phys}:

\begin{itemize}
    \item \texttt{ZONE\_DMA} -- Memory suitable for DMA (Direct Memory Access) by devices that cannot access the full addressable range.
    \item \texttt{ZONE\_NORMAL} -- Standard memory directly accessible by the kernel.
    \item \texttt{ZONE\_MOVABLE} -- Similar to \texttt{ZONE\_NORMAL}, but the content of pages in this zone can be migrated to different physical frames (preserving virtual address).
    \item \texttt{ZONE\_DEVICE} -- Memory reserved for specific hardware, such as GPUs, mapped to device drivers.
    \item \texttt{ZONE\_HIGHMEM} -- Memory not covered by permanent kernel mappings, used only by some 32-bit architectures.
\end{itemize}

To enhance multi-core performance, Linux employs a two-step strategy \cite{linux_mem_phys}. It utilizes a global buddy allocator \cite{buddy_allocator} alongside Per-CPU Pagesets (PCP). Allocations are first attempted from the local PCP and if that fails, the system falls back to the global allocator with full synchronization.

\subsubsection{Virtual Memory}

Each process possesses its own Page Table, responsible for mapping virtual addresses to physical ones. Physical frames are acquired from the Physical Memory Manager \ref{subsec:linux_mem}. Virtual address space management for each process utilizes red-black trees to efficiently locate free areas. The kernel address space is mapped into every process. Additionally, Linux implements demand paging, a lazy allocation approach where physical pages are assigned only when the process actually accesses the memory, utilizing the page-fault mechanism.

\subsection{System Interface}

Linux is a Unix-like operating system kernel. The primary interface for system communication is provided via system calls, wrapped by the GNU C Library (glibc).

\subsubsection{Standards}

Linux adheres to the \textbf{POSIX} standard, implementing functions such as \texttt{fork()} and \texttt{exec()}, as well as the concept of \texttt{pthread}.

\subsubsection{Fork Mechanism}

The \texttt{fork()} mechanism is rooted in historical design decisions. While copying the entire parent process state appears inefficient and can introduce performance issues, Linux implements various optimizations, such as Copy-On-Write (\texttt{COW}), to mitigate overhead. This is particularly relevant given that \texttt{exec()} is frequently called immediately after \texttt{fork()}, discarding the copied state. The advantages and disadvantages of this mechanism include:

\noindent Advantages:
\begin{itemize}
    \item Simplified implementation of pipes and pipelining.
    \item Automatic state propagation, eliminating the need to manually specify files or streams in function call.
\end{itemize}

\noindent Disadvantages:
\begin{itemize}
    \item Complex implementation requiring multiple optimizations to remain efficient.
    \item Requires the duplication of process structures, which is inherently complex.
    \item Potential performance bottlenecks in edge cases, such as with large page tables.
    \item Requires careful resource management by the programmer to release resources before execution.
    \item Conceptually difficult for beginners to grasp.
    \item Not portable to architectures lacking an MMU.
\end{itemize}

\section{Minix 3}

Minix 3 is a microkernel-based operating system designed with a strong emphasis on high reliability. Unlike monolithic kernel architectures, Minix 3 relocates the majority of operating system components --- including device drivers, file systems, memory management mechanisms, and scheduling services --- into user-space processes. As a result, the kernel's responsibilities are strictly minimized to functions such as message passing, interrupt handling, and low-level scheduling. Currently, the system supports two hardware architectures: x86 and ARM.

This architectural approach provides several notable advantages. By isolating critical system services in user space, Minix 3 significantly improves fault tolerance, as failures in individual components do not lead to the compromise of the entire system. Moreover, these components can be restarted independently, enabling efficient recovery from faults instead of system-wide crashes. The resulting modularity also simplifies system maintenance and evolution, since services may be updated, replaced, or extended without requiring recompilation of the kernel.

\begin{figure}[htbp]
  \centering
  \resizebox{1\textwidth}{!}{%
    \begin{tikzpicture}
    \tikzstyle{every node}=[font=\large]

    % --- Background Layers ---
    % Kernel Mode Background
    \draw [fill={rgb,255:red,242; green,242; blue,242}] (2.5,10.75) rectangle (17.25,8.25);
    % User Mode Background
    \draw [fill={rgb,255:red,242; green,242; blue,242}] (2.5,18.5) rectangle (17.25,11);
    
    % Horizontal Separators
    \draw (2.5,16) -- (17.25,16);
    \draw (2.5,13.5) -- (17.25,13.5);

    % --- User Processes Layer (Layer 4) ---
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,18) rectangle node {\large User Processes} (7,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,18) rectangle node {\large Shell} (9.5,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,18) rectangle node {\large Make} (12,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,18) rectangle node {\large User} (14.5,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,18) rectangle node {\LARGE ...} (17,16.5);

    % --- Server Processes Layer (Layer 3) ---
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,15.5) rectangle node {\large Server Processes} (7,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,15.5) rectangle node {\large File} (9.5,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,15.5) rectangle node {\large PM} (12,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,15.5) rectangle node {\large Sched} (14.5,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,15.5) rectangle node {\LARGE ...} (17,14);

    % --- Device Driver Layer (Layer 2) ---
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,13) rectangle node {\large Device Processes} (7,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,13) rectangle node {\large Disk} (9.5,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,13) rectangle node {\large TTY} (12,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,13) rectangle node {\large Net} (14.5,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,13) rectangle node {\LARGE ...} (17,11.5);

    % --- Kernel Layer (Layer 1) ---
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,10.25) rectangle node {\large Kernel} (7.25,8.75);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.75,10.25) rectangle node {\large Clock Task} (12,8.75);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.5,10.25) rectangle node {\large System Task} (17,8.75);
    
    % --- Labels and Brackets ---
    % User Mode bracket
    \draw [thick] (2.0,11) -- (2.0,18.5);
    \draw [thick] (2.0,11) -- (2.2,11);
    \draw [thick] (2.0,18.5) -- (2.2,18.5);
    \node[left, align=center] at (1.8,14.75) {\large \textbf{User}\\\large \textbf{Mode}};
    
    % Kernel Mode bracket
    \draw [thick] (2.0,8.25) -- (2.0,10.75);
    \draw [thick] (2.0,8.25) -- (2.2,8.25);
    \draw [thick] (2.0,10.75) -- (2.2,10.75);
    \node[left, align=center] at (1.8,9.5) {\large \textbf{Kernel}\\\large \textbf{Mode}};
    
    % --- Right Side Layer Labels ---
    \node[right] at (17.5, 17.25) {\large \textbf{Layer 4}};
    \node[right] at (17.5, 14.75) {\large \textbf{Layer 3}};
    \node[right] at (17.5, 12.25) {\large \textbf{Layer 2}};
    \node[right] at (17.5, 9.5)   {\large \textbf{Layer 1}};
    
    \end{tikzpicture}
  }%
\caption{The Minix 3 Microkernel Architecture}
\label{fig:minix_architecture}
\end{figure}

\subsection{Scheduler}
Adhering to the microkernel philosophy, Minix 3 separates scheduling policy from the low-level context switching mechanism. While the kernel remains responsible for the mechanics of context switching and interrupt handling, scheduling decisions are delegated to a dedicated user-space server, referred to as the \texttt{Sched} server.

The scheduler employs a multi-level priority round-robin algorithm \cite{minix_sched}. The system maintains sixteen priority queues organized hierarchically. Kernel tasks occupy the highest priority levels, followed by device drivers, system servers, and user applications. The lowest priority queue is reserved exclusively for idle tasks. At any scheduling decision point, the scheduler selects the next runnable process from the highest non-empty priority queue.

Every process is assigned a specific time quantum, representing the maximum CPU time allowed before preemption occurs. Upon the exhaustion of a process's time quantum, the kernel notifies the scheduler server. To penalize CPU-bound tasks and maintain system responsiveness, the scheduler lowers the priority of such processes, moving them to a lower queue.

To prevent indefinite starvation of low-priority processes, a \texttt{balance\_queues} function executes periodically at five-second intervals \cite{minix_sched_bq}. This function re-evaluates process states and promotes tasks that have not received CPU time for an extended period, thereby ensuring that interactive applications remain responsive.

\subsection{Timing}
The timing subsystem in Minix 3 is responsible for maintaining system time, coordinating scheduling intervals, and handling alarms and timers. The core timing functionality is encapsulated within the Clock Task, a kernel-level process that manages time-related operations.

\subsubsection{Hardware Abstraction}
Minix 3 provides an abstraction layer over platform-specific timing hardware. On x86 systems, this includes the legacy Programmable Interval Timer (PIT) \cite{osdev-pit}, while ARM-based platforms rely on board-specific timer implementations \cite{minix_arch_clock}. During system initialization, the kernel configures the selected hardware timer to generate periodic interrupts. If not specified in the kernel environment, the default interrupt frequency is architecture-dependent: 60 Hz on x86 and 1000 Hz on ARM systems.

\subsubsection{Timers and Alarms}
The kernel maintains a \texttt{clock\_timers} queue to manage synchronous timers and alarms \cite{minix_kernel_clock}. Upon receipt of a hardware timer interrupt, the \texttt{timer\_int\_handler} routine is executed. If the expiration time of the next scheduled timer in the \texttt{clock\_timers} queue has been reached, the kernel executes the associated callback function. 

In the case of system alarms, this callback is the \texttt{cause\_alarm} function \cite{minix_do_setalarm}. This function sends a notification message from the Clock Task to the requesting process (e.g., the Process Manager or Scheduler), informing it that the requested time interval has elapsed. This mechanism allows user-space servers to perform periodic tasks, such as the scheduler's queue balancing.

\subsubsection{Real-Time Clock (RTC)}
Persistent wall-clock time is provided by a dedicated user-space driver, \texttt{readclock}. This driver interfaces directly with the CMOS Real-Time Clock (RTC) to retrieve the current date and time. During system initialization, this information is communicated to the Process Manager to establish the system's initial time reference \cite{minix_readclock}.

\subsection{Memory Management}
Memory management in Minix 3 is primarily handled by the user-space Virtual Memory (VM) server. The kernel retains control over the hardware Memory Management Unit (MMU) and address space switching but delegates higher-level memory policies to the VM server.

\subsubsection{VM Server}
The VM server is responsible for memory region allocation, page table administration, and page fault handling. It implements a region-based memory model in which a process address space is divided into contiguous regions with defined access permissions (read, write, and execute). To efficiently manage free and allocated memory regions, the VM server uses \textbf{AVL trees} \cite{minix_vm}.

\subsubsection{Allocation and Paging}
Internal memory allocation for kernel data structures and VM metadata is performed using a slab allocator \cite{slab_allocation}. Minix 3 supports demand paging: when a process attempts to access an unmapped virtual address, a page fault is raised. The kernel intercepts this exception and forwards the fault information to the VM server, which resolves it by mapping an appropriate physical frame or retrieving the required page from secondary storage \cite{shenoy_lecture}.

\subsubsection{Memory Grants}
Due to the strict isolation of address spaces in the microkernel architecture, processes cannot directly access the memory of others. To facilitate the exchange of large data structures without violating protection boundaries, Minix 3 provides a capability-based mechanism known as \texttt{Memory Grants}, accessed via the \texttt{safecopy} API \cite{minix_safecopy}.

A process (the grantor) dynamically generates a grant capability that explicitly permits a specific peer process (the grantee) to read from or write to a designated memory range. The grantee utilizes this grant ID to request a data transfer from the System Task. The kernel validates the grant permissions before performing the copy operation between the disjoint address spaces. This mechanism ensures that drivers and servers can operate on client buffers while preventing unauthorized access to arbitrary memory locations.

\subsection{System Interface}
Although Minix 3 conforms to the POSIX standard, its underlying system interface differs substantially from monolithic operating systems due to its message-based microkernel architecture.

\subsubsection{IPC Primitives}
Inter-process communication (IPC) is built upon four fundamental primitives provided by the kernel. For each operation, the kernel mediates communication by copying message contents directly from the sender's address space to that of the receiver.
\begin{itemize}
    \item \texttt{send(dest, \&message)}: Sends a message to the specified destination. The sender blocks if the destination is not ready to receive.
    \item \texttt{receive(source, \&message)}: Blocks the calling process until a message is received from the specified source.
    \item \texttt{sendrec(src\_dest, \&message)}: A combined operation that sends a message and then blocks until a reply is received from the same process.
    \item \texttt{notify(dest)}: A non-blocking signaling mechanism used to inform a destination of an event without transmitting a data payload.
\end{itemize}

\subsubsection{Communication Policies}
IPC in Minix 3 is governed by these strict policies:
\begin{enumerate}
    \item \textbf{Access Control:} Processes may exchange messages only with explicitly authorized peers.
    \item \textbf{Hierarchical Communication:} Message flow generally follows the system's layered architecture (Layer 4 $\rightarrow$ Layer 3 $\rightarrow$ Layer 2).
    \item \textbf{User-Space Isolation:} User processes are prohibited from communicating directly with one another or with the kernel.
\end{enumerate}

\subsubsection{System Call Implementation}
In Minix 3, system calls are not implemented as direct kernel invocations. Instead, standard library functions (e.g., \texttt{read}, \texttt{fork}) serve as wrappers that construct a message containing the call arguments and transmit it to the appropriate server using the \texttt{sendrec} primitive \cite{minix_ipc}. If the server needs to perform a privileged operation, it forwards the request to the System Task on behalf of the calling process, as servers possess higher privileges than user processes.

\subsubsection{Server Delegation}
System calls are dispatched to specialized servers according to their functionality \cite{minix_callnr}:
\begin{itemize}
    \item \textbf{Process Manager (PM):} Responsible for process lifecycle operations such as \texttt{fork}, \texttt{exec}, \texttt{exit}, and \texttt{wait}.
    \item \textbf{Virtual File System (VFS):} Handles file-related operations including \texttt{open}, \texttt{read}, \texttt{write}, and \texttt{stat}.
\end{itemize}
The kernel acts solely as a transport mechanism and remains agnostic to the semantics of the system call. After processing a request, the server returns a reply message to the calling process, thereby unblocking it and delivering the result.


% ==================================================================

\chapter{Supporting Multiple Hardware Platforms} 
TODO KTOKOLWIEK

\section{HAL -- Hardware Abstraction Layer} 
TODO KTOKOLWIEK

\section{x86-64 Support} 
TODO KTOKOLWIEK
\subsection{Technical Debt}
TODO KTOKOLWIEK
\subsection{Common Pitfalls}
TODO KTOKOLWIEK


% ==================================================================

\chapter{Our Implementation} 
TODO KTOKOLWIEK

\section{Bootloader}
\todo[inline]{Loader32 and Loader64 are very simplified here. This is very much in progress. Don't read it}

Before the architecture-agnostic kernel can commence execution, the underlying hardware must be brought to a known, deterministic state. As discussed in Section \ref{subsec:theory_bootloader}, the initialization protocols differ significantly depending on the specific hardware architecture and the firmware interface.

For the AlkOS kernel on the x86-64 architecture, we rely on the \textbf{Multiboot2} specification. This allows us to utilize a battle-tested external bootloader, such as GRUB, to handle the intricacies of storage drivers, filesystem parsing, and memory map retrieval. GRUB loads our kernel binary into memory and transfers control to our entry point.

However, the state provided by Multiboot2 is insufficient for the immediate execution of our C++ kernel. Upon entry, the system is in \textbf{32-bit Protected Mode}, paging is disabled, interrupts are disabled, and the stack pointer is undefined. Furthermore, the kernel is loaded at a low physical address, whereas AlkOS is designed as a higher-half kernel.

To bridge this gap, we implemented a two-stage internal bootloader: \textbf{Loader32} and \textbf{Loader64}.

\subsection{Multiboot2 Entry and Loader32}

The entry point of the OS is defined in assembly language within the \texttt{Loader32} section. The primary responsibility of this stage is to transition the CPU from the 32-bit legacy environment into 64-bit Long Mode.

The sequence of operations is as follows:
\begin{enumerate}
    \item \textbf{Stack Setup:} A temporary stack is established to allow C-like operations and basic function calls.
    \item \textbf{Multiboot Validation:} The code verifies the Magic Number in the \texttt{EAX} register to ensure the kernel was loaded by a compliant bootloader. The pointer to the Multiboot information structure (stored in \texttt{EBX}) is saved for later processing.
    \item \textbf{Paging Initialization:} Long Mode requires paging to be enabled. Loader32 constructs a temporary \textbf{Page Global Directory (PML4)}. To allow the code to continue executing seamlessly after enabling paging, we employ \textit{identity mapping} for the lower megabytes of memory (where the loader resides) and simultaneously map the higher-half virtual addresses to the same physical location.
    \item \textbf{Long Mode Enablement:} The transition is performed by setting the \texttt{PAE} (Physical Address Extension) bit in control register \texttt{CR4}, setting the \texttt{LME} (Long Mode Enable) bit in the \texttt{EFER} Model Specific Register (MSR), loading the address of the PML4 into \texttt{CR3}, and finally enabling paging via \texttt{CR0}.
    \item \textbf{GDT Update:} A temporary 64-bit Global Descriptor Table is loaded to define the code segments required for 64-bit execution.
\end{enumerate}

Once these steps are complete, the code performs a long jump to the 64-bit entry point, effectively flushing the CPU pipeline and entering Long Mode.

\subsection{Loader64}

At this stage, the CPU is operating in 64-bit mode, but the environment is still bare. The \texttt{Loader64} is responsible for enabling specific CPU features that the C++ kernel and standard library rely on.

\subsubsection{Feature Discovery and Enablement}
Modern compilers and C++ standard libraries frequently utilize SIMD instructions (SSE/AVX) for optimization, even in non-mathematical code (e.g., \texttt{memcpy} or string operations). However, these features are disabled by default upon boot. Attempting to execute an SSE instruction before enabling the unit results in an \textit{Invalid Opcode} exception.

Loader64 performs the following:
\begin{itemize}
    \item Checks for CPUID availability and verifies supported features.
    \item Enables the \textbf{FPU} (Floating Point Unit) via the \texttt{CR0} register.
    \item Enables \textbf{SSE} (Streaming SIMD Extensions) by setting the OSFXSR bit in \texttt{CR4}.
    \item If supported, enables \textbf{AVX} (Advanced Vector Extensions) by manipulating the XCR0 extended control register.
\end{itemize}

\subsubsection{Runtime Environment Setup}
Before calling the main kernel function, Loader64 performs the final cleanup:
\begin{itemize}
    \item \textbf{High-Memory Stack:} The stack pointer (\texttt{RSP}) is moved to the virtual higher-half address space, unmapping the identity-mapped low memory to catch null-pointer dereferences early.
    \item \textbf{BSS Initialization:} The \texttt{.bss} section (containing uninitialized static variables) is zeroed out, as the bootloader does not guarantee this operation.
    \item \textbf{Global Constructors:} The loader iterates through the \texttt{.init\_array} section to execute C++ global constructors.
\end{itemize}

Finally, Loader64 parses the Multiboot2 tags to extract the memory map and framebuffer information, converts them into AlkOS-internal structures, and calls \texttt{kernel\_main}.

\section{Memory Management}
\label{sec:memory}
TODO KRYCZKA

\section{Interrupts}
\label{sec:interrupts}

Interrupt handling is a critical component of any operating system. Without interrupts, the system would rely on manually polling devices or checking states, which is inefficient and wasteful of resources. Furthermore, interrupts enable the system to be significantly more responsive. For example, when a keyboard key is pressed, the operating system may need to immediately switch execution to the thread responsible for consuming the input, rather than waiting for the current task to finish. More broadly, the OS utilizes the interrupt mechanism to perform context switches. If a task exceeds its allocated execution time, timing devices such as the LAPIC Timer trigger an interrupt, allowing the scheduler to preempt the current task and grant CPU time to others.

\subsection{x86-64 Interrupts}

\subsubsection{Interrupt Handling}
On the x86-64 architecture, every interrupt is assigned a unique number (vector) which maps directly to an entry in the \textbf{Interrupt Descriptor Table} (IDT) \cite{IntelManual-Interrupts}. This table contains instructions for the CPU on how to react to specific interrupts. The entry layout is as follows:

\begin{lstlisting}[caption={IDT Entry Layout}, label={lst:idtEntry}]
  enum class IdtPrivilegeLevel : u8 { kRing0 = 0, kRing1 = 1, kRing2 = 2, kRing3 = 3 };

  struct PACK IdtEntryFlags {
      IdtGateType type : 4;
      u8 zero : 1;
      IdtPrivilegeLevel dpl : 2;
      u8 present : 1;
  };
  struct PACK IdtEntry {
      u16 isr_low;    // The lower 16 bits of the ISR's address
      u16 kernel_cs;  // The GDT segment selector that the CPU
                      //  will load into CS before calling the ISR
      u8 ist;         // The IST in the TSS that the CPU will load into RSP
      IdtEntryFlags attributes;  // Type and attributes
      u16 isr_mid;  // The higher 16 bits of the lower 32 bits of the ISR's address
      u32 isr_high; // The higher 32 bits of the ISR's address
      u32 reserved; // Set to zero
  };
\end{lstlisting}

The most critical component of the IDT entry is the address of the function to be invoked (split into \texttt{isr\_low}, \texttt{isr\_mid}, and \texttt{isr\_high}). Another important field is \texttt{kernel\_cs}, which specifies the code segment \cite{IntelManual-Segments} loaded before executing the handler. In x86-64, there are four privilege levels (Rings). We assume the kernel always operates in Ring 0 (the most privileged level). Therefore, the Ring 0 kernel code segment is always written to this field. Conversely, the \texttt{IdtPrivilegeLevel dpl} field specifies the minimal privilege level required to trigger the interrupt via software, which is essential for implementing system calls (syscalls) invoked from userspace.

\subsubsection{Interrupt Service Routines}

Functions handling interrupts differ significantly from standard functions generated by the compiler. When the CPU calls an interrupt handler, it first switches the stack pointer to the kernel stack if a privilege level change occurs (e.g., Ring 3 to Ring 0). This transition is managed via the Task State Segment (TSS) mechanism (for details, refer to \cite{osdev-tss}). The CPU then changes the code segment as specified in the \textbf{IDT entry} (Listing \ref{lst:idtEntry}). Subsequently, it pushes the state of the interrupted procedure onto the stack.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=0cm,
      start chain=going below,
      stacknode/.style={
          draw, 
          minimum width=5cm, 
          minimum height=1cm, 
          outer sep=0pt, 
          font=\ttfamily
      },
      labelnode/.style={
          minimum height=1cm,
          font=\footnotesize\sffamily,
          anchor=east
      }
  ]
  
  \node (highmem) at (0, 1) {Higher address (High Memory)};
  \draw[->] (highmem) -- (0, 0.2);

  \node [stacknode, on chain, fill=gray!10] (ss) {Stack Segment (SS)};
  \node [stacknode, on chain, fill=gray!10] (rsp) {Stack Pointer (RSP)};
  \node [stacknode, on chain, fill=gray!10] (rflags) {RFLAGS};
  \node [stacknode, on chain, fill=gray!10] (cs) {Code Segment (CS)};
  \node [stacknode, on chain, fill=gray!10] (rip) {Instruction Pointer (RIP)};
  \node [stacknode, on chain, fill=gray!10] (error) {Error Code (optional)};
  \draw[<-, thick, red] (error.east) -- ++(1.5,0) node[right, text=red] {Current RSP};

  \node [below=0.5cm of error] (lowmem) {Lower address (Low Memory)};
  \draw[->] (lowmem.north) -- (error.south);

  \end{tikzpicture}
  \caption{Interrupt Service Routine stack layout on entry}
  \label{fig:stackframe}
\end{figure}

The layout illustrated in Figure \ref{fig:stackframe} is known as the \textbf{Interrupt Frame}. This structure is fundamental to the kernel architecture, serving as the basis for context switching, context conversion, and jumping to userspace (Ring 3). A key distinction is that an \textbf{ISR} must return using the special instruction \textbf{IRETQ} \cite{IntelManual-Interrupts}, which reverses the actions described above, including restoring the privilege level and code segment. 

A significant challenge with standard compiler-generated functions is that they may modify the stack frame (prologue/epilogue) in ways that interfere with the hardware-defined layout. To maintain full control and prevent stack corruption, we implement assembly wrappers. These wrappers perform the necessary architecture-specific actions before invoking the architecture-agnostic interrupt handling code defined in C++.

\begin{lstlisting}[style=nasmstyle, caption={Assembly ISR wrapper}, label={lst:isr_asm}]
%macro context_switch_if_needed 0
  cmp rax, 0
  je .done                         ; Omit context switch if there is no need

  mov r13, rax                     ; save next TCB

  mov rdi, r13
  mov rsi, rsp
  call cdecl_ContextSwitchOnInterrupt

  mov rsp, [r13+Thread.kernel_stack]   ; Change the stack
.done:

  load_user_gs_if_needed

  pop_all_regs                    ; Restore registers.
  add rsp, _all_reg_size          ; Deallocate register save space.
  add rsp, 8                      ; Pop error code.
  iretq
%endmacro

; Macro for hardware or software interrupts.
; Calls a handler with the signature 'void handler(u16 lirq, void* frame)'.
%macro interrupt_wrapper 3 ; %1: Logical IRQ, %2: idt idx %3: C handler function
isr_wrapper_%+%2:
    push 0                          ; Push a dummy error code for unification.
    sub rsp, _all_reg_size          ; Allocate space for saving registers.
    push_all_regs                   ; Save registers.

    load_kernel_gs_if_needed

    cld                         ; Clear direction flag for string operations.
    mov rdi, %1                 ; Arg1: mapped lirq number.
    mov rsi, rsp                ; Arg2: pointer to stack frame.
    call %3                     ; Call the specific ISR handler.

    context_switch_if_needed
%endmacro
\end{lstlisting}

\subsubsection{Synchronization}

Since an interrupt can potentially trigger a context switch, synchronization is crucial. This applies to both kernel-space threads at any moment of their lifetime and userspace programs executing system calls. It would be catastrophic if a timer interrupt forced a context switch while the kernel was in the middle of updating scheduler structures or memory tables. Therefore, even on a single-core system, synchronization must be enforced. This is achieved by disabling hardware interrupts (using \textbf{CLI} and \textbf{STI} instructions on x86-64) during critical sections.

\subsection{Unified Interrupt Frame}
\label{subsec:unifiedFrame}

To simplify interrupt handling, we introduced a unified frame structure based on the hardware Interrupt Frame (Figure \ref{fig:stackframe}). Since the state of a thread must be preserved before a context switch, all general-purpose registers must be saved. To achieve this efficiently, these registers are pushed onto the stack by the assembly wrapper, creating the \textbf{Unified Interrupt Frame}.

The x86-64 architecture introduces an inconsistency in the stack layout depending on the interrupt source. Certain exceptions, such as Page Faults (vector 14) or General Protection Faults (vector 13), automatically push an error code onto the stack by the CPU. Hardware interrupts and other exceptions do not. To use a single, unified C++ structure for all interrupt handling (\texttt{IsrErrorStackFrame}), the assembly entry wrappers must normalize the stack. For vectors that do not produce a hardware error code, the wrapper explicitly pushes a dummy value (typically 0) before saving the general-purpose registers (as seen in Listing \ref{lst:isr_asm}). This ensures that the stack pointer is always aligned correctly and points to a uniform structure when the C++ handler is invoked.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=0cm,
      start chain=going below,
      stacknode/.style={
          draw, 
          minimum width=5cm, 
          minimum height=1cm, 
          outer sep=0pt, 
          font=\ttfamily
      },
      labelnode/.style={
          minimum height=1cm,
          font=\footnotesize\sffamily,
          anchor=east
      }
  ]
  
  \node (highmem) at (0, 1) {Higher address (High Memory)};
  \draw[->] (highmem) -- (0, 0.2);

  \node [stacknode, on chain, fill=gray!10] (ss) {Stack Segment (SS)};
  \node [stacknode, on chain, fill=gray!10] (old_rsp) {Old Stack Pointer (RSP)};
  \node [stacknode, on chain, fill=gray!10] (rflags) {RFLAGS};
  \node [stacknode, on chain, fill=gray!10] (cs) {Code Segment (CS)};
  \node [stacknode, on chain, fill=gray!10] (rip) {Instruction Pointer (RIP)};
  \node [stacknode, on chain, fill=gray!10] (error) {Error Code (or Dummy)};
  \node [stacknode, on chain, fill=gray!10] (genregs) {General Registers (RDI, RSI, ...)};
  \node [stacknode, on chain, fill=gray!10] (rax) {Last Saved Register};
  
  \draw[<-, thick, red] (rax.east) -- ++(1.5,0) node[right, text=red] {RSP (New Stack Pointer)};

  \node [below=0.5cm of rax] (lowmem) {Lower address (Low Memory)};
  \draw[->] (lowmem.north) -- (rax.south);

  \end{tikzpicture}
  \caption{Unified Interrupt Frame (IsrErrorStackFrame)}
  \label{fig:unifiedstackframe}
\end{figure}

\subsection{Context Switch}

As previously mentioned, the OS utilizes the interrupt mechanism to perform task switching. The interrupt mechanism handles most of the necessary context-switching operations automatically:
\begin{itemize}
\item Manages ring permissions (automatically swapping code and stack segments).
\item Automatically swaps the stack from user stack to kernel stack (using the TSS).
\item Restores \textbf{RFLAGS} (which includes the \textbf{interrupt flag} responsible for enabling/disabling hardware interrupts).
\item Jumps back to the code address pointed to by \textbf{RIP}.
\end{itemize}

To switch contexts, we must ensure the current thread's state is preserved in a \textbf{Unified Interrupt Frame} (Figure \ref{fig:unifiedstackframe}) on its kernel stack. Additionally, a valid frame must exist for the target thread. If a thread has run previously, it will have saved its frame naturally during its last preemption. However, for a new thread that has never executed, this frame must be constructed manually. 

We assume that all threads begin execution in kernel space before eventually jumping to user space. Since all interrupt handling occurs within the kernel, the interrupt frame always resides on the kernel stack. Therefore, we can initialize the kernel stack of the new thread with a fabricated frame before the context switch. The initialization procedure is as follows:

\begin{lstlisting}[caption={Thread stack initialization}, label={lst:threadStack}]
void InitializeThreadStack(void **stack, const Sched::Task &task)
{
    /* NOTE: Thread entry always starts in Kernel Code */
    auto stack_top = static_cast<byte *>(*stack) - sizeof(IsrErrorStackFrame);
    auto frame     = reinterpret_cast<IsrErrorStackFrame *>(stack_top);

    memset(stack_top, 0, sizeof(IsrErrorStackFrame));

    /* Initialize IsrErrorStackFrame (Hardware Part + Error Code) */
    frame->isr_stack_frame.rip    = reinterpret_cast<u64>(task.func);
    frame->error_code             = 0; // Dummy error code
    frame->isr_stack_frame.cs     = static_cast<u64>(cpu::GDT::kKernelCodeSelector);
    frame->isr_stack_frame.rflags = kInitialRFlags;
    frame->isr_stack_frame.rsp    = reinterpret_cast<u64>(*stack);
    frame->isr_stack_frame.ss     = static_cast<u64>(cpu::GDT::kKernelDataSelector);

    /* Initialize function arguments (Software Part / Registers) */
    if (task.args_count > 0) {
        frame->registers.rdi = task.args[0];
    }

    // ... other args

    if (task.args_count > 5) {
        frame->registers.r9 = task.args[5];
    }

    /* Save adjusted stack address */
    *stack = reinterpret_cast<void *>(stack_top);
}
\end{lstlisting}

With both stack frames prepared, the context switch logic is straightforward: swap the \textbf{RSP} (current stack pointer) to the kernel stack of the target thread and execute the \textbf{IRETQ} instruction. This instruction restores the state and resumes execution, as demonstrated in the \textbf{context\_switch\_if\_needed} macro (Listing \ref{lst:isr_asm}).

\subsection{Jumping to Userspace}

Transitioning to userspace is performed similarly to a thread context switch. We construct an artificial interrupt frame, but in this case, the code segment and stack segment within the frame are set to the user space selectors, and the stack pointer is set to the user space stack. The implementation is shown below:

\begin{lstlisting}[caption={Userspace Jump C++ code}, label={lst:userSpaceJump}]
extern "C" void cdecl_JumpToUserSpaceEntry(void *addr, IsrStackFrame *frame)
{
    ASSERT_NOT_NULL(addr);
    ASSERT_NOT_NULL(frame);
    ASSERT_NOT_NULL(hardware::GetCoreLocalTcb());

    auto thread          = hardware::GetCoreLocalTcb();
    thread->kernel_stack = thread->kernel_stack_bottom; // reset kernel stack

    frame->rip    = reinterpret_cast<u64>(addr);
    frame->cs     = static_cast<u64>(cpu::GDT::kUserCodeSelector);
    frame->rflags = static_cast<u64>(kInitialRFlags);
    frame->rsp    = reinterpret_cast<u64>(thread->user_stack_bottom);
    frame->ss     = static_cast<u64>(cpu::GDT::kUserDataSelector);

    const u64 t            = TimingModule::Get().GetSystemTime().ReadLifeTimeNs();
    thread->kernel_time_ns = t - thread->timestamp;
    thread->timestamp      = t;

    SetThreadGs(thread);
    __asm__ volatile("swapgs" ::: "memory");
}
\end{lstlisting}

\begin{lstlisting}[style=nasmstyle, caption={Userspace Jump NASM code}, label={lst:userSpaceJumpNasm}]
; c_decl
; void JumpToUserSpace(void (*func)(), void* arg)
;   RDI = func
;   RSI = arg
; Note: Caller is responsible for ensuring proper environment before calling (disabling IRQs)
JumpToUserSpace:
    sub rsp, _jump_userspace_stack_space
    push rsi
    ; aligned properly

    mov rsi, rsp
    add rsi, 8
    call cdecl_JumpToUserSpaceEntry

    xor rax, rax
    mov rax, _user_data_selector
    mov ds, ax
    mov es, ax
    mov fs, ax
    mov gs, ax

    pop rsi
    mov rdi, rsi ; prepare void* arg for func if needed
    iretq
\end{lstlisting}

\subsection{Interrupts Hardware Abstraction}

To maintain architectural independence, the Interrupt Table is abstracted. Upon any interrupt, control is passed to the \textbf{Logical Interrupt Table (LIT)}, which is responsible for executing the necessary actions common to all interrupts. The architecture-specific code establishes the mapping between the hardware interrupts and the logical interrupt table, from that point forward, interrupt management is handled entirely by the LIT. Responsibilities of the LIT include:

\begin{itemize}
\item Managing interrupt handlers, allowing the kernel to modify interrupt responses dynamically.
\item Tracking interrupt nesting levels.
\item Collecting statistics, such as interrupt counts per thread, kernel time, and user space time.
\item Interacting with hardware interrupt drivers such as the \textbf{PIC} or \textbf{APIC}.
\item Masking (blocking) individual interrupts.
\end{itemize}

The LIT handling procedures may return a pointer to the next thread scheduled for execution. If such a pointer is returned, the hardware wrapper performs the context switch upon exit. This mechanism enables the system to react rapidly to state changes or preempt the current thread via a timer interrupt.
\section{Timing}
\label{sec:timing}

Prior to the implementation of the Scheduler, and alongside functional memory management and interrupt handling, it was necessary to establish the infrastructure and drivers for timing mechanisms. This includes devices capable of measuring the system's uptime, referred to as \textbf{Clocks}. Additionally, the system requires devices capable of generating interrupts at specific intervals. These are essential for preempting the currently executing thread if it fails to yield the CPU voluntarily, thereby ensuring fair scheduling. Such devices are referred to as \textbf{Event Clocks}. To unify the timing subsystem, all devices and mechanisms rely on nanoseconds as the fundamental unit of time abstraction.

\subsection{Infrastructure}

The architecture-specific code is responsible for detecting available hardware and registering it within the central timing infrastructure, specifically the \textbf{ClockRegistry} and \textbf{EventClockRegistry}. Subsequently, architecture-agnostic components, such as the \textbf{ACPI} subsystem, may register additional clocks by parsing system description tables. Finally, the architecture-defined functions \texttt{hal::PickSystemClockSource()} and \texttt{hal::PickSystemEventClockSource()} are invoked during the initialization of the timing module to select the optimal sources for each purpose. The infrastructure allows the scheduler to implement either a tick-based or a tickless strategy.

\subsection{Clocks}

Clock Drivers are defined by the following structure:

\begin{lstlisting}[caption={Clock Driver Structure}, label={lst:clockDriver}]
struct alignas(arch::kCacheLineSizeBytes) ClockRegistryEntry : data_structures::RegistryEntry {
  /* Clock numbers */
  u64 frequency_kHz; // Frequency in kHz
  u64 ns_uncertainty_margin_per_sec;  
    // Uncertainty margin in femtoseconds per second
  u64 clock_numerator; // For conversion to nanoseconds, this is the numerator
  u64 clock_denominator; // For conversion to nanoseconds, this is the denominator

  /* Callbacks */
  u64 (*read)(ClockRegistryEntry *);
  bool (*enable_device)(ClockRegistryEntry *);
  bool (*disable_device)(ClockRegistryEntry *);
  void (*stop_counter)(ClockRegistryEntry *);
  void (*resume_counter)(ClockRegistryEntry *);

  /* Own data */
  void *own_data;
};
\end{lstlisting}

For the x86-64 architecture, clock selection is prioritized based on availability and precision in the following order: TSC > HPET > RTC > PIT. Currently, support is implemented only for the TSC and HPET drivers.

\subsubsection{TSC}
The Time Stamp Counter (TSC) is the optimal clock source due to its high precision and core-locality. Accessing the TSC involves reading a CPU register, which requires only a few cycles, unlike external clocks that may require hundreds. However, the TSC has historical limitations. On older processors, the counter's frequency was tied to the core frequency. Consequently, frequency scaling (throttling) caused the time measurement to drift, rendering it unreliable. Modern Intel processors introduced the Invariant TSC, which ensures a constant frequency regardless of the core's power state. Another limitation is that on certain CPUs, the TSC frequency is not explicitly known and must be measured against a known reference. For this purpose, the system utilizes the HPET, which serves as the minimal hardware requirement for reliable calibration (see \cite{IntelManual-TSC}).

\subsubsection{HPET}
The High Precision Event Timer (HPET) also offers high precision and is generally more stable than the TSC on older hardware. However, it is an external device mapped via Memory-Mapped I/O (MMIO). As a result, accessing the HPET is significantly slower than reading the TSC, potentially taking up to 1000 cycles per read. Due to this performance overhead, the HPET is designated as the secondary choice. It is primarily utilized for calibrating other clocks rather than for frequent timekeeping operations.

\subsection{Event Clocks}

Event Clock Drivers are defined by the following structure:

\begin{lstlisting}[caption={Event Clock Driver Structure}, label={lst:eventClockDriver}]
struct PACK EventClockFlags {
    bool IsCoreLocal : 1;
    u32 padding : 31;
};
static_assert(sizeof(EventClockFlags) == sizeof(u32));

enum class EventClockState : u8 {
    kDisabled = 0,  // Clock is disabled
    kPeriodic,      // Clock is in periodic mode
    kOneshot,       // Clock is in oneshot mode
    kOneshotIdle,   // Clock is in oneshot mode but no event is scheduled
    klast,
};

struct alignas(arch::kCacheLineSizeBytes) EventClockRegistryEntry : data_structures::RegistryEntry {
    /* Clock numbers */
    u64 min_next_event_time_ns;  // Minimum time for the next event in nanoseconds

    /* Clock specific data */
    EventClockFlags flags;     // Features of the event clock, e.g., core-local
    CoreMask supported_cores;  // Cores that support this event clock

    /* infra data */
    u64 next_event_time_ns;  // Time for the next event in nanoseconds
    EventClockState state;   // Current state of the clock

    /* Driver data */
    void *own_data;  // Pointer to the clock's own data, used for callback

    /* callbacks */
    struct callbacks {
        // Callback to set next event time
        u32 (*next_event)(EventClockRegistryEntry *, u64);  
        // Callback to set clock state
        u32 (*set_oneshot)(EventClockRegistryEntry *);
        // Callback to set clock state
        u32 (*set_periodic)(EventClockRegistryEntry *);     
        void (*on_entry)(EventClockRegistryEntry *);        // optional
        void (*on_exit)(EventClockRegistryEntry *);         // optional
    } cbs;
};
\end{lstlisting}

For x86-64, the selection of event clocks is based on the following priority order: LAPIC Timer > HPET > PIT. Currently, only the LAPIC Timer is supported as it provides the core-local interrupt capabilities required for efficient scheduling.

\subsubsection{LAPIC Timer}
Similar to the TSC, the LAPIC Timer is local to the processor core, ensuring extremely low-latency access to its registers. A significant advantage of this architecture is that each core possesses an independent timer, which eliminates the need for shared resource management or synchronization between cores. However, like the TSC, the LAPIC Timer operates at a frequency derived from the CPU bus or core frequency, which is not standard across various CPUs. Consequently, it requires calibration against a reference clock, for which the HPET is utilized.

\section{File System}
\label{section:fs}

The file system is a fundamental part of any operating system, providing mechanisms to store, access, and manage data on storage devices. We have designed a unified abstraction layer known as the Virtual File System (VFS). The VFS abstracts the underlying implementation details of specific file systems, providing the kernel and user space with an API for operations such as reading, writing, creating, moving, and deleting files and directories.

\subsection{VFS}

The VFS enables the operating system to interact with various file system types (e.g., FAT12, FAT16, FAT32) through a uniform interface, eliminating the need for the kernel to understand the specific implementation details. This modularity is achieved through a combination of compile-time enforced interfaces (Concepts) and a runtime dispatch mechanism.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=1.5cm,
      block/.style={
        rectangle, draw, fill=gray!10,
        text width=5cm, text centered,
        rounded corners, minimum height=1cm
      },
      component/.style={
        rectangle, draw, fill=gray!10,
        text width=5cm, text centered,
        minimum height=1cm
      },
      interface/.style={
        rectangle, draw, dashed, fill=gray!10,
        text width=5cm, text centered,
        minimum height=1cm
      },
      storage/.style={
        cylinder, draw, fill=gray!10,
        shape aspect=0.5,
        text width=4.5cm, text centered,
        minimum height=1cm
      },
      arrow/.style={-Latex, thick}
  ]

  \node (user_process) [block] {User Process};
  \node (fd_manager) [block, below=of user_process] {File Descriptor Manager};
  \node (vfs_module) [block, below=of fd_manager] {VFS Module};
  \node (fs_interface) [interface, below=of vfs_module] {Filesystem Interface};
  \node (fs_driver) [component, below=of fs_interface] {File System Driver \\ (FAT12 / FAT16 / FAT32)};
  \node (vfs_io) [interface, below=of fs_driver] {VFS I/O Interface};
  \node (storage_device) [storage, below=of vfs_io] {Storage Device \\ (RAM, Disk, Network)};

  \draw [arrow] (user_process) -- (fd_manager) node [midway, right] {Syscalls};
  \draw [arrow] (fd_manager) -- (vfs_module) node [midway, right] {File Operations};
  \draw [arrow] (vfs_module) -- (fs_interface) node [midway, right] {Dispatch};
  \draw [arrow] (fs_interface) -- (fs_driver) node [midway, right] {Implements};
  \draw [arrow] (fs_driver) -- (vfs_io) node [midway, right] {Uses};
  \draw [arrow] (vfs_io) -- (storage_device) node [midway, right] {I/O Operations};

  \end{tikzpicture}
  \caption{High-level VFS Architecture}
  \label{fig:vfs_architecture}
\end{figure}

As illustrated in Figure \ref{fig:vfs_architecture}, the VFS architecture is composed of four primary layers:

\begin{itemize}
\item \textbf{VFS Module}: The central orchestrator for path resolution and operation delegation.
\item \textbf{Filesystem Interface}: A struct containing function pointers that abstract specific driver operations.
\item \textbf{Filesystem Driver}: The concrete implementation of a specific file system format.
\item \textbf{VFS I/O Interface}: An abstraction for block-level data access.
\end{itemize}

The following subsections describe each of these components in detail.

\subsubsection{VFS Module}

The VFS module serves as the entry point for all file operations and is responsible for managing file system mounts. It exposes the internal kernel API for operations such as opening, reading, and writing files. The module's primary responsibility is to translate these requests into calls to the appropriate filesystem instance based on the provided file path and the currently registered mount points.

When a VFS operation (e.g., \texttt{CreateFile}) is invoked, the module executes the following sequence:
\begin{enumerate}
    \item \textbf{Find Mount Point}: The system utilizes a \textbf{crit-bit tree} \cite{critbit} to efficiently locate the longest prefix match for the given path among all registered mount points. This step identifies the specific filesystem driver responsible for handling the operation.
    \item \textbf{Check Permissions}: The module validates the operation against the mount point options (e.g., ensuring write operations are not attempted on read-only mounts).
    \item \textbf{Path Translation}: The absolute system path is translated into a path relative to the mount point root.
    \item \textbf{Delegation}: The operation is delegated to the corresponding method of the specific filesystem driver.
\end{enumerate}

\begin{lstlisting}[caption={VfsModule CreateFile Implementation}, label={lst:vfsModuleCreateFile}]
Result<> internal::VfsModule::CreateFile(const Path &path)
{
    auto mount_result = FindMountPoint(path);
    RET_UNEXPECTED_IF_ERR(mount_result);

    MountPoint *mount = mount_result.value();

    RET_UNEXPECTED_IF(mount->options.read_only, VfsError::kReadOnly);

    Path relative_path = GetRelativePath_(path, mount->path);
    return mount->fs.CreateFile(relative_path);
}
\end{lstlisting}

\subsubsection{Filesystem Interface}

The \texttt{vfs::Filesystem} struct acts as a uniform interface that all concrete filesystem drivers must expose. It contains function pointers for various file and directory manipulations, alongside metadata about the filesystem. Each function pointer accepts a \texttt{void* ctx} argument, allowing the generic VFS layer to pass the concrete driver instance.

\begin{lstlisting}[caption={vfs::Filesystem Structure}, label={lst:vfsFilesystem}]
struct Filesystem {
    struct Operations {
        // File operations
        Result<> (*create_file)(void *ctx, const Path &path);
        Result<size_t> (*read_file)(
            void *ctx, const Path &path, void *buffer, size_t size, size_t offset
        );
        Result<size_t> (*write_file)(
            void *ctx, const Path &path, const void *buffer, size_t size, size_t offset
        );
        Result<> (*delete_file)(void *ctx, const Path &path);
        // ... (additional operations omitted for brevity)
    };

    struct Info {
        Type type;
        const char *name;  // e.g., "FAT32", "FAT16"
    };
    
    Filesystem() = delete;
    explicit Filesystem(void *context, const Operations &operations, const Info &info)
    : context_(context), ops_(operations), info_(info) {}

private:
    void *context_;  // Pointer to the concrete driver instance
    Operations ops_;
    Info info_;
};
\end{lstlisting}

\subsubsection{Filesystem Driver}

Each filesystem driver implements the logic required for a specific filesystem type (e.g., FAT12, FAT16, FAT32). The driver interprets the raw data structures on the storage device and performs the requested operations. For instance, the FAT32 driver handles the manipulation of the File Allocation Table, directory entries, and cluster chains according to the FAT32 specification (for details, see \cite{fat-spec}).

To implement these drivers efficiently, we employ the Curiously Recurring Template Pattern (CRTP) \cite{crtp}. Each driver class inherits from a templated base class that provides common functionality, while the derived class implements format-specific details. This approach enables static polymorphism, reducing the runtime overhead typically associated with virtual function calls.

\begin{lstlisting}[caption={FAT Driver CRTP Base Class}, label={lst:fatCrtp}]
template <template <typename> typename T, typename IO>
class Fat
{
    using ImplT  = T<IO>;
    using Traits = FatTraits<T, IO>;

protected:
    // ... common FAT structures, validation, and operations

public:
    NODISCARD Filesystem GetFilesystem()
    {
        return Filesystem(
            this, // 'this' pointer is passed as the context
            Filesystem::Operations{
                .create_file = &Fat::CreateFileCallback_,
                // ...
            },
            Filesystem::Info{
                .type = ImplT::kFsType,
                .name = ImplT::kFsName,
            }
        );
    }

private:
    FAST_CALL Result<> CreateFileCallback_(void *ctx, const Path &path)
    {
        return static_cast<Fat *>(ctx)->CreateFile(path);
    }
    // ...
};
\end{lstlisting}

Each \texttt{Fat} derivative implements its specific logic (e.g., Getting FAT entries differs between FAT12 and FAT32/16) and provides a static \texttt{IsValid(IO \&io)} method to probe whether a given I/O device contains a valid instance of that filesystem.

\begin{lstlisting}[caption={Fat12 GetFATEntry Implementation}, label={lst:fat12GetFatEntry}]
    NODISCARD FORCE_INLINE_F ClusterNumT GetFATEntry_(ClusterNumT cluster) const
    {
        ASSERT_LT(
            cluster, BaseT::cluster_count_ + BaseT::kFirstClusterNumber,
            "Cluster number out of range"
        );
        const size_t fat_offset = cluster + (cluster / 2);
        const size_t sector_number =
            BaseT::fat_region_.start + (fat_offset / boot_sector_.fat.bytes_per_sector);
        const size_t sector_offset = fat_offset % boot_sector_.fat.bytes_per_sector;

        // Load 2 sectors if entry spans two sectors
        size_t count =
            (sector_offset == static_cast<size_t>(boot_sector_.fat.bytes_per_sector - 1)) ? 2 : 1;
        auto range = BaseT::io_.ReadRange({sector_number, count});
        if ((cluster % 2) == 0) {  // Even cluster
            return internal::get<ClusterNumT>(range, sector_offset) & kClusterMask;
        } else {
            return internal::get<ClusterNumT>(range, sector_offset) >> 4;
        }
    }
\end{lstlisting}

\subsubsection{VFS Interfaces and Concepts}

To enforce architectural compliance at compile time, we utilize C++20 Concepts to define strict contracts for both filesystem drivers and low-level storage operations.

The \texttt{VFSInterface} concept (Listing \ref{lst:vfsInterfaceConcept}) mandates that any compliant driver class must provide a constructor accepting an I/O backend, a static method to validate the filesystem signature on the storage medium, and a method to retrieve the runtime function table.

\begin{lstlisting}[caption={VFSInterface Concept Definition}, label={lst:vfsInterfaceConcept}]
template <template <typename> typename T, typename IO>
concept VFSInterface = VFSIO<IO> and requires(T<IO> fs, IO io) {
    T<IO>(io);
    { T<IO>::IsValid(io) } -> std::same_as<bool>;
    { fs.GetFilesystem() } -> std::same_as<Filesystem>;
};
\end{lstlisting}

The VFS I/O interface abstracts low-level data access. The \texttt{VFSIO} concept (Listing \ref{lst:vfsioConcept}) establishes a contract for block-based input and output. By adhering to this concept, filesystem drivers remain decoupled from the underlying hardware, enabling seamless interaction with diverse storage backends --- such as RAM disks, physical partitions, or network storage --- without requiring implementation changes.

\begin{lstlisting}[caption={VFSIO Concept Definition}, label={lst:vfsioConcept}]
template <typename IO>
concept VFSIO =
    requires(IO io, size_t offset, io::SectorRange range, std::span<const byte> data, size_t size) {
        { io.ReadRange(range) } -> std::same_as<std::span<byte>>;
        { io.ReadSector(offset) } -> std::same_as<std::span<byte>>;
        { io.WriteRange(range, data) } -> std::same_as<void>;
        { io.WriteSector(offset, data) } -> std::same_as<void>;
        { io.GetSectorSize() } -> std::same_as<size_t>;
    };
\end{lstlisting}

\subsection{File Descriptors}

The file descriptor system provides a hierarchical structure to handle file I/O operations initiated by user processes. This design tracks open files, maintains current read/write offsets, and enforces access rights.

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{res/fd-diagram.svg}
  \caption{File Descriptor Design}
  \label{fig:fd_architecture}
\end{figure}

As depicted in Figure \ref{fig:fd_architecture}, the system comprises three main tables:

\subsubsection{FdTable (Per-Process)}
Each process maintains its own \texttt{FdTable}, which is an array mapping integer file descriptors to \texttt{RefPtr<OpenFileEntry>} objects.

\begin{lstlisting}[caption={FdTable Allocation}, label={lst:fdTableAllocate}]
FdResult<fd_t> FdTable::Allocate(data_structures::RefPtr<OpenFileEntry> global_entry)
{
    std::lock_guard lock(lock_);
    for (size_t i = 0; i < kMaxFdsPerProcess; ++i) {
        if (entries_[i] == nullptr) {
            entries_[i] = std::move(global_entry);
            ++count_;
            return static_cast<fd_t>(i);
        }
    }
    return std::unexpected(FdError::kFdTableFull);
}
\end{lstlisting}

\subsubsection{OpenFileTable (Global)}

The \texttt{OpenFileTable} is a system-wide pool of \texttt{OpenFileEntry} objects. Each entry represents a unique "open" instance of a resource (file or pipe), maintaining dynamic state such as the current read/write offset, access flags (e.g., Read/Write), and a handle to the underlying resource. Multiple file descriptors from different processes can point to the same \texttt{OpenFileEntry}, allowing them to share the cursor position.

\begin{lstlisting}[caption={OpenFileTable Creation}, label={lst:openFileTableOpenFile}]
FdResult<data_structures::RefPtr<OpenFileEntry>> OpenFileTable::OpenFile(File *file, OpenMode flags)
{
    RET_UNEXPECTED_IF(file == nullptr, FdError::kInvalidArgument);

    std::lock_guard lock(lock_);

    const size_t idx = entries_.Allocate();
    RET_UNEXPECTED_IF(idx == std::numeric_limits<size_t>::max(), FdError::kIoError);

    OpenFileEntry *entry = entries_.Get(idx);
    ASSERT_NOT_NULL(entry);

    new (entry) OpenFileEntry();
    entry->pool_idx_ = idx;

    entry->handle    = FileHandle::Wrap(file);
    entry->flags     = static_cast<u32>(flags);
    entry->offset    = 0;
    entry->is_append = HasMode(flags, OpenMode::kAppend);
    ++count_;

    return data_structures::RefPtr(entry);
}
\end{lstlisting}

The \texttt{FileHandle} utilizes a \texttt{NonOwningTaggedPtr} to hold either a pointer to a \texttt{File} object or a \texttt{Pipe} instance, enabling the \texttt{OpenFileEntry} to manage both cases seamlessly.

\subsubsection{FileTable (Global)}
The \texttt{FileTable} manages unique \texttt{File} objects system-wide. Each \texttt{File} object acts as the in-memory representation of a physical file on the filesystem, storing static attributes such as the path and file size. This ensures that a file on the VFS has a single representation in memory, regardless of how many times it has been opened.

\begin{lstlisting}[caption={FileTable GetOrCreate}, label={lst:fileTableGetOrCreate}]
FdResult<data_structures::RefPtr<File>> FileTable::GetOrCreate(const vfs::Path &path)
{
    RET_UNEXPECTED_IF(path.IsEmpty(), FdError::kInvalidArgument);

    File *existing = Find(path);
    if (existing != nullptr) {
        return data_structures::RefPtr(existing, false);
    }

    const size_t idx = files_.Allocate();
    RET_UNEXPECTED_IF(idx == std::numeric_limits<size_t>::max(), FdError::kIoError);

    File *file = files_.Get(idx);
    ASSERT_NOT_NULL(file);

    new (file) File();
    file->pool_idx_ = idx;

    file->size = 0;
    file->mode = 0;
    file->path = path;
    ++count_;

    return data_structures::RefPtr(file);
}
\end{lstlisting}

\subsubsection{FdManager}
The \texttt{FdManager} integrates these three tables and exposes file descriptor operations (e.g., \texttt{Open}, \texttt{Close}, \texttt{Read}). When a process requests to open a file, the \texttt{FdManager} coordinates the interaction: it validates existence via the VFS module, retrieves the \texttt{File} object, creates a new entry in the \texttt{OpenFileTable}, and finally assigns a file descriptor in the process's \texttt{FdTable}.

\begin{lstlisting}[caption={FdManager Open Implementation}, label={lst:fdManagerOpen}]
FdResult<fd_t> FdManager::Open(const vfs::Path &path, OpenMode flags)
{
    // Check if file exists in VFS
    auto exists_result = VfsModule::Get().FileExists(path);
    RET_UNEXPECTED_IF(!exists_result, FdError::kIoError);
    RET_UNEXPECTED_IF(!*exists_result, FdError::kNotFound));

    auto file_result = file_table_.GetOrCreate(path);
    RET_UNEXPECTED_IF_ERR(file_result);

    auto open_result = open_file_table_.OpenFile(file_result->Get(), flags);
    RET_UNEXPECTED_IF_ERR(open_result);

    FdTable *fd_table = GetCurrentProcessFdTable();
    RET_UNEXPECTED_IF(fd_table == nullptr, FdError::kIoError);

    auto fd_result = fd_table->Allocate(std::move(*open_result));
    RET_UNEXPECTED_IF_ERR(fd_result);

    return *fd_result;
}
\end{lstlisting}

\subsubsection{Resource Lifecycle Management}

To ensure efficient memory usage and prevent resource leaks, the life cycle of \texttt{OpenFileEntry} and \texttt{File} objects is managed through intrusive reference counting. Both structures inherit from a \texttt{RefCounted} base class and are managed via smart pointers (\texttt{RefPtr} and \texttt{TaggedPointer}).

When a process closes a file descriptor (or terminates), the reference to the corresponding \texttt{OpenFileEntry} in the process's \texttt{FdTable} is released. If the reference count drops to zero --- indicating that no other file descriptors (across any process) refer to this open stream --- the \texttt{OpenFileEntry} is automatically deallocated and removed from the global \texttt{OpenFileTable}.

Consequently, the destruction of an \texttt{OpenFileEntry} releases its hold on the underlying \texttt{File} object. Similarly, if the reference count of the \texttt{File} object reaches zero, it implies that no active streams are reading from or writing to that specific file. The system then automatically reclaims the associated memory from the \texttt{FileTable}. This cascading deallocation mechanism ensures that kernel memory is only consumed for files that are actively in use by at least one process.

\section{Scheduling}

The scheduling module is the final core component of the operating system, relying heavily on and utilizing the previously described modules. Although the current implementation targets a single-core architecture for simplicity, the design is extensible to multi-core systems, as discussed later in this document.

\subsection{Process Structure}

The process structure is defined as follows:

\begin{lstlisting}[caption={Process Structure}, label={lst:processStruct}]
struct PACK Pid {
    u16 id;
    u64 count : 48;

    bool operator==(const Pid &other) const = default;
};

struct PACK ProcessFlags {
    bool KernelSpaceOnly : 1;
    bool PreserveFloats : 1;
};
static_assert(sizeof(ProcessFlags) == 1);

enum class ProcessState : u64 {
    kReady = 0,
    kWaitingForJoin,
    kTerminated,
    kLast,
};
static_assert(sizeof(ProcessState) == sizeof(u64));

struct Process : hal::Process {
    static constexpr size_t kMaxNameLength = vfs::kMaxComponentSize;

    /* Management */
    char name[kMaxNameLength];
    Pid pid;
    ProcessFlags flags;
    Thread *threads;
    u64 live_threads;
    u64 threads_to_clean;
    ProcessState state;
    WaitQueue<Thread, 3> *wait_queue;
    int status;

    /* Process resources */
    Mem::VPtr<Mem::AddressSpace> address_space;

    /* File descriptor table */
    Mem::VPtr<Fs::FdTable> fd_table;

    /* Standard I/O pipes (owned by process) */
    IO::Pipe<Fs::kStdioBufferSize> stdin_pipe;
    IO::Pipe<Fs::kStdioBufferSize> stdout_pipe;
    IO::Pipe<Fs::kStdioBufferSize> stderr_pipe;
};
\end{lstlisting}

\subsubsection{PID}
\label{subsubsec:pid}

As mentioned in the limitations section \ref{subsec:limitations}, the system imposes a hard limit on the maximum number of existing processes. This constraint allows for optimized process lookup via direct indexing. The \textbf{PID} (Process Identifier) is composed of a reservable identifier and an atomic counter, ensuring that each PID remains unique throughout the entire lifetime of the system.

\subsubsection{Process Flags}

\textbf{KernelSpaceOnly} -- enables the creation of kernel-only threads and processes. These processes do not require a separate user-space address space or user-space stack. This minimization of resource consumption improves performance; additionally, context switching between kernel-only threads does not necessitate switching the address space.

\textbf{PreserveFloats} -- indicates that all threads within the process save and restore floating-point registers during context switches by default. Disabling this flag allows the system to omit these potentially expensive operations for threads that do not utilize floating-point arithmetic.

\subsubsection{Process State}

The process state has less significance than the Thread State (described later) and is primarily utilized for resource management when waiting for a process to terminate. It ensures that process resources are not deallocated more than once.

\subsubsection{Address Space}

The address space is a fundamental component of the process structure. It defines all information regarding virtual memory, specifically establishing the translation between the virtual and physical layers. Each process operates within its own virtual address space, facilitating simpler memory management regarding fragmentation, performance, and security. Refer to Memory Management \ref{sec:memory} for further details.

\subsubsection{File Descriptor Table}

This table contains all file descriptors opened by threads executing within the process. Refer to the File System section \ref{section:fs} for more information.

\subsubsection{Pipes}

Pipes handle the buffered I/O of the process, primarily used for standard input and output communication with the user.

\subsubsection{Name}

The name of the process typically corresponds to the name of the executable. Unlike the unique PID, the process name is not required to be unique.

\subsubsection{Threads}

This field points to the first element of a doubly linked list of threads running in this environment. It is used primarily for operations such as termination (kill/exit).

\subsubsection{Live Threads and Threads to Clean}

These counters are used for synchronization with the \textbf{ProcessRipper}. Refer to Kernel Workers \ref{subsubsec:kworkers} for more details.
 
\subsubsection{Status}

The exit status of the process, which is passed to any thread waiting for the process to complete.

\subsubsection{Wait Queue}

The \textbf{Wait Queue} stores threads that are waiting for this process to finish. All threads in this queue are woken up upon process termination.

\subsection{Kernel Address Space}

The entire operating system shares a single Kernel Address Space. Every kernel process operates within this space. Additionally, the kernel address space is mapped into every user-space process's address space. This design choice enhances performance and simplifies architecture, as user threads frequently invoke system calls. Switching address spaces on every system call would incur significant performance overhead and complicate the logic required for handling syscalls.

\subsection{Thread Structure}

The thread structure is defined as follows:

\begin{lstlisting}[caption={Thread Structure}, label={lst:threadStruct}]
enum class UserPriority : u8 { kLow = 0, kMediumLow, kMedium, kMediumHigh, kHigh, kLast };

struct PACK Tid {
    u16 id;
    u64 count : 48;

    bool operator==(const Tid &other) const = default;
};

struct PACK ThreadFlags {
    SchedulingPolicy policy : 8;
    u8 priority : 8;
    UserPriority user_priority : 3;
    bool preserve_floats : 1;
    bool detached : 1;
    u64 padding : 43;
};
static_assert(sizeof(ThreadFlags) == 8);

enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kSleeping,
    kBlockedOnWaitQueue,
    kWaitingForJoin,
    kTerminated,
    kLast,
};
static_assert(sizeof(ThreadState) == sizeof(u64));

static constexpr int kSchedulingIntrusiveLevel  = 0;
static constexpr int kSleepingIntrusiveLevel    = 1;
static constexpr int kProcessListIntrusiveLevel = 2;
static constexpr int kWaitQueueIntrusiveLevel   = 3;

struct Thread : data_structures::IntrusiveRbNode<Thread, u64, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveRbNode<Thread, u64, kSleepingIntrusiveLevel>,
                data_structures::IntrusiveListNode<Thread, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveListNode<Thread, kSleepingIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kWaitQueueIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kProcessListIntrusiveLevel> {
    /* Management */
    Tid tid;
    Pid owner;
    ThreadFlags flags;
    ThreadState state;
    void *retval;
    WaitQueue<Thread, kWaitQueueIntrusiveLevel> *wait_queue;

    /* Thread resources */
    void *kernel_stack;
    void *kernel_stack_bottom;
    void *user_stack;
    void *user_stack_bottom;

    /* Statistics */
    u64 kernel_time_ns;
    u64 user_time_ns;
    u64 timestamp;
    u64 timestamp_execution_start_ns;
    u64 num_interrupts;
    u64 num_syscalls;
    u64 num_context_switches;

    /* Arch */
    hal::Thread arch_data;

    NODISCARD u64 CalculateCpuTime();
};
\end{lstlisting}

\subsubsection{TID}

Similar to the PID \ref{subsubsec:pid}, the system limits the number of available threads. This property is utilized to create unique \textbf{TIDs} (Thread Identifiers), allowing for rapid lookups.

\subsubsection{Owner's PID}

The PID of the process that owns the thread.

\subsubsection{Thread Flags}

A collection of bitfields that determine how the scheduler manages the thread:

\textbf{SchedulingPolicy policy} -- defines the scheduling policy under which the thread currently operates. Refer to Policies \ref{subsec:policies} for more details.

\textbf{u8 priority} -- defines the internal priority of the thread within its assigned policy. This value is managed by the kernel.

\textbf{UserPriority user\_priority} -- may be specified by User Space to inform the scheduler of the relative importance of threads.

\textbf{bool preserve\_floats} -- specifies whether the floating-point state is preserved during context switches.

\textbf{bool detached} -- specifies whether the scheduler should automatically clean up the thread upon termination, without waiting for a join operation.

\subsubsection{Thread State}

Describes the current status of the thread. This state tracking allows the kernel to monitor each thread individually and verify system integrity, as only specific state transitions are permitted. The thread state transitions graph is defined as follows:

\newgeometry{top=1cm, bottom=1.5cm, left=1cm, right=1cm} 

\begin{figure}[p]
    \centering
    \includesvg[height=0.95\textheight, width=\textwidth, keepaspectratio]{res/thread-states.svg}
    \caption{Thread State Transitions Graph}
    \label{fig:thread-states}
\end{figure}

\restoregeometry

\subsubsection{Stacks}

Two distinct stacks are required for each thread: one for user space and one for kernel space. A separate kernel stack is essential for security and stability. Furthermore, sharing a single kernel stack is inefficient because context switches may occur within kernel code, such as during thread joining or other synchronization events.

\subsubsection{Statistics}

Statistics tracking allows for the analysis of execution specifics for each thread. This data is used to dynamically adjust the scheduling module and to debug or test the scheduler.

\subsubsection{Wait Queue}

Allows other threads to block and wait for a specific thread to finish execution (Thread joining).

\subsection{Kernel Workers}
\label{subsubsec:kworkers}

Before initializing the scheduling module, the operating system creates three essential kernel workers:

\textbf{Trace Dumper} -- responsible for dumping kernel traces to the terminal or storage files, ensuring debuggability and stability. This task cannot be performed directly by the kernel code during critical execution paths, as writing to devices or files is too slow for system calls or interrupt handlers. The buffers are dumped periodically every 20ms.

\textbf{Thread Ripper} -- responsible for the deallocation of thread resources, including descriptors and stacks.

\textbf{Process Ripper} -- responsible for the deallocation of process resources, including the address space.

\subsection{Intrusive Data Structures}

Since threads frequently migrate between different linked lists and data structures, allocating and freeing list nodes for every operation introduces significant overhead. To address this, the system employs intrusive data structures. Instead of allocating a node that contains a pointer to the object, the node structure is embedded directly within the object itself. Operations on the lists are performed by simply modifying the fields of the object.

\begin{lstlisting}[caption={Intrusive Nodes}, label={lst:intrusiveNodes}]
template <class T, int kIntrusiveLevel>
struct IntrusiveListNode {
    T *next;
};

template <class T, int kIntrusiveLevel>
struct IntrusiveDoubleListNode {
    T *next;
    T *prev;
};

template <class T, class KeyT, int kIntrusiveLevel>
struct IntrusiveRbNode {
    enum class Color : u8 {
        kBlack = 0,
        kRed   = 1,
    };

    T *parent;
    union {
        struct {
            T *left;
            T *right;
        };
        T *child[2];
    };

    Color color;
    KeyT key;
};
\end{lstlisting}

The Thread data structure \ref{lst:threadStruct} demonstrates the usage of such nodes. This approach offers multiple benefits, including:
\begin{itemize}
    \item No dynamic allocation or deallocation during insert or pop operations.
    \item Improved memory access patterns, the object is accessed directly rather than via a pointer from a separate node.
    \item Verification of whether an object belongs to a list in \textbf{O(1)} time, given only the object itself.
    \item Removal of the object from the list in \textbf{O(1)} time.
    \item Removal of the object from the list without requiring access to the list head structure.
\end{itemize}

\subsection{Meta-Scheduler}

The scheduler architecture follows the \textbf{meta-scheduler} design pattern, which is common in modern operating systems. This approach relies on abstracting scheduling logic into Policies. Policies are primarily responsible for selecting the next task from the set of tasks they manage and reacting to thread behavior (e.g., punishing or rewarding threads with CPU time based on workload characteristics). The scheduler itself is responsible for all other operations, including:
\begin{itemize}
    \item Managing thread states.
    \item Transitioning threads to sleep.
    \item Waking up threads.
    \item Managing idle time.
    \item Configuring next timing events via the timing infrastructure.
    \item Blocking threads on wait queues.
    \item Releasing threads from wait queues.
\end{itemize}
A hierarchy exists between policies, ensuring that threads from higher-priority policies are selected before threads from lower-priority ones.

\subsection{Timing Model}

The system implements a tickless kernel architecture. Instead of relying on a periodic interrupt (the \textbf{Kernel Tick}) at a fixed frequency, the scheduler calculates precisely when the next timing event must occur. This approach improves efficiency and precision.

\subsection{Policies}

The following scheduling policies have been implemented:

\textbf{Round Robin Scheduling Policy} -- simple policy that iterates through a linked list from front to back. Threads are ordered based on their arrival time in the policy.

\textbf{Priority Queue Scheduling Policy} -- policy that orders threads based on kernel-assigned priorities. Priorities are capped at a range of 0-64 to enable the use of an \textbf{O(1)} priority queue, known as a \textbf{Bitmap Priority Queue}. This structure maintains an array of linked lists representing individual priorities. Additionally, a bitmask indicates the presence of tasks at specific priority levels. Lookup operations utilize bitwise instructions (counting leading/trailing zeros) to efficiently find the minimum or maximum priority value (\textbf{O(1)}).

\textbf{Multi-Level Feedback Queue (MLFQ) Scheduling Policy} -- policy that segregates threads into distinct queues according to their workload characteristics. The policy penalizes CPU-bound threads by demoting them to lower priorities, whereas I/O-bound threads are elevated to higher priorities, thereby favoring interactive performance. To prevent the starvation of low-priority tasks, a periodic reset mechanism promotes all threads to the highest priority queue every 100ms. Within each queue, threads are managed using \textbf{Red-Black trees}, sorted by a key that combines user-defined priority and aggregated CPU burst times (the amount of time a thread spends executing on the CPU before it either completes, requires I/O operations, or is interrupted by the operating system). This ensures that each thread in the given queue receives a fair share of CPU time.

\noindent These policies establish a hierarchy defined by the following enumeration:

\begin{lstlisting}[caption={Scheduling Policy Hierarchy}, label={lst:policyHierarchy}]
// PO > P1 > .. > P4
enum SchedulingPolicy {
    kUberTask_PQ_P0 = 0,
    kDrivers_PQ_P1,
    kUrgentTasks_PQ_P2,
    kNormalTasks_MLFQ_P3,
    kBackgroundTasks_RR_P4,
    kLast,
};
\end{lstlisting}

Each policy is designated for a specific group of tasks:

\textbf{Uber Tasks} -- Tasks that must be executed as quickly as possible, such as emergency recovery actions or critical kernel state preservation (e.g., saving state before shutdown or terminating processes during out-of-memory conditions).

\textbf{Drivers} -- Driver tasks that require immediate execution to prevent device blocking and ensure smooth system operation (e.g., audio drivers or network interfaces).

\textbf{Urgent Tasks} -- Tasks that are less critical than drivers but more important than standard user tasks. These may include privileged user-space tasks that consume data from drivers.

\textbf{Normal Tasks} -- The most common group of tasks, comprising standard user-space programs such as graphical interfaces and data processing applications. Basic kernel workers, including those listed in the kernel workers section \ref{subsubsec:kworkers}, also belong to this category.

\textbf{Background Tasks} -- Tasks that are not time-critical and should only be executed when the system is otherwise idle. Examples include update checks or non-urgent cleanup operations.

\subsubsection{Policies Abstraction}
\label{subsec:policies}

The policy abstraction is defined as follows:

\begin{lstlisting}[caption={Policy Abstraction}, label={lst:policyStruct}]
struct Policy {
    struct {
        Thread *(*pick_next_task)(void *);

        void (*add_task)(void *, Thread *);
        void (*remove_task)(void *, Thread *);

        u64 (*get_preempt_time)(void *, Thread *);
        bool (*is_first_higher_priority)(void *, Thread *, Thread *);
        bool (*validate_flags)(void *, const ThreadFlags *);

        void (*on_thread_yield)(void *, Thread *);
        void (*on_periodic_update)(void *, u64 current_time_ns);
    } cbs;
    void *self;
};
\end{lstlisting}

As shown, the primary operation is \textbf{pick\_next\_task}, alongside standard \textbf{add\_task} and \textbf{remove\_task} operations. Additionally, helper functions are provided for the scheduler and for statistics gathering.

\subsection{Sleeping}

To provide high-precision sleeping, the system relies on one-shot, precise timing events. Upon receiving a timer interrupt, the scheduler determines the timestamp for the next event based on the system state, which includes the current thread preemption time and the wake-up times of sleeping threads. Consequently, the scheduler must inspect the sleeping queue to process expired events. To ensure high performance and stable time complexity, an Intrusive Red-Black Tree was selected as the underlying data structure for the \textbf{Sleeping Queue}. The critical operations are \textbf{Insert}, \textbf{ExtractMin}, and \textbf{Remove}, all of which must have predictable execution times. The requirement for efficient arbitrary removal excluded binary heaps, while the need for stability excluded heaps with amortized complexity bounds. The Red-Black Tree satisfied all requirements and was already implemented for other system components, making it the optimal choice.

% ==================================================================

\chapter{Results}
TODO KTOKOLWIEK

\section{User's Manual}

This section describes the usage of the AlkOS build environment and documents the public interfaces available to user-space applications. It outlines the build and execution workflow, the integration of user programs, and the system call interface provided by the operating system.

\subsection{Building and Running the OS}

AlkOS provides a command-line utility script, \texttt{alkos\_cli.bash}, located in the \texttt{scripts/} directory. This script automates the setup of the development environment, the compilation of the kernel, and the execution of the operating system within an emulator. The build infrastructure is based on CMake and requires a Linux host system.

Automated dependency installation is officially supported for Arch Linux and Ubuntu distributions. Nevertheless, the build process is expected to function on other Linux distributions, provided that the required dependencies are installed manually. Reference installation dependencies for supported environments can be found in the \texttt{scripts/env/} directory.

The build and execution workflow consists of three sequential stages:

\begin{enumerate}
    \item \textbf{Environment Initialization:}
    Prior to compilation, a custom cross-compilation toolchain and all required system dependencies must be installed. This can be achieved by executing:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --install all
    \end{verbatim}
    Alternatively, to install only the cross-compiler toolchain, the following command may be used:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --install toolchain
    \end{verbatim}

    \item \textbf{Configuration:}
    During the configuration phase, build configuration and kernel feature flags are generated and stored in the \texttt{config/} directory. The default configuration can be created using:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --configure
    \end{verbatim}
    For customized configurations, the following script is provided:
    \begin{verbatim}
    ./scripts/config/configure.bash <platform> <build_type> [options...]
    \end{verbatim}
    Here, \texttt{<platform>} denotes the target architecture (e.g., \texttt{x86\_64}), \texttt{<build\_type>} specifies either \texttt{Debug} or \texttt{Release}. A complete list of options can be obtained using the \texttt{\text{-}\text{-}help} flag.

    \item \textbf{Compilation and Execution:}
    In the final stage, the kernel and all registered user-space applications are compiled, the root filesystem image and bootable ISO are generated, and the operating system is launched using the QEMU emulator. This process can be initiated with the command:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --run
    \end{verbatim}
\end{enumerate}

In the event of errors occurring at any stage of the process, the scripts may be executed with the \texttt{\text{-}\text{-}verbose} flag, which enables detailed logging output to facilitate debugging.

\subsection{Developing Userspace Applications}

AlkOS supports user-space applications written in C and C++. These applications are integrated into the operating system at build time and included directly in the generated root filesystem.

\subsubsection{Project Structure}

User-space programs reside in the \texttt{userspace/programs/} directory. Each application is placed in its own subdirectory and must include all relevant source files along with a \texttt{CMakeLists.txt} configuration file. A minimal example of a user application is shown in Listing \ref{lst:minimalUserAppStructure}.

\begin{lstlisting}[caption={Minimal User Application Structure}, label={lst:minimalUserAppStructure}]
// userspace/programs/my_app/main.cpp
#include <stdio.h>

extern "C" int main() {
    printf("Hello from AlkOS User Space!\n");
    return 0;
}
\end{lstlisting}

\subsubsection{Build System Integration}

To include a user-space application in the build process, it must be registered with the CMake-based build system. AlkOS provides the helper macro \texttt{alkos\_register\_userspace\_app}, which automatically configures compiler flags and links the application against the custom C and C++ standard libraries (\texttt{libc} and \texttt{libc++}). An example registration is shown in Listing \ref{lst:userAppCMakeRegistration}.

{
    \renewcommand{\lstlistingname}{CMake Snippet}
    \begin{lstlisting}[style=cmakestyle, caption={User Application Registration}, label={lst:userAppCMakeRegistration}]
    # userspace/programs/my_app/CMakeLists.txt
    alkos_find_sources(MY_APP_SOURCES)
    alkos_register_userspace_app(my_app "${MY_APP_SOURCES}")
    \end{lstlisting}
}

After registration, the application is automatically compiled, linked, and placed in the \texttt{/bin} directory of the root filesystem during the next build.

\subsection{C Standard Library}

Instead of relying on an external standard library implementation, AlkOS provides a custom C standard library tailored to its kernel interface.

\subsubsection{Standard C Support}

The library includes implementations of commonly used standard headers such as \texttt{<stdio.h>}, \texttt{<stdlib.h>}, and \texttt{<string.h>}. This enables the compilation of conventional C programs with minimal adaptation. Standard file operations, including \texttt{fopen}, \texttt{fread}, and \texttt{fwrite}, are fully supported and internally routed through the VFS.

\subsubsection{AlkOS-Specific Extensions}

Certain low-level system interactions are beyond the scope of the standard C library. To expose kernel-specific functionality, AlkOS provides additional interfaces via the \texttt{<alkos/calls.h>} header. These interfaces enable access to hardware abstraction, threading primitives, and system control mechanisms. The most important extensions include:

\begin{itemize}
    \item \textbf{Input and Output Subsystem (\texttt{alkos/sys/video.h}, \texttt{alkos/sys/input.h}):}
    The \texttt{GetVideoBufferInfo} function maps the framebuffer into the process address space, allowing direct graphical output. Keyboard input is accessed via \texttt{GetKeyState}, which enables polling of individual keys.

    \item \textbf{Process Management, Threading, and Timing (\texttt{alkos/sys/thread.h}, \texttt{alkos/sys/time.h}):}
    Thread lifecycle management is provided through \texttt{ThreadCreate}, \texttt{ThreadJoin}, and \texttt{ThreadDetach}. High-precision timing services are available via \texttt{NanoSleep} and \texttt{NanoSleepUntil}. Process-level control is facilitated by \texttt{Exec} and \texttt{ProcExit}.

    \item \textbf{Power Management (\texttt{alkos/sys/power.h}, \texttt{alkos/sys/proc.h}):}
    System power states can be controlled using the \texttt{Shutdown} and \texttt{Reboot} interfaces, which invoke the underlying ACPI mechanisms.

    \item \textbf{Extended Filesystem Operations (\texttt{alkos/sys/fs/fs.h}):}
    While standard I/O functions support file access, directory enumeration and metadata queries require OS-specific calls. The \texttt{ReadDirectory} function enables directory traversal, and \texttt{FileInfo} provides information about filesystem objects.
\end{itemize}

\subsection{Syscalls}

The system call interface represents the controlled boundary between user-space applications executing in Ring 3 and the kernel executing in Ring 0. On the \texttt{x86\_64} architecture, AlkOS utilizes the \texttt{int 0x80} software interrupt mechanism to invoke system calls.

\subsubsection{Calling Convention}

The calling convention is largely inspired by the System V AMD64 ABI, with modifications to accommodate system call semantics. The register usage is summarized in Table \ref{tab:syscall_abi}.

\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Register} & \textbf{Purpose} \\ \hline
\texttt{RAX} & System call number (input) / return value (output) \\ \hline
\texttt{RDI} & Argument 1 \\ \hline
\texttt{RSI} & Argument 2 \\ \hline
\texttt{RDX} & Argument 3 \\ \hline
\texttt{R10} & Argument 4 \\ \hline
\texttt{R8} & Argument 5 \\ \hline
\texttt{R9} & Argument 6 \\ \hline
\end{tabular}
\caption{System Call Register Mapping on x86\_64}
\label{tab:syscall_abi}
\end{table}

\subsubsection{Provided System Services}

The available system calls are grouped into logical categories:

\paragraph{Process and Thread Management}
\begin{itemize}
    \item \texttt{exec(path)}: Loads and executes a program.
    \item \texttt{proc\_exit(status)} and \texttt{proc\_abort()}: Terminates the calling process.
    \item \texttt{kill(pid)} and \texttt{wait(pid)}: Manage inter-process control and synchronization.
    \item \texttt{thread\_create}, \texttt{thread\_exit}, \texttt{thread\_join}, \texttt{thread\_detach}: Thread management primitives.
    \item \texttt{get\_heap\_start()}: Returns the base address of the process heap.
\end{itemize}

\paragraph{File I/O and Filesystem Management}
\begin{itemize}
    \item \texttt{open}, \texttt{close}, \texttt{read}, \texttt{write}, \texttt{seek}: File descriptor operations.
    \item \texttt{dup(fd)} and \texttt{dup\_to(fd, newfd)}: File descriptor duplication.
    \item \texttt{read\_directory} and \texttt{file\_info}: Filesystem inspection.
    \item \texttt{create\_directory}, \texttt{delete\_file}, \texttt{move\_file}: Filesystem modification operations.
\end{itemize}

\paragraph{Graphics and Input}
\begin{itemize}
    \item \texttt{create\_graphic\_session(info)}: Initializes a graphical context.
    \item \texttt{blit()}: Requests composition of the application backbuffer.
    \item \texttt{get\_key\_state(vk)}: Retrieves the state of a virtual key.
\end{itemize}

\paragraph{Time and Synchronization}
\begin{itemize}
    \item \texttt{nanosleep(ns)} and \texttt{nanosleep\_until(sys\_ns)}: Suspends execution for a defined duration.
    \item \texttt{get\_clock\_value}: Returns the current value of a specified clock.
    \item \texttt{get\_timezone}: Retrieves the system time zone configuration.
\end{itemize}

\paragraph{System Control and Debugging}
\begin{itemize}
    \item \texttt{power(action)}: Controls system power states.
    \item \texttt{panic(msg)}: Terminates execution and records a diagnostic message.
\end{itemize}


\section{Example Programs}
TODO ADAM

% TODO: extension
% \section{Performance Analysis}
% \subsection{KMalloc Performance}
% \subsection{KFree Performance}
% \subsection{Context Switch Performance}
% \subsection{Syscall Performance}
% \subsection{Scheduler Tests}


\chapter{Future Work}
TODO KTOKOLWIEK

\chapter{Conclusion}
TODO KTOKOLWIEK

% ------------------------------- BIBLIOGRAPHY ---------------------------

\printbibliography[heading=bibintoc]

\pagenumbering{gobble}
\thispagestyle{empty}



% ----------------------- LIST OF SYMBOLS AND ABBREVIATIONS ------------------
\chapter*{List of symbols and abbreviations}

\begin{tabular}{cl}
API & Application Programming Interface \\
APIC & Advanced Programmable Interrupt Controller \\
CRTP & Curiously Recurring Template Pattern \\
IDT & Interrupt Descriptor Table \\
IPC & Inter-Process Communication \\
ISR & Interrupt Service Routine \\
LAPIC & Local Advanced Programmable Interrupt Controller \\
LIT & Logical Interrupt Table \\
MLFQ & Multi-Level Feedback Queue \\
MMIO & Memory-Mapped I/O \\
MMU & Memory Management Unit \\
OS & Operating System \\
PIC & Programmable Interrupt Controller \\
PID & Process IDentifier \\
PIT & Programmable Interval Timer \\
RTC & Real-Time Clock \\
TID & Thread IDentifier \\
TSC & TimeStamp Counter \\
TSS & Task State Segment \\
VFS & Virtual File System \\
CFS & Completly Fair Scheduler \\
EEVDF & Earliest Eligible Virtual Deadline First \\
DMA & Direct Memory Access \\
PCP & Per-CPU Pagessets \\
COW & Copy On Write \\
SMP & Symmetric MultiProcessing \\
ACPI & Advanced Configuration and Power Interface \\
PCI & Peripheral Component Interconnect \\

\end{tabular}
\\
\thispagestyle{empty}


% ----------------------------  LIST OF FIGURES --------------------------------
\listoffigures
\thispagestyle{empty}

% -----------------------------  LIST OF TABLES --------------------------------
\renewcommand{\listtablename}{List of Tables}
\listoftables
\thispagestyle{empty}

\end{document}

% ==================================================================
% TODOS:
% - wytlumaczyc na starcie co zakladamy etc i dlaczego np segmenty
