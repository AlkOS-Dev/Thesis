\documentclass[a4paper,9pt,twoside]{report}

% --------   PREAMBLE PART ----------

% -------- ENCODING & LANGUAGES --------

\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage[polish, english]{babel}


\usepackage{amsmath, amsfonts, amsthm, latexsym}

\usepackage[final]{pdfpages}
\usepackage[backend=biber, style=numeric, sorting=none]{biblatex}
\addbibresource{references.bib}


\usepackage{commath}

\usepackage[hidelinks]{hyperref}

\usepackage[inkscapepath=../output/svg/]{svg}


% ------ MARGINS, INDENTATION, LINESPREAD ------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}


\linespread{1.5}
\allowdisplaybreaks

\usepackage{indentfirst}
\setlength{\parindent}{5mm}


%------ RUNNING HEAD - CHAPTER NAMES, PAGE NUMBERS ETC. -------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[LE,RO]{\thepage} 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}

\renewcommand{\headrulewidth}{0 pt}


\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[LE,RO]{\thepage}
  
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.0pt}
}

%------ code listings -------

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstdefinestyle{cppstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=C++,
    morekeywords={constexpr, nullptr, size_t, uint64_t}
}
\lstset{style=cppstyle}
\renewcommand{\lstlistingname}{C++ Code Snippet}

\lstdefinestyle{nasmstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=[x86masm]Assembler,
    morekeywords={rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8, r9, r10, r11, r12, r13, r14, r15,
                  eax, ebx, ecx, edx, esi, edi, ebp, esp,
                  cr0, cr2, cr3,
                  mov, push, pop, call, ret, int, iretq, jmp, je, jne, jg, jl, cmp, test,
                  add, sub, mul, div, inc, dec, xor, or, and,
                  lidt, lgdt, sti, cli, hlt,
                  section, global, extern, db, dw, dd, dq, resb, resw, resd, resq,
                  macro, endmacro, \%define}
}

\lstdefinestyle{cmakestyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true, 
    captionpos=b,
    keepspaces=true,                 
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    language=CMake,
    morekeywords={project, add_executable, add_library, target_link_libraries, set, include_directories,
                  cmake_minimum_required, file, macro, endmacro, foreach, endforeach, if, endif, else,
                  find_package, add_custom_command, add_custom_target, install, alkos_find_sources, alkos_register_userspace_app}
}

% --------- DRAWING -------

\usepackage{tikz}
\usetikzlibrary{fit, backgrounds, shapes, arrows.meta, positioning, calc, chains}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[inkscapepath=../output/svg/]{svg}
\usepackage{graphicx}

% --------- CHAPTER HEADERS -------

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 

    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% --------- TABLE OF CONTENTS SETUP ---------

\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}
  [0pt]
  {}
  {\bfseries \thecontentslabel.\quad}
  {\bfseries}
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% -------- TABLES AD FIGURES NUMBERING --------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}


% ----- DEFINING ENVIRONMENTS FOR THEOREMS, DEFINITIONS ETC. -----

\makeatletter
\newtheoremstyle{definition}
{3ex}
{3ex}
{\upshape}
{}
{\bfseries}
{.}
{.5em}
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% ------- END OF PREAMBLE PART (MOSTLY) ----------





% ---------- USER SETTINGS ---------

\newcommand{\tytul}{Funkcjonalne jądro systemu operacyjnego: AlkOS}
\renewcommand{\title}{From Bare Metal to a Functional Kernel: The AlkOS Operating System}
\newcommand{\type}{Engineer}
\newcommand{\supervisor}{mgr inż. Paweł Sobótka}



\begin{document}
\sloppy
\selectlanguage{english}

\includepdf[pages=-]{titlepage-en}

\null\thispagestyle{empty}\newpage

% ------ PAGE WITH SIGNATURES ------------

%\thispagestyle{empty}\newpage
%\null
%
%\vfill
%
%\begin{center}
%\begin{tabular}[t]{ccc}
%............................................. & \hspace*{100pt} & .............................................\\
%supervisor's signature & \hspace*{100pt} & author's signature
%\end{tabular}
%\end{center}
%


% ---------- ABSTRACT -----------

{  \fontsize{12}{14} \selectfont
\begin{abstract}

\begin{center}
\title
\end{center}

TODO

\end{abstract}
}

\null\thispagestyle{empty}\newpage

%% --------- DECLARATIONS ------------
%
%%
%%	IT IS NECESSARY OT ATTACH FILLED-OUT AUTORSHIP DEECLRATION. SCAN (IN PDF FORMAT) NEEDS TO BE PLACED IN scans FOLDER AND IT SHOULD BE CALLED, FOR EXAMPLE, DECLARATION_OF_AUTORSHIP.PDF. IF THE FILENAME OR FILEPATH IS DIFFERENT, THE FILEPATH IN THE NEXT COMMAND HAS TO BE ADJUSTED ACCORDINGLY.
%%
%%	command attacging the declarations of autorship
%%
%\includepdf[pages=-]{scans/declaration-of-autorship}
%\null\thispagestyle{empty}\newpage
%
%% optional declaration
%%
%%	command attaching the declaataration on granting a license
%%
%\includepdf[pages=-]{scans/declaration-on-granting-a-license}
%%
%%	.tex corresponding to the above PDF files are present in the 3. declarations folder 
%
\null\thispagestyle{empty}\newpage
% ------- TABLE OF CONTENTS -------
\selectlanguage{english}
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\newpage % IF YOU HAVE EVEN QUANTITY OD PAGES OF TOC, THEN REMOVE IT OR ADD \null\newpage FOR DOUBLE BLANK PAGE BEFORE INTRODUCTION


% -------- THE BODY OF THE THESIS ------------

\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11}
\chapter{Introduction}
\markboth{}{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\section{Scope of the Thesis} 
TODO: JAKUB

\section{Limitations and Assumptions}
TODO: JAKUB
TODO: KRYCZKA, ADAM, JAKUB

\subsection{Minimal requirements}
\begin{itemize}
  \item avx
  \item osxsave
  \item swapgs
  \item invariant tsc
  \item HPET
  \item APIC
  \item acpi
  \item 64bit LAPIC
  \item mmu
\end{itemize}

\subsection{Known Limitations}
\label{subsec:limitations}

\begin{itemize}
  \item max process name = 128
  \item max processes = 4096
  \item max threads = 8192
\end{itemize}

Functionalities:
\begin{itemize}
  \item aaa
\end{itemize}

\section{Achieved Functionalities} 
TODO: JAKUB
\section{Work Division -- Code} 
TODO: KRYCZKA, ADAM, JAKUB
\section{Work Division -- Thesis}
TODO: KRYCZKA, ADAM, JAKUB

% ==================================================================

\chapter{Creating an Operating System from Scratch}

This chapter outlines the development sequence of the kernel's most critical components, upon which the majority of the subsequent code depends. The proposed progression is derived from the experience gained and materials consulted during the development process. While this is not the only valid approach to system development, it represents a structured path designed to minimize redundant discovery and facilitate implementation. The discussion focuses exclusively on the monolithic kernel design with memory virtualization, where every module resides within the same address space - the kernel address space.

\section{Host Environment Preparation}

Before proceeding to writing actual kernel code we must first do some preparation on our development tools and development environemnts. Below we can find brief description of needed things.

\subsection{Cross-Compilation Toolchain}
TODO: ADAM

\subsection{Building Machinery}
TODO: ADAM

\subsection{Emulation}
TODO: KRYCZKA

\section{General Design Considerations}
TODO: JAKUB

\subsection{Intrusive Data Structures}
TODO: JAKUB

\subsection{Ref-counting}
TODO: ADAM

\subsection{Archtiecture Abstraction}
TODO: KRYCZKA

\section{Target Environment}
TODO: JAKUB

\subsection{Implementation of Libc and Libc++}
TODO: ADAM

\subsection{Bootloader}
\label{subsec:theory_bootloader}

The process of bringing a computer from a powered-off state to a fully functional operating system is governed by a rigid chain of physical and logical constraints. At the hardware level, the Central Processing Unit (CPU) functions as a complex state machine. Upon the application of power or a reset signal, the CPU resets its internal registers to default values and sets the Instruction Pointer to a specific, hardcoded physical address known as the \textit{Reset Vector} \cite{IntelManual-Reset}.

\subsubsection{The Memory Paradox and Storage}
A fundamental challenge in this sequence is the source of the initial instructions. The standard Random Access Memory (RAM), which serves as the primary workspace for modern operating systems, is volatile. It requires active electrical flow to maintain its state. When the system is powered off, the state is lost; upon power-up, the memory cells contain random garbage data. Consequently, the CPU cannot fetch valid instructions from standard RAM immediately after a reset.

To resolve this, hardware architects map the Reset Vector address to a non-volatile memory region, typically Flash Memory or Read-Only Memory (ROM), which retains data without power.

\subsubsection{Embedded vs. Complex Architectures}
In simple embedded architectures (e.g., microcontrollers used in automotive braking systems or household appliances), the entire application code is often stored in this non-volatile memory. The memory controller maps this storage directly into the CPU's addressable space. This technique, known as \textbf{Execute In Place (XIP)}, allows the CPU to fetch and execute the developer's code from the very first clock cycle \cite{ARM-CortexM4-Generic-User-Guide}. The developer "owns" the machine from the first nanosecond.

In contrast, more complex architectures (such as ARM-based smartphones or single-board computers like the Raspberry Pi) often store the main operating system on external, complex storage media like SD cards or eMMC chips. The CPU cannot simply memory-map an SD card; it requires a sophisticated software driver to communicate with the storage controller. To bridge this gap, manufacturers embed a tiny, immutable piece of software called the \textbf{BootROM} directly into the silicon. This code initializes the minimal required hardware (often internal SRAM) and loads a secondary bootloader from the external storage into that SRAM, which in turn loads the main software.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    % Standard block style
    block/.style={
        rectangle, 
        draw=black!70, 
        rounded corners=3pt, 
        minimum width=2.2cm, 
        minimum height=1cm, 
        align=center, 
        fill=white,
        font=\small
    },
    % Style for the "Silicon" container
    soc/.style={
        rectangle,
        draw=black!40,
        dashed,
        fill=gray!5,
        rounded corners=5pt,
        inner sep=0.5cm
    },
    % Arrow styles
    arrow/.style={-Latex, thick, color=black!80},
    data/.style={-Latex, thick, dashed, color=black!80},
    % Label style for arrows
    lbl/.style={
        font=\footnotesize\bfseries, 
        fill=white, 
        inner sep=1pt,
        text=black!80
    }
]

% =========================================================
% LEFT SIDE: SIMPLE XIP
% =========================================================

\node (cpu1) [block, fill=blue!10] {CPU};
\node (flash) [block, below=1.5cm of cpu1, fill=orange!10] {NOR Flash\\(Memory Mapped)};
\node (label1) [above=0.1cm of cpu1, font=\bfseries] {Simple (XIP)};

% Arrow
\draw[arrow] (cpu1) -- node[midway, right, font=\footnotesize] {Direct Fetch} (flash);


% =========================================================
% RIGHT SIDE: COMPLEX BOOT
% =========================================================

% We place CPU2 to the right
\node (cpu2) [block, right=6cm of cpu1, fill=blue!10] {CPU};

% Define relative positions for BootROM and SRAM relative to CPU2
\node (bootrom) [block, below left=1.2cm and -0.5cm of cpu2, fill=gray!20] {BootROM\\(Immutable)};
\node (sram) [block, below right=1.2cm and -0.5cm of cpu2, fill=green!10] {Internal\\SRAM};

% Draw the SoC Boundary around them
\begin{scope}[on background layer]
    \node (soc_box) [soc, fit=(cpu2) (bootrom) (sram), label={[anchor=south west, inner sep=5pt]north west:\tiny System on Chip (SoC)}] {};
\end{scope}

% External Storage below the SoC
\node (sdcard) [block, below=1.0cm of soc_box, fill=orange!10] {SD Card / Disk\\(External)};
\node (label2) [above=0.5cm of soc_box, font=\bfseries] {Complex (Bootload)};


% =========================================================
% ARROWS & FLOW (Complex)
% =========================================================

% 1. Power On -> BootROM
\draw[arrow] (cpu2) -- node[lbl, pos=0.6] {1. Init} (bootrom);

% 2. BootROM -> External
\draw[arrow] (bootrom) -- node[lbl, pos=0.4] {2. Load} (sdcard);

% 3. External -> SRAM (Data copy)
\draw[data] (sdcard) -- node[lbl, pos=0.4] {3. Copy} (sram);

% 4. CPU -> SRAM (Execution)
\draw[arrow] (cpu2) -- node[lbl, pos=0.6] {4. Jump} (sram);

\end{tikzpicture}
\caption{Comparison of Boot Architectures}
\label{fig:boot_methods}
\end{figure}

On modern "heavy" machines, such as x86-64 workstations, the initialization process is exponentially more complex. The main CPU is fragile and dependent on a specific environment to function. Before the main cores can execute a single instruction, the hardware requires:
\begin{enumerate}
    \item \textbf{Power Sequencing:} Multiple voltage rails (Vcore, VccSA, VccIO) must be brought up in a specific order with millisecond-precision timing.
    \item \textbf{Clock Stabilization:} Phase-Locked Loops (PLLs) must be tuned and stabilized to generate the gigahertz-range frequencies required by the cores.
    \item \textbf{DRAM Training:} Modern DDR4/DDR5 memory requires a complex calibration process to align signal timing before it becomes usable.
\end{enumerate}
\cite{Intel-Datasheet-Vol1}

To manage this, modern chipsets often include a smaller, dedicated processor (e.g., the Intel Management Engine or AMD Platform Security Processor) that starts before the main CPU. This co-processor initializes the platform hardware to a state where the main CPU can begin execution.

\subsubsection{The Chain of Trust and Abstraction}
\label{subsubsec:chain_of_trust_and_abstraction}
By the time a modern Operating System kernel begins execution, it is likely the fourth or fifth program in the boot chain. The entity responsible for defining the interface between the hardware and the OS is the \textbf{System Firmware} \cite{UEFI-Base-Spec, UEFI-PI-Spec}.

The firmware's responsibility is to abstract the diverse implementations of different motherboards (e.g., how the disk controller is wired) and provide a mechanism to load an OS from a disk into RAM. However, relying solely on firmware is often insufficient for a portable operating system:
\begin{itemize}
    \item \textbf{Inconsistent State:} Different firmware implementations may leave the CPU in varying states (e.g., different interrupt configurations or privilege modes).
    \item \textbf{Feature Limitations:} Firmware is designed to be simple and compatible, often leaving the CPU in a conservative, low-feature mode with caches or advanced vector units disabled.
    \item \textbf{Interface Variance:} The method used to retrieve a memory map or video configuration can vary wildly between hardware generations.
\end{itemize}

To solve this, a \textbf{Third-Party Bootloader} is often utilized. This program acts as a "Normalizer" \cite{Limine-Spec}. It knows how to talk to various firmware types and storage devices. Its job is to abstract away the firmware differences, load the kernel file into memory, and pass control to the OS in a unified, predictable manner.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1cm,
    stage/.style={rectangle, draw, fill=white, text width=4cm, align=center, minimum height=1cm},
    arrow/.style={-Latex, thick}
]

\node (power) [stage, fill=gray!10] {\textbf{Hardware Power-On}\\(Reset Vector)};
\node (firmware) [stage, below=of power] {\textbf{System Firmware}\\(BIOS / UEFI)};
\node (loader) [stage, below=of firmware] {\textbf{Bootloader}\\(GRUB / Limine)};
\node (trampoline) [stage, below=of loader] {\textbf{OS Trampoline}\\(Arch Specific)};
\node (kernel) [stage, below=of trampoline, fill=gray!30] {\textbf{Kernel Main}\\(Arch Agnostic)};

\draw[arrow] (power) -- node[right, font=\footnotesize] {Init Platform} (firmware);
\draw[arrow] (firmware) -- node[right, font=\footnotesize] {Load from Disk} (loader);
\draw[arrow] (loader) -- node[right, font=\footnotesize] {Normalize State} (trampoline);
\draw[arrow] (trampoline) -- node[right, font=\footnotesize] {Enable Features (AVX, Paging)} (kernel);

\end{tikzpicture}
\caption{Typical Boot Chain for x86\_64}
\label{fig:boot_chain}
\end{figure}

\subsubsection{The OS-Level Trampoline}
Even with a standardized bootloader, the kernel cannot assume full control immediately. A generic bootloader cannot know the specific internal requirements of the OS. For instance:
\begin{itemize}
    \item The kernel may typically require memory to be mapped to a specific virtual address range (e.g., the higher half).
    \item Specific hardware features (like Floating Point Units or Virtualization Extensions) are often disabled by default to save power and must be explicitly enabled.
    \item The OS must define its own memory protection structures to enforce its specific security model.
\end{itemize}

Therefore, a robust operating system must implement its own initialization stage, effectively an \textbf{OS-Level Bootloader} or "Trampoline." This architecture-specific code is responsible for taking the machine from the normalized state provided by the external bootloader, enabling the specific CPU features required by the kernel, and establishing the runtime environment before passing control to the architecture-agnostic kernel main function.

\section{Memory Preloading and Discovery}
\label{sec:mem_discovery}
One of the first and most critical responsibilities of a kernel during the bootstrap phase is to establish an authoritative map of the system's physical memory. Unlike user-space applications, which simply request memory from the operating system via system calls (e.g., \texttt{malloc} or \texttt{mmap}), the kernel is the manager responsible for fulfilling those requests. Upon entry, the kernel does not know how much RAM is available, where it is located, or which memory ranges are reserved for hardware mapped I/O (MMIO).

This discovery process is not standardized. It is strictly coupled to the target architecture, the silicon vendor, and the residing firmware. Depending on the platform complexity, the kernel may acquire the memory map through one of three primary mechanisms: static definition, firmware interrogation, or hardware description structures.

\subsection{Static Definition}

On strictly embedded architectures (e.g., ARM Cortex-M or AVR), the physical memory layout is immutable. The location and size of SRAM banks, Flash storage, and peripheral registers are defined by the silicon vendor and do not change. In these environments, runtime discovery is redundant.

The memory map is hardcoded directly into the kernel's source code or linker scripts, matching the specific System-on-Chip (SoC) datasheet \cite{ARM-CortexM4-Generic-User-Guide}. The developer explicitly defines the boundary between kernel code, stack, and heap. As illustrated in Figure \ref{fig:cortex_m4_memory}, the address space is rigid; the kernel assumes ownership of specific addresses immediately upon reset without querying external entities.

In this context, the Operating System does not "discover" memory. The kernel code assumes these addresses are valid from the first instruction. For example, a Cortex-M4 kernel may be hardcoded to expect code at 0x00000000 and RAM at 0x20000000. If the software is flashed onto a different chip variant, it will simply fault; flexibility is sacrificed for minimizing initialization overhead.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    % Main memory block style
    memblock/.style={
        rectangle, 
        draw=black, 
        thick,
        minimum width=4.0cm, 
        align=center, 
        anchor=south,
        outer sep=0pt
    },
    % Left-side exploded box style
    sideblock/.style={
        rectangle,
        draw=black,
        minimum width=3.8cm,
        text width=3.5cm,
        align=center,
        font=\sffamily\scriptsize,
        fill=white
    },
    % Address label style
    addr/.style={
        font=\ttfamily\scriptsize
    }
]

% ==========================================
% 1. DRAW MAIN MEMORY STACK
% ==========================================

% Code
\node[memblock, minimum height=1.5cm] (code) at (0,0) {Code\\ \scriptsize 0.5GB};

% SRAM
\node[memblock, minimum height=1.5cm, above=0cm of code] (sram) {SRAM\\ \scriptsize 0.5GB};

% Peripheral
\node[memblock, minimum height=1.5cm, above=0cm of sram] (periph) {Peripheral\\ \scriptsize 0.5GB};

% External RAM
\node[memblock, minimum height=2.0cm, above=0cm of periph] (extram) {External RAM\\ \scriptsize 1.0GB};

% External Device
\node[memblock, minimum height=2.0cm, above=0cm of extram] (extdev) {External Device\\ \scriptsize 1.0GB};

% PPB
\node[memblock, minimum height=0.6cm, above=0cm of extdev] (ppb) {Private peripheral\\bus \scriptsize 1.0MB};

% Vendor
\node[memblock, minimum height=1.0cm, above=0cm of ppb] (vendor) {Vendor-specific\\memory \scriptsize 511MB};


% ==========================================
% 2. ADDRESS LABELS (RIGHT SIDE)
% ==========================================
\node[addr, anchor=south west] at (vendor.north east) {0xFFFFFFFF};
\node[addr, anchor=south west] at (vendor.south east) {0xE0100000};
\node[addr, anchor=south west] at (ppb.south east)    {0xE0000000};
\node[addr, anchor=south west] at (extdev.south east) {0xA0000000};
\node[addr, anchor=south west] at (extram.south east) {0x60000000};
\node[addr, anchor=south west] at (periph.south east) {0x40000000};
\node[addr, anchor=south west] at (sram.south east)   {0x20000000};
\node[addr, anchor=south west] at (code.south east)   {0x00000000};


% ==========================================
% 3. LEFT SIDE BIT-BANDING (MANUAL PLACEMENT)
% ==========================================

% - SRAM GROUP -
% Define the target point (Base of SRAM)
\coordinate (sram_target) at (sram.south west);

% 1MB Region Box (Level with SRAM base)
\node[sideblock, anchor=east] (sram_region) at (-2.5, 1.2) {1MB Bit band region};
% Address for 1MB Region
\node[addr, anchor=east] at (sram_region.south west) {0x20000000};
\node[addr, anchor=east] at (sram_region.north west) {0x200FFFFF};

% Alias Box (Floated above)
\node[sideblock, anchor=east, minimum height=0.8cm] (sram_alias) at (-2.5, 2.5) {32MB Bit band alias};
% Address for Alias
\node[addr, anchor=east] at (sram_alias.south west) {0x22000000};
\node[addr, anchor=east] at (sram_alias.north west) {0x23FFFFFF};

% Connect lines for SRAM
\draw (sram_alias.north east) -- (sram_target);
\draw (sram_alias.south east) -- (sram_target);
\draw (sram_region.north east) -- (sram_target);
\draw (sram_region.south east) -- (sram_target);


% - PERIPHERAL GROUP -
% Define the target point (Base of Peripheral)
\coordinate (periph_target) at (periph.south west);

% 1MB Region Box (Level with Peripheral base)
\node[sideblock, anchor=east] (periph_region) at (-2.5, 4.2) {1MB Bit band region};
% Address for 1MB Region
\node[addr, anchor=east] at (periph_region.south west) {0x40000000};
\node[addr, anchor=east] at (periph_region.north west) {0x400FFFFF};

% Alias Box (Floated above)
\node[sideblock, anchor=east, minimum height=0.8cm] (periph_alias) at (-2.5, 5.5) {32MB Bit band alias};
% Address for Alias
\node[addr, anchor=east] at (periph_alias.south west) {0x42000000};
\node[addr, anchor=east] at (periph_alias.north west) {0x43FFFFFF};

% Connect lines for Peripheral
\draw (periph_alias.north east) -- (periph_target);
\draw (periph_alias.south east) -- (periph_target);
\draw (periph_region.north east) -- (periph_target);
\draw (periph_region.south east) -- (periph_target);

\end{tikzpicture}
\caption{Cortex-M4 Memory Map with Bit-banding regions (Adapted from \cite{ARM-CortexM4-Generic-User-Guide})}
\label{fig:cortex_m4_memory}
\end{figure}

\subsection{Flattened Device Tree (DTB)}

To allow a single kernel binary to support multiple board configurations without hardcoding, architectures such as ARM64 and RISC-V utilize the \textbf{Flattened Device Tree}. The bootloader passes a pointer to a binary structure (the DTB Blob) which describes the hardware topology, including physical memory ranges and reserved regions.

The DTB format encodes the device tree into a linear, pointerless data structure. It consists of a fixed-size header followed by three variable-sized blocks:
\begin{enumerate}
    \item \textbf{Memory Reservation Block}: Lists physical memory ranges that the kernel must not overwrite (e.g., firmware runtime data).
    \item \textbf{Structure Block}: Describes the device nodes and properties in a linear tree format using token-based tags.
    \item \textbf{Strings Block}: A pool of null-terminated property names referenced by offset.
\end{enumerate}

The kernel parses this blob at boot to discover available RAM.

\cite{Devicetree-Spec}

\subsection{Firmware Interrogation}

On general-purpose platforms (x86-64), the hardware is modular. The kernel cannot predict the amount of installed RAM or the physical address map. In this scenario, the kernel must query the system firmware directly. This introduces a dependency on the firmware interface:
\begin{itemize}
    \item \textbf{Legacy BIOS:} Requires invoking interrupt vectors (e.g., \texttt{INT 0x15, EAX=0xE820}) to retrieve a list of memory ranges.
    \item \textbf{UEFI:} Requires calling specific boot services (\texttt{GetMemoryMap}) to retrieve descriptors of physical pages and their attributes.
\end{itemize}

\cite{UEFI-Base-Spec}

\subsection{Hardware Abstraction}
How does the kernel handle such diverse set of methods of querying the memory? This is a part of a bigger topic - namely Hardware / Architecture Abstraction - and wil be discussed in more detail in \todo[inline]{add label to hardware abstraction layer / problem}

\subsection{Discovering and Enabling CPU Features}
\label{subsec:cpu_discovery}

Modern Central Processing Units (CPUs) are not monolithic entities with a fixed feature set. Instead, they represent an accumulation of decades of architectural extensions. A generic x86-64 processor guarantees a baseline instruction set (User-level ISA), but specific capabilities regarding vectorization (AVX, AVX-512), cryptography (AES-NI), security (SMEP, SMAP), and performance (PCID, TSC-Deadline) vary significantly between processor generations and manufacturers.
\cite{Intel-AVX}

An operating system kernel cannot blindly execute advanced instructions. Doing so on hardware that lacks support results in an \textit{Invalid Opcode} exception, causing a kernel panic. Therefore, a robust kernel must perform a feature discovery handshake during the early initialization phase.

\subsubsection{Feature Identification}
On the x86 architecture, this discovery is performed via the \texttt{CPUID} instruction. This instruction acts as a query interface where the software loads a leaf index into the \texttt{EAX} register (and optionally a subleaf in \texttt{ECX}) and executes \texttt{CPUID}. The processor returns feature bitmaps and vendor information in the general-purpose registers (\texttt{EAX}, \texttt{EBX}, \texttt{ECX}, \texttt{EDX}).
\cite{Intel-CPUID}

While x86 relies on this dynamic instruction-based discovery, other architectures employ different strategies:
\begin{itemize}
    \item \textbf{ARM64 (AArch64)} utilizes special system registers (e.g., \texttt{ID\_AA64PFR0\_EL1}) that the kernel reads to determine support for floating-point units or cryptographic extensions.
    \item \textbf{RISC-V} typically employs the Device Tree Blob (DTB) or the \texttt{misa} (Machine ISA) Control and Status Register to inform the kernel about supported standard extensions (e.g., Atomics, Floats).
\end{itemize}
\cite{ARM-Arch-Ref, RISCV-Priv-Spec}

\subsubsection{Feature Enablement}
Identifying that a feature exists is often insufficient; the kernel must explicitly enable it. To maintain backward compatibility and minimize power consumption, processors often boot with advanced features disabled.

A prime example is the Floating Point Unit (FPU) and Vector Extensions (SSE/AVX). On x86-64, even if \texttt{CPUID} reports that AVX is supported, attempting to execute a \texttt{VMOVDQA} instruction will fault unless the kernel has:
\begin{enumerate}
    \item Enabled the FPU by clearing the Emulation bit in Control Register \texttt{CR0}.
    \item Enabled SSE by setting the \texttt{OSFXSR} bit in \texttt{CR4}.
    \item Enabled XSAVE/XRSTOR support by setting the \texttt{OSXSAVE} bit in \texttt{CR4}.
    \item Explicitly enabled AVX state saving in the Extended Control Register (\texttt{XCR0}).
\end{enumerate}
\cite{Intel-ControlRegisters}

This enabling phase is critical not only for allowing instruction execution but also for the scheduler. The operating system must know the size of the processor's register state (Context) to correctly save and restore threads during context switches. If AVX-512 is enabled, the context size increases significantly compared to standard SSE, impacting memory usage and context switch latency.

\cite{Intel-XSAVE}

\subsection{Establishment of Basic Communication}

One of the primary objectives when initializing code on a target architecture is to establish an external communication channel. In an emulation environment, this is often achieved by interacting with the emulator's framework (e.g., QEMU utilizes a serial port that can be attached to a Linux shell session). On physical hardware, the developer may need to render fonts on a screen (e.g., using the VGA standard on AMD64 desktop platforms) or implement a basic network stack. The preferred method should support bidirectional communication during the early development stages to facilitate testing and provide input to the kernel. This functionality is primarily required for debugging and testing and as development progresses, it is advisable to disable this communication and associated debug traces via compilation flags. It is crucial to strictly separate debug-only communication from standard output devices, as the former must be disabled in release builds to ensure performance and security.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{res/debug-output.png}
    \caption{QEMU serial-port Communication}
    \label{fig:qemu_comms}
\end{figure}

As shown in Figure \ref{fig:qemu_comms}, alongside typical output to the screen and keyboard input, the kernel transmits text to the QEMU serial port, which is then streamed to the host shell. This approach enables host-side scripts to parse logs, detect bugs or failures, and allow manual inspection of the system state immediately preceding a crash.

\subsection{Enabling Interrupts and Exceptions}
\label{subsec:os-tutorial-interrupts}

Prior to the implementation of memory management (Section \ref{subsec:os_tutorial_mem}), it is essential to establish a basic interrupt handling mechanism. On most platforms, the exception system relies entirely on interrupt mappings. Consequently, without a functional interrupts, the kernel is unable to display debug information on the designated communication device when an error occurs.

Exception handlers should output a descriptive message detailing the failure. This message must include the CPU state, the source of the exception, the instruction pointer where the error occurred, and, if applicable, an error code explaining the cause. An example of a kernel dump following an exception is presented below:

\begin{lstlisting}[caption={Example Kernel Panic on Exception}, label={lst:kernel_panic}]
    [ KERNEL PANIC ] Received exception: 0 (Divide Error (#DE))
    And error: 0
    At instruction address: 0xffffffff8021b856
    rip:                    0xffffffff8021b856
    rflags:                 0x0000000000010246
    rsp:                    0xffffffff8138dc20
    rax:                    0x0000000000000009
    rbx:                    0x000000000006f000
    rcx:                    0x0000000000000000
    rdx:                    0x0000000000000000
    rsi:                    0xffffffff803648f3
    rdi:                    0xffffffff80818c28
    rbp:                    0xffffffff8138dc40
    r8:                     0xffffffff80305d00
    r9:                     0xffffffff80305d59
    r10:                    0xffffffff8138df10
    r11:                    0xffffffff802fdfaf
    r12:                    0x0000000000000010
    r13:                    0x0000000000180000
    r14:                    0x0000000001a91000
    r15:                    0x0000000000000800
    
    RFLAGS:                 0x0000000000010246
\end{lstlisting}

Before initializing Memory Management, exception handling is the primary function required from the interrupt system. At this stage, there is typically no use case for interrupt-driven devices, as the consumers (processes) have not yet been booted, and multicore operations (which require memory for bookkeeping structures) are not active. However, the design of the interrupt subsystem must account for future extensibility and requirements.

\subsubsection{Design Considerations}

It is important to note that low-level, hardware-compliant interrupt handlers differ significantly from standard compiled functions. They often require specific entry and exit instructions, making the runtime swapping of these functions difficult. A robust solution is to create a low-level assembly wrapper that efficiently performs all architecture-dependent operations before invoking a high-level function responsible for the kernel's logic. However, this approach has limitations. The high-level function is typically hardcoded into the low-level handler, preventing runtime reconfiguration.

To address this, a hardware abstraction layer is required to serve as an intermediary between the architecture-specific wrapper and the high-level kernel logic. This abstraction should manage the low-level tables and facilitate the swapping of handlers at runtime. This can be implemented using an array of abstract handlers mapped to hardware interrupts. Furthermore, devices are mapped to hardware interrupts dynamically during runtime based on user configuration and hardware discovery, necessitating the ability to swap drivers and logic dynamically. Without this modularity, the system would require multiple stages of interrupt initialization, multiple architecture-specific handlers, or complex handlers that query the kernel state to determine valid operations (e.g., a page fault handler checking for virtualization support each time is invoked).

It is also critical to design handlers to perform minimal work. This precludes operations such as waiting, sleeping, or extensive tracing. If device handling requires complex logic or a state change, the handler should invoke the scheduler to perform a context switch upon exiting the interrupt. Executing complex logic within the interrupt context blocks other interrupts and may lead to data loss. Consequently, the interrupt abstraction layer must also support task switching.

When implementing this abstraction, three distinct classes of interrupts can be identified:

\begin{itemize}
    \item \textbf{Exceptions} -- Generated by the CPU to indicate specific conditions requiring immediate attention, such as Page Faults, Division by Zero, or invalid instruction operands.
    \item \textbf{Hardware Interrupts} -- Generated by external devices (e.g., Timers, Keyboard, Disk Controller) to communicate efficiently with the kernel without the need for polling.
    \item \textbf{Software Interrupts} -- Initiated by software instructions. For example, on the x86-64 architecture, the instruction \texttt{INT 0x80} triggers an interrupt with the vector number \texttt{0x80}.
\end{itemize}

Architecture-specific details must also be considered. for example, the AMD64 architecture utilizes the legacy PIC \cite{osdev-pic} and the improved, SMP-supporting APIC \cite{osdev-apic}. Finally, to enable Symmetric Multiprocessing (SMP) and utilize multiple cores, the interrupt mechanism serves as the primary method of inter-core communication.

\subsection{Tracing System}

As discussed in Section \ref{subsec:os-tutorial-interrupts}, direct tracing inside interrupt handlers is generally discouraged due to latency concerns. However, in a general context, kernel logs and debug messages must be preserved to a file or terminal. The challenge is that writing to a file or physical device incurs significant latency, while system code must execute as rapidly as possible. To resolve this, the tracing framework must be robust enough to operate in a concurrent environment (handling interrupts and SMP) while decoupling the generation of traces from their output. This can be achieved by allocating a large circular buffer for messages, which is then asynchronously flushed to the output device by a dedicated, low-priority task. Furthermore, to facilitate effective debugging in a multi-threaded and multi-core environment, each log entry should automatically capture essential context metadata, specifically the high-precision timestamp, the current Core ID, and the Process ID.

This goes about the general design, but during initial development system there is no need to have full tracing system, simple print with formatting is enough to proceed.

\subsection{Testing framework}

Once bidirectional communication with an external system is established and tracing capabilities are active, it is advisable to implement a protocol for verifying code directly on the target system. Such a framework facilitates regression and unit testing, enabling the detection of failures with every code modification. Testing must be conducted on the target architecture, as code that functions correctly on a host Linux environment may fail locally due to missing CPU features, implementation differences in system calls, memory management constraints, or discrepancies in generated assembly. The design must isolate test executions to ensure that the side effects of a failed test do not compromise subsequent ones. Ideally, isolation is achieved by rebooting the system into a testing module after each test. While this method introduces latency, it prevents false positives that would otherwise consume significant development time. Consequently, the framework must be designed to yield reliable and repeatable results.

\section{Memory Management}
\label{subsec:os_tutorial_mem}
TODO: KRYCZKA

\section{Physical Memory Management}
\label{subsubsec:physical_memory_management}

The Physical Memory Manager (PMM) constitutes the foundational layer of the operating system's memory subsystem. Its primary responsibility is the accounting of the machine's finite Random Access Memory (RAM). In a monolithic kernel with virtual memory support, the PMM typically operates at two distinct levels of granularity:

\begin{enumerate}
\item \textbf{Page Frame Allocation:} The allocation of raw, contiguous physical memory in fixed-size units called \textit{page frames} (typically 4096 bytes on x86-64). This is primarily used to back Virtual Memory mappings for user-space processes.
\item \textbf{Kernel Heap Allocation:} The sub-allocation of memory within those pages for the kernel's own internal data structures (e.g., thread control blocks, file descriptors). These requests vary wildly in size, from a few bytes to several kilobytes.
\end{enumerate}

While the granularity differs, the fundamental algorithmic challenge remains the same: how to satisfy a request of size $S$ efficiently while minimizing wasted space. The remainder of this section explores the theoretical constraints and algorithmic solutions common to both layers, starting with the arch-nemesis of memory management: fragmentation.

\subsection{The Core Challenge: Fragmentation}

If an allocator simply handed out memory sequentially and never had to accept returns (frees), the implementation would be a trivial pointer increment. Complexity arises because memory is borrowed and returned in an arbitrary order. This leads to \textbf{fragmentation}: the inability to reuse memory that is technically free. Fragmentation manifests in two distinct forms \cite{wilson1995survey}:

\begin{itemize}
    \item \textbf{Internal Fragmentation:} This occurs when the allocator assigns a block larger than what was requested. For example, if a request for 20 bytes is rounded up to a 32-byte block to satisfy alignment requirements, 12 bytes are wasted. This waste is "internal" to the allocated block and unusable by others.
    \item \textbf{External Fragmentation:} This occurs when free memory exists in the system, but it is scattered in small, non-contiguous holes such that a request for a large contiguous block cannot be satisfied. The total free memory might be sufficient, but the geometry of the holes prevents allocation.
\end{itemize}

\textbf{The Root Cause: Isolated Deaths.} Fragmentation fundamentally arises from \textit{isolated deaths} - when an object is freed but its neighbors remain allocated, creating an unusable hole. If adjacent objects always died together, their combined space would be reclaimed as one contiguous block. The allocator's core challenge is predicting which objects will die at similar times and placing them contiguously. Since the allocator cannot know future behavior, it must rely on heuristics that exploit regularities in program behavior~\cite{wilson1995survey}.

The goal of any allocator is to balance the minimization of these two types of fragmentation against the computational cost (latency) of the allocation operation.

\subsection{Anatomy of an Allocator}

To understand how different allocators address these challenges, it is helpful to use the taxonomy proposed by Wilson et al. \cite{wilson1995survey}, which separates allocator design into three levels of abstraction:

\subsubsection{1. Strategy}
The strategy is the high-level philosophy used to combat fragmentation. It relies on heuristics about program behavior. Research into allocation traces reveals three dominant patterns \cite{wilson1995survey}:

\begin{itemize}
    \item \textbf{Ramps:} Monotonic accumulation of long-lived data (e.g., building a parse tree).
    \item \textbf{Peaks:} Bursty allocation of temporary structures that are collectively discarded after a phase (e.g., processing one request).
    \item \textbf{Plateaus:} Rapid initial allocation followed by stable long-term usage.
\end{itemize}

These patterns are exploitable: objects allocated together often die together. By clustering objects that are likely to die at the same time, the allocator increases the probability that large contiguous blocks will be freed simultaneously.

\subsubsection{2. Policy}
The policy is the specific decision logic used to select a free block for a given request. Common policies include:
\begin{itemize}
    \item \textbf{First Fit:} Scan the free memory from the beginning and return the first block that is large enough. This is fast but can accumulate small "splinters" of free memory at the start of the list.
    \item \textbf{Best Fit:} Search the entire list to find the smallest block that satisfies the request. This minimizes the wasted remainder (unused space after the split) but requires a comprehensive search, which can be slow.
    \item \textbf{Next Fit}: Resume searching from where the last allocation stopped. Scatters allocations and often
performs poorly
\end{itemize}

\subsubsection{3. Mechanism}
The mechanism is the data structure and algorithm used to implement the policy efficiently. For example, a "Best Fit" policy could be implemented via a linear scan of a linked list (mechanism: $O(N)$), or by looking up a size index in a tree or bitmap (mechanism: $O(log N)$ or $O(1)$).

The following subsections examine specific mechanisms used in OS development, ordered by increasing complexity.

\subsection{Mechanism: Bitmap Allocation}

The bitmap allocator is conceptually the simplest mechanism, often used for \textbf{Page Frame Allocation} due to its static nature. In this scheme, the physical memory is divided into fixed-size units (pages). A separate region of memory, the bitmap, tracks the status of these units. Each bit in the bitmap corresponds to one page frame: a 0 indicates free, and a 1 indicates allocated.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    bit/.style={draw, minimum width=0.5cm, minimum height=0.5cm, anchor=south west},
    freeb/.style={bit, fill=green!20},
    usedb/.style={bit, fill=red!20}
]

% Draw bitmap array
\node[anchor=east] at (-0.2, 0.25) {\textbf{Bitmap:}};
\node[freeb] at (0, 0) {0};
\node[usedb] at (0.5, 0) {1};
\node[usedb] at (1.0, 0) {1};
\node[freeb] at (1.5, 0) {0};
\node[freeb] at (2.0, 0) {0};
\node[usedb] at (2.5, 0) {1};
\node[freeb] at (3.0, 0) {0};
\node[freeb] at (3.5, 0) {0};
\node at (4.2, 0.25) {...};

% Index labels
\node[font=\tiny] at (0.25, -0.3) {0};
\node[font=\tiny] at (0.75, -0.3) {1};
\node[font=\tiny] at (1.25, -0.3) {2};
\node[font=\tiny] at (1.75, -0.3) {3};
\node[font=\tiny] at (2.25, -0.3) {4};
\node[font=\tiny] at (2.75, -0.3) {5};
\node[font=\tiny] at (3.25, -0.3) {6};
\node[font=\tiny] at (3.75, -0.3) {7};

% Physical addresses
    \node[anchor=west, font=\scriptsize] at (5, 0.25) {Frame $i$ $\rightarrow$ Address $i \times PageSize$};

\end{tikzpicture}
\caption{Bitmap Representation of Physical Memory}
\label{fig:bitmap_allocator}
\end{figure}

The primary advantage of bitmaps is space efficiency. To manage 4 GiB of RAM using 4 KiB pages, there are roughly 1 million frames. This requires 1 million bits, or merely 128 KiB of overhead. Furthermore, bitmaps naturally support the allocation of \textit{contiguous} multipage blocks; the allocator simply needs to find a run of $N$ consecutive zeros.

However, the search complexity is linear. To find free memory, the allocator must scan the bitmap. In the worst case (a nearly full system), it may scan the entire map ($O(N)$) before finding a free page or determining that memory is exhausted.

\subsection{Mechanism: Stack-Based Allocation (Free Lists)}

While bitmaps handle contiguous allocations well, their linear search time is often unacceptable for the most frequent operation: allocating a single page. To achieve constant-time $O(1)$ performance, many kernels employ a \textbf{Stack} or \textbf{Linked List} allocator.

In this scheme, the allocator maintains a pointer to a "head" page. This page contains a pointer to the next free page, which points to the next, forming a chain. Since these pages are by definition unused, the OS can store the "next" pointer inside the page itself, requiring no external metadata storage.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    page/.style={draw, minimum width=2.5cm, minimum height=0.8cm, fill=green!10},
    arrow/.style={-Latex, thick}
]

\node[page] (p1) at (0, 0) {Page @ 0x5000};
\node[page] (p2) at (0, -1.2) {Page @ 0x9000};
\node[page] (p3) at (0, -2.4) {Page @ 0x2000};
\node[font=\scriptsize] at (0, -3.4) {...};

\node[anchor=east, font=\bfseries] at (-1.8, 0) {HEAD $\rightarrow$};

\draw[arrow] (p1.south) -- (p2.north);
\draw[arrow] (p2.south) -- (p3.north);

\node[anchor=west, align=left, font=\scriptsize] at (2, -0.6) {Pop: $O(1)$\\Push: $O(1)$};
\node[anchor=west, align=left, font=\scriptsize, text=red!70!black] at (2, -2.0) {Loss of Contiguity};

\end{tikzpicture}
\caption{Stack-Based Free List Allocator}
\label{fig:stack_allocator}
\end{figure}

The stack acts as a cache of "hot" pages. When a page is freed, it is pushed onto the stack; when a page is requested, it is popped. This is extremely fast but suffers from a fatal flaw regarding fragmentation: the stack order reflects \textit{execution history}, not \textit{physical address} order. A stack might contain physical page 0x1000 followed by page 0x5000. If a driver requests two contiguous pages, the stack cannot satisfy the request even if the system has plenty of memory. Consequently, stacks are often used as a fast frontend cache layered on top of a more complex backing allocator.

\subsection{Mechanism: The Buddy System}

To solve the contiguity problem while maintaining decent performance, general-purpose operating systems (like Linux) utilize the \textbf{Buddy System}. The Buddy System is a specific implementation of a broader class of allocators known as \textbf{Segregated Fits}, and its theoretical foundations are thoroughly described by Knuth \cite[Section 2.5]{knuth1973art}.

\subsubsection{Concept: Segregated Fits}
In a segregated fit architecture, memory is not viewed as one monolithic pool. Instead, the allocator maintains an array of free lists. Each list acts as a bin dedicated to blocks of a specific size class. For example, index 0 might hold 4KiB blocks, index 1 hold 8KiB blocks, and so on. This allows the allocator to quickly locate a block that fits a request without searching through blocks that are vastly too large or too small.

\subsubsection{The Binary Buddy Algorithm}
The Buddy System applies a strict discipline to segregated fits: all block sizes must be powers of two. The allocator maintains an array of lists for orders $0$ to $MAX\_ORDER$. Order $k$ typically represents a block size of $PAGE\_SIZE \times 2^k$.

\textbf{Splitting (Allocation):} When a request for a block of order $k$ arrives:
\begin{enumerate}
    \item The allocator checks the list at index $k$.
    \item If the list is empty, it moves up to list $k+1$, splits a larger block into two halves (buddies), adds one half to list $k$, and returns the other.
    \item If list $k+1$ is also empty, it recurses upward until a block is found or memory is exhausted.
\end{enumerate}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    block/.style={draw, minimum height=0.6cm, fill=blue!10},
    freeblock/.style={draw, minimum height=0.6cm, fill=green!20},
    arrow/.style={-Latex, thick, dashed}
]

% Original block
\node[block, minimum width=8cm] (orig) at (0, 2) {Order 3 Block (32 KiB)};

% Split into two Order 2
\node[freeblock, minimum width=4cm] (l2a) at (-2, 0.8) {Order 2 (16 KiB)};
\node[block, minimum width=4cm] (l2b) at (2, 0.8) {Order 2 (16 KiB)};

% Further split left Order 2
\node[freeblock, minimum width=2cm] (l1a) at (-3, -0.4) {Order 1};
\node[block, minimum width=2cm] (l1b) at (-1, -0.4) {Order 1};

% Further split
\node[freeblock, minimum width=1cm] (l0a) at (-3.5, -1.6) {O0};
\node[block, minimum width=1cm, fill=red!20] (l0b) at (-2.5, -1.6) {O0};

\draw[arrow] (orig.south) -- (l2a.north);
\draw[arrow] (orig.south) -- (l2b.north);
\draw[arrow] (l2a.south) -- (l1a.north);
\draw[arrow] (l2a.south) -- (l1b.north);
\draw[arrow] (l1a.south) -- (l0a.north);
\draw[arrow] (l1a.south) -- (l0b.north);

\node[anchor=west, font=\scriptsize] at (4.5, 0) {Recursive splitting};
\node[anchor=west, font=\scriptsize, text=red!70!black] at (-0.5, -1.6) {Allocated};
\node[anchor=west, font=\scriptsize, text=green!50!black] at (-3, -2.2) {Buddy (free)};

\end{tikzpicture}
\caption{Buddy System: Splitting on Allocation}
\label{fig:buddy_split}
\end{figure}

\textbf{Coalescing (Freeing):} The elegance of the Buddy System lies in how it handles fragmentation. Every block has a unique "buddy" that is its physical neighbor in memory. When a block is freed, the allocator checks if its buddy is also free. If so, they merge (coalesce) into a block of the next higher order. This repeats recursively.

Crucially, the address of a block's buddy can be calculated instantly using an XOR operation, without traversing lists. For a block of size $2^k$ at address $A$, the buddy is located at:
\[ \texttt{buddy\_addr} = A \oplus 2^k \]

\subsubsection{Why Buddy is Insufficient for Real-Time}
While the Buddy System is efficient ($O(log N)$) and handles external fragmentation well via coalescing, it suffers from significant \textbf{Internal Fragmentation}. Because every request must be rounded up to a power of two, a request for 33 KiB requires a 64 KiB block, wasting 47\% of the allocated memory.

Furthermore, the coalescing logic is restrictive. Two adjacent free blocks of size $2^k$ cannot always be merged; they must be \textit{buddies} (i.e., aligned on a strict $2^{k+1}$ boundary). This can lead to cases where contiguous memory is available but unusable because the blocks are "cousins" rather than "buddies" \cite{wilson1995survey}. These limitations motivate the need for more granular allocators in real-time systems, such as TLSF.

\subsection{Mechanism: Two-Level Segregated Fit (TLSF)}

While the Buddy System is fast, its reliance on power-of-two sizing creates unacceptable internal fragmentation for workloads that frequently allocate objects of irregular sizes (e.g., 50 bytes or 800 bytes). Additionally, real-time systems require a strict guarantee of constant-time $O(1)$ performance, regardless of the fragmentation state of the heap.

The \textbf{Two-Level Segregated Fit (TLSF)} allocator \cite{masmano2004tlsf} was designed specifically to address these requirements. It is a segregated fit allocator (like Buddy) but with a much finer granularity of size classes and a more flexible coalescing strategy.

\subsubsection{Addressing Internal Fragmentation: The Two-Level Index}
To reduce internal fragmentation, an allocator needs size classes that are closer together than powers of two. However, having thousands of free lists (one for 10 bytes, one for 11, etc.) makes searching for a free block slow. TLSF solves this by organizing free lists into a two-dimensional matrix, denoted by indices $(f, s)$.

\begin{enumerate}
    \item \textbf{First-Level Index (FLI):} This corresponds to the power-of-two bucket, roughly equivalent to the "Order" in the Buddy System. It is calculated as the position of the most significant bit set in the size: $f = \lfloor \log_2(\text{size}) \rfloor$.
    \item \textbf{Second-Level Index (SLI):} To bridge the large gap between powers of two (e.g., the gap between 256 and 512 is 256 bytes wide), TLSF subdivides each FLI bucket into $2^{SLI}$ linearly spaced sub-buckets.
\end{enumerate}

For example, if we configure the allocator with $SLI = 2$ (which means 4 subdivisions), the range $[256, 512)$ is split into 4 bands: $[256, 320)$, $[320, 384)$, $[384, 448)$, and $[448, 512)$. A request for 260 bytes no longer needs to be rounded up to 512; it fits into the first subdivision. This significantly tightens the bound on internal fragmentation.

\subsubsection{O(1) Search via Bitmaps}
The core innovation of TLSF is how it finds a suitable free block in constant time. The allocator maintains a set of \textbf{Bitmaps} that act as a summary of the free lists.
\begin{itemize}
    \item A bit is set to \textbf{1} if the corresponding free list contains at least one block.
    \item A bit is set to \textbf{0} if the list is empty.
\end{itemize}

When a request arrives, the allocator maps the size to indices $(f, s)$. It checks the bitmap for that list. If the bit is 0 (empty), it does not need to loop through lists. Instead, it uses a hardware instruction - \textit{Find First Set} (\texttt{ffs}) or \textit{Count Leading Zeros} (\texttt{clz}) - to atomically find the index of the next available bit in the bitmap. This operation allows the allocator to skip over empty size classes instantly, guaranteeing $O(1)$ allocation time regardless of the heap state.

\subsubsection{Physical Block Management: Boundary Tags}
The Buddy System calculates the address of a neighbor using bitwise XOR. This is fast, but it restricts merging: a block at address 0x2000 of size 0x1000 can \textit{only} merge with its specific buddy at 0x3000. It cannot merge with a free block at 0x1000, even though they are physically adjacent.

TLSF abandons the "buddy" logic in favor of \textbf{Boundary Tags} to support merging any physically adjacent free blocks.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\footnotesize,
    block/.style={draw, minimum height=1cm, anchor=west},
    header/.style={draw, minimum height=0.3cm, fill=gray!30, anchor=west},
    freeblk/.style={block, fill=green!15},
    usedblk/.style={block, fill=red!15}
]

% Memory layout
\node[header, minimum width=0.8cm] (h1) at (0, 0) {\tiny H};
\node[freeblk, minimum width=2cm] (b1) at (0.8, 0) {Free (64B)};
\node[header, minimum width=0.8cm] (h2) at (2.8, 0) {\tiny H};
\node[usedblk, minimum width=3cm] (b2) at (3.6, 0) {Used (96B)};
\node[header, minimum width=0.8cm] (h3) at (6.6, 0) {\tiny H};
\node[freeblk, minimum width=2cm] (b3) at (7.4, 0) {Free (48B)};

% Arrows for neighbor check
\draw[-Latex, thick, blue] (b2.north west) to[out=120, in=60] node[above, font=\scriptsize] {check prev} (b1.north east);
\draw[-Latex, thick, blue] (b2.north east) to[out=60, in=120] node[above, font=\scriptsize] {check next} (b3.north west);

\node[anchor=north, font=\scriptsize] at (4.5, -0.8) {Headers store size $\rightarrow$ locate neighbors in $O(1)$};

\end{tikzpicture}
\caption{Boundary Tags Enable Arbitrary Coalescing}
\label{fig:tlsf_coalesce}
\end{figure}

Every block in memory (whether free or allocated) is prefixed with a header containing its size and a flag indicating if the \textit{previous} physical block is free.
\begin{itemize}
    \item \textbf{Freeing:} When block $B$ is freed, it checks its header to see if the previous neighbor is free. If so, it reads the previous neighbor's footer (located immediately before $B$'s header) to find its start address and merges with it. It then checks the next neighbor (at address $B + size$). If that is also free, it merges.
    \item \textbf{Result:} This allows TLSF to counteract external fragmentation much more aggressively than the Buddy System, as any contiguous free space is immediately coalesced into the largest possible block.
\end{itemize}

\subsubsection{Summary of Trade-offs}
TLSF is the preferred allocator for real-time kernels because it guarantees worst-case execution time ($O(1)$) and minimizes wasted memory (low fragmentation). However, it is slightly slower on average than the Buddy System due to the overhead of maintaining the doubly-linked lists for every size class and the boundary tag manipulation. For general-purpose operating systems where average throughput is more important than worst-case latency, the Buddy System is often retained.

\subsection{Advanced Layering: Object Caching (Slab)}

The mechanisms described so far (Buddy, TLSF) manage raw blocks of memory. However, the operating system kernel rarely wants just "64 bytes"; it usually wants a specific object, such as a process control block, a file descriptor, or a synchronization primitive.

Initializing these objects can be expensive. For example, initializing a kernel mutex involves setting internal flags, initializing wait queues, and perhaps interacting with hardware interrupt controllers. If an object is allocated, initialized, used, destroyed, and freed repeatedly, the CPU spends significant cycles just setting up and tearing down the same state.

The \textbf{Slab Allocator}, introduced by Jeff Bonwick for SunOS \cite{bonwick1994slab}, solves this by observing that the state of a freed object is often valid for the next allocation. It separates the concepts of \textit{memory release} and \textit{object destruction}.

\subsubsection{The Concept of Object Caching}
A Slab Allocator organizes memory into caches of specific object types (e.g., a cache for \texttt{task\_structs}, a cache for \texttt{inodes}).
\begin{itemize}
    \item \textbf{Construction:} When the cache grows, it allocates a page from the underlying PMM and calls a \textit{constructor} on every slot in that page. For a mutex, this initializes the lock state.
    \item \textbf{Allocation:} When the kernel requests an object, the allocator pops one from the cache. Crucially, the object is \textit{already constructed}. The mutex is ready to be acquired immediately.
    \item \textbf{Freeing:} When the kernel releases the object, the allocator returns it to the cache but \textit{does not} call the destructor. The state (the initialized mutex) remains intact.
    \item \textbf{Reclamation:} Only when the system is under extreme memory pressure does the allocator reclaim the slab. It calls the \textit{destructor} on the objects and returns the raw pages to the PMM.
\end{itemize}

This effectively turns memory management into a caching problem. The allocator caches \textit{constructed state}, reducing the overhead of allocation to little more than pointer arithmetic.

\subsection{Scalability: Allocating on Multicore Systems}

In a Symmetric Multiprocessing (SMP) environment, a single global heap becomes a major bottleneck. If all cores must acquire the same spinlock to call \texttt{malloc}, the system's performance will degrade as core counts rise due to lock contention.

To solve this, modern allocators use a two-tiered hierarchy:
\begin{enumerate}
    \item \textbf{Per-CPU Caches:} Each processor is assigned a small local heap (a cache of free blocks). Allocations are satisfied from this local cache without acquiring a global lock, as no other CPU touches this memory.
    \item \textbf{Global Heap:} When a local cache empties, it grabs a batch of memory from the global heap (requiring a lock). When a local cache overflows, it returns a batch to the global heap.
\end{enumerate}

\subsubsection{The Hoard Allocator}
A naive implementation of per-CPU heaps suffers from a specific pathology known as \textit{blowup}. Consider a "Producer-Consumer" pattern where Thread A (on CPU 1) continuously allocates packets, and Thread B (on CPU 2) consumes and frees them. CPU 1's heap constantly empties (requesting more from global), and CPU 2's heap constantly fills up (never returning to global). The system runs out of memory despite having ample free space, because that space is trapped on CPU 2.

The \textbf{Hoard Allocator} \cite{berger2000hoard} addresses this by organizing memory into \textit{Superblocks} - large chunks of memory containing multiple objects of the same size. Hoard tracks the "emptiness" of these superblocks. If a superblock on a local heap becomes mostly empty, it is moved to the global heap, allowing other processors to reuse the memory. This guarantees that the memory consumption of the allocator is bounded within a constant factor of the ideal required memory, solving the blowup problem while maintaining scalability.

\subsection{Hardware Constraints: Zoning}

Finally, the PMM must respect the physical limitations of the hardware. Not all RAM is created equal.
\begin{itemize}
    \item \textbf{DMA Limitations:} Legacy hardware (like ISA devices) may only be able to address the bottom 16 MiB of RAM (24-bit addressing).
    \item \textbf{High Memory:} On 32-bit architectures with more than 4 GiB of RAM, the kernel cannot map all physical memory into its virtual address space simultaneously.
    \item \textbf{NUMA Topology:} On multi-socket systems, physical memory is partitioned across NUMA nodes. Allocating from a remote node incurs latency penalties, so the PMM should prefer local memory when possible.
\end{itemize}

A common solution is to partition the physical address space into disjoint \textbf{regions} or \textbf{zones}, each managed by a separate allocator instance. Allocation requests carry metadata (flags or capabilities) indicating which regions are acceptable. The allocator attempts the preferred region first and falls back to others only if permitted. Critically, a request for DMA-capable memory \textit{cannot} fall back to high memory-the hardware simply cannot address those frames.

For an in-depth treatment of allocation algorithms, fragmentation theory, and experimental methodology, the comprehensive survey by Wilson et al.~\cite{wilson1995survey} remains an essential reference.


\subsubsection{Virtual Memory Management}
TODO: KRYCZKA
\subsubsection{Virtual Address Space}
TODO: KRYCZKA
\subsubsection{Kernel Space Allocation API}
TODO: KRYCZKA

\subsection{Discovering External Devices and System Capabilities}

Upon the successful initialization of the fully functional memory management subsystem, the kernel can proceed to discover external devices and implement subsystems for capabilities such as storage, input, and networking. It is not necessary to support an exhaustive list of peripherals to achieve a minimal working kernel. Therefore, this section covers only the absolute minimum set of devices required to support the scheduler and execute user-space programs.

\subsubsection{Discovery and Abstraction (Drivers)}

Device discovery varies significantly between architectures, necessitating platform-specific implementations. For instance, on most desktop AMD64 computers, the ACPI subsystem \cite{ACPI-spec} is required to enumerate core components (such as CPUs, APICs, and non-PCI devices), followed by PCI discovery \cite{osdev-pci} to detect devices operating on PCI lanes. Architecture-independent kernel must provide some unified way allowing the archtiecture to perform the discovery at a proper moment. Furthermore it must provide well defined abstraction for various device types, which must be strictly followed by architecture implementation. Example driver interface may look like this:

\begin{lstlisting}[caption={Event Clock Driver Structure}, label={lst:eventClockDriver-tutorial}]
    struct alignas(arch::kCacheLineSizeBytes) EventClockRegistryEntry : data_structures::RegistryEntry {
        /* Clock numbers */
        u64 min_next_event_time_ns;  // Minimum time for the next event in nanoseconds
    
        /* Clock specific data */
        EventClockFlags flags;     // Features of the event clock, e.g., core-local
        CoreMask supported_cores;  // Cores that support this event clock
    
        /* infra data */
        u64 next_event_time_ns;  // Time for the next event in nanoseconds
        EventClockState state;   // Current state of the clock
    
        /* Driver data */
        void *own_data;  // Pointer to the clock's own data, used for callback
    
        /* callbacks */
        struct callbacks {
            // Callback to set next event time
            u32 (*next_event)(EventClockRegistryEntry *, u64);  
            // Callback to set clock state
            u32 (*set_oneshot)(EventClockRegistryEntry *);
            // Callback to set clock state
            u32 (*set_periodic)(EventClockRegistryEntry *);     
            void (*on_entry)(EventClockRegistryEntry *);        // optional
            void (*on_exit)(EventClockRegistryEntry *);         // optional
        } cbs;
    };
\end{lstlisting}

This structure must define crucial operations for the specific device type alongside data describing its functionality, allowing the kernel to select the most appropriate driver either through user configuration or automatic selection algorithms.

\subsubsection{Core Local Storage}

Implementing an SMP kernel necessitates an infrastructure for accessing core-local data early in the boot process to facilitate future scalability and prevent extensive refactoring. This initialization must occur after memory setup and CPU topology discovery, as structures must be allocated for each core individually. In advanced development stages, numerous per-core structures are required to eliminate locking overhead and enhance performance in critical sections. Furthermore, data strictly correlated with the execution context, such as the currently running thread, must be easily accessible for the core we are runnig on. Consequently, the architecture must provide a mechanism, such as a dedicated register or instruction, to access these unique core-specific areas. On the x86-64 architecture, this is typically implemented using the \texttt{GS} or \texttt{FS} segment registers. Since the full 64-bit base address cannot be loaded into these segment selectors directly, the kernel must utilize Model Specific Registers (specifically \texttt{IA32\_GS\_BASE} or \texttt{IA32\_KERNEL\_GS\_BASE}) to map the per-core structure to the logical address space. The \texttt{SWAPGS}\cite{osdev-gs} instruction is then employed during interrupt entry and exit to switch between user-space and kernel-space thread-local storage transparently.

\begin{lstlisting}[caption={Example Core Local Usage}, label={lst:core-local}]
    void cdecl_UpdateTcbOnSyscallEntry()
    {
        const auto thread = hardware::GetCoreLocalTcb();
        const u64 t       = TimingModule::Get().GetSystemTime().ReadLifeTimeNs();
        thread->user_time_ns += t - thread->timestamp;
        thread->timestamp = t;
        thread->num_syscalls++;
    };
\end{lstlisting}

\subsubsection{Timing Design}

Before discussing timing devices, a fundamental design decision must be made regarding the kernel type: \texttt{tickless} or \texttt{ticking}. Ticking kernels rely on a global constant defining the frequency of periodic timing interrupts. These values typically operate with millisecond granularity, as higher frequencies would degrade system performance. In contrast, tickless kernels do not define periodic interrupts. Instead, they schedule the next timing event based on immediate requirements, such as the sleep schedule or the time slice of the currently running process. It is important to note that ticking kernels also require higher granularity interrupts if support for \texttt{nanosleep} is needed \ref{subsec:linux-timing}.

\subsubsection{Clocks}

The first category of devices required is a simple clock capable of tracking the time elapsed since system startup, primarily for gathering statistics and maintaining timekeeping. These are essential for placing tasks on a timeline and scheduling interrupts for high-precision sleep operations. Without such capabilities, waking tasks would require iterating over all sleeping entities to decrement remaining time, resulting in $O(n)$ complexity. Since interrupt handlers must execute rapidly, tasks should be managed in a high-performance timeline based priority queue, such as a Red-Black tree, which guarantees stability and $O(\log n)$ removal time. On AMD64, the TSC clock \cite{IntelManual-TSC} is suitable for measuring system uptime (sufficient for the scheduler), while the RTC is used for wall-clock time displayed to the user. If multiple timers are available, the kernel should select the one offering the best performance-accuracy ratio.

\subsubsection{Event Clocks}

At this stage, the approach to timing event logic must be established, depending on the target architecture and the capabilities exposed to users. 

The first consideration is sleep granularity. A standard precision of approximately one millisecond is achievable on nearly every architecture using periodic interrupts. However, implementing high-precision nanosecond or microsecond sleep requires timer devices capable of fast, reprogrammable one-shot interrupts (e.g., the LAPIC Timer on AMD64), as handling periodic interrupts at microsecond intervals is technically infeasible.

Another critical factor is the support for multiple cores. In an SMP environment, it is preferable to utilize timing devices that reside locally on the cores and operate independently, thereby maximizing performance. While most modern architectures provide such devices (e.g., LAPIC Timer on AMD64), some older or embedded SMP platforms may lack them. In such cases, the kernel must utilize a global timer and employ interrupt-driven inter-core communication to simulate local interrupts.

Consequently, a suitable timing device must be selected. When supporting multiple architectures, the kernel typically implements drivers for all available options and dynamically selects the appropriate one based on device capabilities.

An example of device availability schemes by architecture is as follows:
\begin{itemize}
\item SMP fast local one-shot timer $\rightarrow$ core local interrupt system + nanosleep mechanism available.
\item SMP local periodic-only timer $\rightarrow$ core local interrupt system + low precision msleep only available.
\item SMP non-local one-shot timer $\rightarrow$ owner core of timer interrupts propagates the interrupt to target cores + nanosleep.
\item SMP non-local periodic-only timer $\rightarrow$ owner core of timer interrupts propagates the interrupt to target cores + low precision msleep only available.
\end{itemize}

Most modern architectures support both local and high-precision timers. For example, on AMD64 with SMP, the presence of local LAPIC Timers \cite{IntelManual-APIC} (high precision, one-shot) is guaranteed, significantly simplifying the implementation.

\subsubsection{Keyboard}

Lastly we need some way to provide input to the kernel on the real machine not only emulated environment. For that we will need some keyboard handling. First easier way is to utilize some simple ports like PS-2 \cite{osdev-ps21}\cite{osdev-ps22} if are available on our device. Otherwise we will probably have to implement some usb drivers what will be much more difficult \cite{usb20-spec}\cite{osdev-usb}.

\subsection{File Systems}
TODO: ADAM

\subsection{Screen Drawing}
TODO: KRYCZKA

\subsection{Scheduling}

The fundamental unit for resource management is the process. This object represents the execution environment for the threads operating within it. Consequently, it must act as a container for all resources shared between these threads. At a minimum, a process structure must manage:

\begin{itemize}
    \item \textbf{Page Tables} -- Architecture-specific structures describing the mapping between virtual and physical addresses.
    \item \textbf{Mapping Metadata} -- Structures enabling the tracking and allocation of virtual address ranges.
    \item \textbf{File Descriptor Table} -- Descriptors facilitating access to files and I/O streams.
\end{itemize}

While additional resources may be required to implement features such as Inter-Process Communication (IPC), pipes, or synchronization primitives, the three elements listed above constitute the bare minimum for the process concept. A straightforward implementation involves defining separate structures for \texttt{Thread} and \texttt{Process} objects, although some designs merge these into a single task structure (as discussed in Section \ref{subsec:linux-task}).
At this stage, it is prudent to prepare data structures that allow for efficient querying of processes, facilitating operations such as termination, joining, or IPC. It is important to note that the creation and destruction of processes are computationally expensive operations. They require building page tables, mapping the kernel address space, and allocating memory for bookkeeping structures. To mitigate latency, some of this workload, particularly during destruction, can be deferred to dedicated kernel worker threads, allowing the system to return control to the caller more rapidly.

\noindent The minimal stuct may look like this:
\begin{lstlisting}[caption={Process Structure}, label={lst:processStruct}]
    struct Process {
        /* Process resources */
        data_structures::DoubleLinkedList<VMemArea *> area_list;
        PPtr<void> page_table_root;
        Mem::VPtr<Fs::FdTable> fd_table;
    };
\end{lstlisting}

In a production environment, this structure would naturally include additional fields for management purposes, such as synchronization primitives, statistics, configuration flags, and a state enumeration for lifecycle management.

\subsubsection{Thread}

The second primary component of the scheduling subsystem is the thread. The thread represents the fundamental unit of execution - the actual code consuming CPU time and utilizing resources from its parent process. To effectively restore, start, and switch execution, the system must track the thread's state, which includes CPU registers, the instruction pointer, and architecture-specific flags. All of this state information can be compressed into a single field: the thread's stack pointer.

This approach simplifies the architecture significantly and enhances performance. Since the stack is heavily utilized, it is typically cached, eliminating the need to allocate separate memory regions for saving context. Consequently, all context-switching operations are unified to use the stack as the storage medium for the thread's state. When saving context, registers are pushed onto the stack. When initializing a thread, the stack is artificially populated with boot-up values.

\noindent A concise initial structure can be defined as follows:
\begin{lstlisting}[caption={Thread Structure First Iteration}, label={lst:os-tutorial-thread-v0}]
struct Thread {
    Process* owner;

    void *user_stack;
    void *user_stack_bottom;
};
\end{lstlisting}

To manage the thread lifecycle, it is necessary to introduce a state enumeration. This field allows the scheduler to determine the appropriate action for a given thread. For instance, a waiting thread must be removed from the run queue, while a blocked thread must be removed from the specific wait queue it is blocked on.

\noindent The structure is updated to reflect these requirements:
\begin{lstlisting}[caption={Thread Structure Second Iteration}, label={lst:os-tutorial-thread-v1}]
enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kBlocked,
    kTerminated,
};

struct Thread {
    Process* owner;
    ThreadState state;

    void *user_stack;
    void *user_stack_bottom;
};
\end{lstlisting}

A critical design consideration regarding context switching is the location where the state is saved. Saving the execution state directly onto the user stack is inherently insecure. A malicious program could manipulate the stack pointer to overwrite critical kernel structures, or the kernel could unintentionally leak sensitive data (such as cryptographic keys) onto the user stack during a switch. Furthermore, to allow kernel code to be preempted or to block (i.e., perform a context switch while inside the kernel), the kernel state must be preserved. This necessitates a separate, protected kernel stack for each thread. Relying on a single kernel stack per core is insufficient for a preemptible kernel.

\noindent The final minimal structure is presented below:
\begin{lstlisting}[caption={Thread Structure Third Iteration}, label={lst:os-tutorial-thread-v2}]
enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kBlocked,
    kTerminated,
};

struct Thread {
    Process* owner;
    ThreadState state;

    void *kernel_stack;
    void *kernel_stack_bottom;
    void *user_stack;
    void *user_stack_bottom;
};
\end{lstlisting}

\subsubsection{Context Creation}
\label{subsec:context-init}

With the context storage mechanism defined, the next step is context initialization. This requires a function responsible for fabricating a context frame on the thread's stack, adhering to a predefined schema. This schema must mirror the layout of registers and CPU flags expected by the context restoration logic. To create a new context, the stack is populated with initial values: zeros for general-purpose registers (or specific function arguments, depending on the ABI), the address of the entry point function, and architecture-specific default flags.

\subsubsection{Context Conversion}

Before a context switch can occur, an initial context must exist. This implies the creation of the first process and the preparation of all associated structures described previously, utilizing the context initialization logic \ref{subsec:context-init}. Several approaches exist to achieve this; for example, the kernel may spawn an initial user-space program (init) responsible for booting other programs, or it may create a kernel-space process to perform further system initialization.

\subsubsection{Context Switch}

Once multiple tasks are established, the system can perform context switching. Assuming the entire execution state resides on the stack, the context switch operation is relatively straightforward. The sequence of operations is as follows:

\begin{itemize}
    \item Push the current thread's context (registers and flags) onto its stack.
    \item Perform any necessary bookkeeping or state updates.
    \item Switch the stack pointer to the target thread's stack.
    \item Restore the state (registers and flags) from the new stack.
\end{itemize}

\noindent Based on AMD64 assembly, the implementation resembles the following:
\begin{lstlisting}[style=nasmstyle, caption={Assembly ISR wrapper}, label={lst:isr_asm}]
; c_decl
; void ContextSwitch(Thread* thread)
;   RDI = thread
ContextSwitch:
    sub rsp, _context_switch_stack_space_ext     ; Allocate space for saving registers.
    push_all_regs                                ; Save the state

    ; ... Some other booking, state changes -> depends on supported features
.done:
    mov rsp, [rdi+Thread.kernel_stack]   ; Change the stack

    pop_all_regs            ; Restore registers of NEW thread's stack
    add rsp, _all_reg_size  ; Deallocate register save space.
    iretq                   ; Load next thread's RIP from its stack
\end{lstlisting}

\subsubsection{Scheduler}

Having thread and process management done last thing to implement we need some machinery to pick best suitable task to run next on preemption or during yielding. And that is the main responsiblity of the scheduler to track the execution of the threads and always pick most suitable tasks to run. There are multiple approaches to design the the scheduler itself and multiple algorithms picking the task

\subsubsection{SMP Scheduling}
TODO: JAKUB

\subsubsection{Synchronization}
TODO: JAKUB

\subsubsection{Sleep}
TODO: JAKUB

\subsubsection{Blocking - Wait Queues}
TODO: JAKUB

\subsection{User Space}
TODO: JAKUB

\subsubsection{Syscalls}
\label{subsubsec:syscalls}

System calls (syscalls) are the primary interface through which userspace applications request services from the operating system kernel. Modern processors enforce a strict separation between unprivileged applications and the operating system kernel to ensure stability and security. This privilege separation is architecturally defined: x86-64 uses Protection Rings (Ring 3 for users, Ring 0 for kernel) \cite{IntelManual-Protection-Rings}, ARM64 uses Exception Levels (EL0 for users, EL1 for kernel) \cite{ARM-Arch-Ref-Exception-Levels}, and RISC-V uses Privilege Modes (User Mode and Supervisor Mode) \cite{RISCV-Priv-Spec-Privilege-Levels}.

A system call is a synchronous transition wherein an application explicitly requests a service from the kernel. This process involves privilege escalation, switching stacks, performing the operation, and safely returning to the unprivileged state.

\paragraph{Design Considerations}

Designing an efficient and secure syscall mechanism is crucial for overall system performance and stability. The syscall interface must balance ease of use, performance, and security. Key considerations include:

\begin{itemize}
    \item \textbf{Performance:} Syscalls should introduce minimal overhead to avoid bottlenecks in application performance. Techniques such as fast syscall instructions and optimized context switching are essential.
    \item \textbf{Security:} The syscall interface must enforce strict access controls to prevent unauthorized access to kernel resources. This includes validating parameters and ensuring that userspace applications cannot compromise kernel integrity.
    \item \textbf{Compatibility:} The syscall interface should maintain compatibility with established standards such as POSIX to ensure portability of existing software.
    \item \textbf{Extensibility:} The syscall interface should be designed to accommodate future extensions without breaking existing applications. This can be achieved through versioning and reserved syscall numbers.
\end{itemize}

\paragraph{Triggering Mechanisms}

There are two primary methods for implementing system calls, distinguished by the balance between simplicity and performance.

\begin{enumerate}
    \item \textbf{Software Interrupts:} Historically, systems used software interrupts (e.g., \texttt{int 0x80} on x86) to trigger syscalls. In this model, the CPU looks up a handler in the Interrupt Descriptor Table (IDT), performs extensive security checks (privilege verification, segment checks), and automatically pushes the execution context onto the kernel stack. 
    
    While robust and easy to debug (as it unifies syscalls with exception handling), this mechanism is slow, typically consuming approximately 100--200 CPU cycles per call depending on the architecture. It is often the recommended starting point for new OS projects due to its implementation simplicity.

    \item \textbf{Fast System Call Instructions:} Modern architectures provide dedicated instructions optimized for low-latency transitions (approximately 30--50 cycles). These bypass the complex interrupt logic but require the kernel to perform more manual state management.
    \begin{itemize}
        \item \textbf{x86-64 (\texttt{syscall}/\texttt{sysret}):} The CPU loads the kernel entry point and code segments from Model-Specific Registers (MSRs). The CPU does not switch the stack pointer automatically. The kernel must explicitly save the user stack and load the kernel stack immediately upon entry.
        \item \textbf{ARM64 (\texttt{svc}/\texttt{eret}):} The processor saves the state and return address into specific system registers (SPSR and ELR) \cite{ARM-Arch-Ref-Exception-Levels}. Unlike x86-64, ARM64 hardware automatically switches the stack pointer to the kernel's stack (\texttt{SP\_EL1}).
        \item \textbf{RISC-V (\texttt{ecall}/\texttt{sret}):} Following a minimalist philosophy, the hardware does little more than jump to the trap vector and switch privilege modes \cite{RISCV-Priv-Spec-System-Call}. Specifically, the hardware automatically saves the return address to \texttt{mepc} (or \texttt{sepc}), records the exception cause in \texttt{mcause} (or \texttt{scause}), updates the privilege bits in \texttt{mstatus} (or \texttt{sstatus}), and disables interrupts. However, the kernel is responsible for saving all general-purpose registers and manually swapping the stack pointer, typically using a scratch register.
    \end{itemize}
\end{enumerate}

\paragraph{The Application Binary Interface (ABI)}

Once the processor transitions to kernel mode, the kernel must identify which service is requested and where the arguments are located. This is defined by the ABI. To maximize performance, the ABI specifies that arguments are passed in CPU registers rather than on the stack. This avoids the overhead of the kernel accessing user memory directly, which creates security risks.

\paragraph{Dispatch and Execution}

Upon entry to the kernel, the raw system call number must be mapped to a specific internal kernel function. The standard and most efficient mechanism for this is a plain Dispatch Table. This approach utilizes a contiguous array of function pointers, where the system call number serves as the index. The dispatch logic is minimal:
\begin{enumerate}
    \item \textbf{Bounds Check:} The kernel verifies that the requested number is less than the total number of supported syscalls.
    \item \textbf{Invocation:} The kernel executes an indirect call to the address stored at \texttt{table[syscall\_number]}.
\end{enumerate}
This method guarantees $O(1)$ constant-time performance, ensuring that the latency of invoking a system call remains minimal. This approach requires syscall numbers to be contiguous; gaps in the numbering space are typically handled by null pointers or stub functions in the table.

\paragraph{Security and Validation}

The syscall interface is the primary attack surface of the kernel. The kernel must treat all data from userspace as untrusted.
\begin{itemize}
    \item \textbf{Pointer Validation:} The kernel must ensure that pointers passed by the user actually point to valid userspace memory. Failing to perform this check allows a malicious program to trick the kernel into reading or overwriting its own internal structures (see the "Confused Deputy" problem \cite{confused_deputy_problem}).
    \item \textbf{TOCTOU (Time-of-Check to Time-of-Use):} If the kernel validates data in user memory and reads it later, a separate thread could modify that data in between. For example, a kernel might check that a filename pointer is valid, but before it reads the actual filename, another thread could modify that memory to point to a privileged file. To prevent this, the kernel must copy data into kernel-space buffers \textit{before} validation and processing.
\end{itemize}

\paragraph{Performance Optimization}

For high-frequency operations where even the minimal syscall overhead is too high (e.g., querying the system time), Linux employs \textbf{vDSO} (virtual Dynamic Shared Object). This mechanism maps a read-only page of kernel memory containing specific data and code into the user process. The application can then execute a standard function call to read this data without ever triggering a context switch or entering kernel mode \cite{linux_vdso}.

\subsubsection{Libc System Headers}

Although the system call interface constitutes the fundamental mechanism for interaction with the kernel, it operates at a level of abstraction that is unsuitable for general-purpose application development. In practice, application programmers do not invoke syscalls directly via assembly instructions. Instead, they rely on the \texttt{libc}, whose header files define the API between userspace software and the operating system.

In the context of custom operating system development, system headers are commonly structured into three distinct categories in order to ensure clarity, portability, and standards compliance.

\paragraph{Standard ISO C Headers}
To enable existing software to compile and run on a new operating system, the \texttt{libc} implementation must provide all headers required by the ISO C standard \cite{iso_c_std}. These headers must strictly conform to the standardized function signatures and data types.

Internally, such functions act as abstraction layers over the kernel interface. The implementation of \texttt{fopen} typically allocates a userspace data structure to represent the file stream and subsequently invokes the kernel's syscall to obtain a file descriptor or handle. This design effectively decouples application-level code from kernel-specific details such as syscall numbers, calling conventions, and ABI constraints.

\paragraph{POSIX and System Headers}
In addition to the ISO C standard, many operating systems implement parts of the POSIX specification \cite{posix_std} to improve compatibility with existing UNIX-like software. Headers associated with POSIX functionality are commonly placed within the \texttt{sys/} directory (e.g., \texttt{<sys/types.h>} or \texttt{<sys/stat.h>}).

\paragraph{OS-Specific Extensions}
Custom operating systems frequently introduce features that fall outside the scope of ISO C or POSIX, such as direct frame buffer access, specialized inter-process communication mechanisms, or hardware-specific controls.

It is considered best practice to place operating system-specific extensions within a clearly separated namespace, for example under a dedicated directory such as \texttt{<alkos/...>}. This approach enables applications developed specifically for the target operating system to access unique kernel features.

\subsubsection{Conversion to User Space}
TODO: JAKUB

\section{Host Environment -- Working OS}

\subsection{Root Filesystem (Rootfs)}
\label{subsec:rootfs}

The root filesystem constitutes the fundamental userspace environment of an operating system. It provides the configuration files and userspace programs (e.g., a shell) that render the system usable and interactive. In the context of operating system development, the construction of a rootfs is a structured process that transforms source code and static resources into a binary filesystem image that can be parsed and mounted by the kernel.

Conceptually, the root filesystem construction process can be decomposed into four sequential phases, each responsible for a distinct transformation step within the build pipeline:
\begin{itemize}
    \item \textbf{Staging and Hierarchy Construction:} Creation of a temporary directory tree on the host system that reflects the intended structure of the target filesystem.
    \item \textbf{Artifact Compilation and Population:} Cross-compilation of userspace programs followed by their installation into the corresponding locations within the staging directory.
    \item \textbf{Overlay:} Integration of static configuration files and resources into the staging area to complement the compiled executables.
    \item \textbf{Filesystem Image Creation:} Conversion of the fully populated staging directory into a single binary image adhering to a chosen filesystem format.
\end{itemize}
An overview of this pipeline is illustrated in Figure \ref{fig:rootfs_pipeline}.

\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{res/rootfs-pipeline.svg}
    \caption{Rootfs creation pipeline}
    \label{fig:rootfs_pipeline}
\end{figure}

\subsubsection{Staging and Hierarchy Construction}

Rootfs creation begins with a staging phase, during which the build system generates a temporary directory tree on the host system. This tree mirrors the intended layout of the target operating system's filesystem and serves as an intermediate representation prior to image generation.

For early-stage operating systems, strict adherence to comprehensive standards such as the Linux Filesystem Hierarchy Standard (FHS), which mandates directories such as \texttt{/usr}, \texttt{/var}, and \texttt{/opt}, is often unnecessary. Consequently, a simplified directory structure is typically adopted, for example:
\begin{itemize}
\item \textbf{Binaries Directory (\texttt{/bin}):} Contains executable programs compiled for the target architecture.
\item \textbf{Configuration Directory (\texttt{/cfg} or \texttt{/etc}):} Stores text-based system configuration files.
\item \textbf{Library Directory (\texttt{/lib}):} Holds shared libraries when dynamic linking is supported.
\item \textbf{Drivers Directory (\texttt{/drv}):} Contains kernel-loadable drivers.
\end{itemize}
The staging directory is recreated for each build, ensuring reproducibility and eliminating residual artifacts from previous compilation runs.

\subsubsection{Artifact Compilation and Population}

After establishing the directory hierarchy, the staging area must be populated with executable content. Userspace applications are compiled using a cross-compilation toolchain and linked against standard libraries compatible with the kernel's Application Binary Interface (ABI).

A well-designed build system automates this process and enforces correct dependency ordering. In particular, building the kernel should implicitly trigger the compilation of userspace programs. The resulting binaries are then installed into the appropriate locations within the staging directory (e.g., \texttt{/bin}). This automation minimizes the risk of inconsistencies, such as a kernel attempting to execute outdated userspace binaries that rely on obsolete system calls.

\subsubsection{Configuration Overlays}

Executable binaries alone are insufficient for a functional operating system; configuration data and static resources are also required. These include configuration files, initialization scripts, and other static assets.

A common approach to managing such data is the use of a configuration overlay. In this model, the source repository contains a predefined directory tree of static files. During the build process, this tree is recursively merged into the staging directory, augmenting the compiled binaries. This separation of code and configuration promotes flexibility, as system behavior can be modified (for example, by adjusting startup parameters) without recompilation of userspace programs.

\subsubsection{Filesystem Image Synthesis}

Once the staging directory has been fully populated, it must be transformed into a single binary image conforming to a specific filesystem format, such as FAT, EXT2, or an archive-based formats.

In academic and hobbyist operating system projects, simpler filesystems such as FAT are frequently preferred over more sophisticated journaling filesystems. FAT offers broad host-side tool support, simplifying inspection and debugging, and its relatively simple on-disk structures reduce driver implementation complexity.

Image synthesis is typically performed using host-side utilities (e.g., \texttt{mtools} and \texttt{mkfs.fat}). These tools determine the required image size, format a blank image file, and populate it with the contents of the staging directory. The output is a filesystem image ready for deployment alongside the kernel.

\subsubsection{The Initial Ramdisk and the Bootstrapping Problem}

A fundamental challenge in early operating system development is the so-called “chicken-and-egg” problem associated with storage drivers. In order to mount a root filesystem from persistent storage devices such as NVMe or SATA, the kernel must already possess functioning disk drivers. However, these drivers are commonly stored within the root filesystem itself.

This problem is addressed through the use of an \textbf{Initial Ramdisk (initrd)}. The bootloader loads the entire image into main memory alongside the kernel and passes its memory location as a boot parameter. The kernel can then expose this memory region as a block device and mount it as the root filesystem (\texttt{/}) during early initialization. This approach allows the system to complete its initialization sequence before transitioning to a more permanent storage-backed root filesystem.

\subsection{Compiling Userspace Programs}

The compilation of userspace applications for a custom operating system differs fundamentally from standard application development on a host machine. Standard host toolchains are configured to target the host environment, automatically linking against its standard library and adhering to its specific runtime initialization model. Consequently, generating binaries for a new operating system requires adaptation of the build system to target the new environment. This section outlines the necessary components and configurations needed to compile userspace programs for a custom OS.

\subsubsection{Executable Format}

A standardized executable container format must be selected to serve as a formal contract between the compiler toolchain and the kernel loader. The Executable and Linkable Format (ELF) is the standard for Unix-like systems and custom operating system projects due to its extensibility and broad toolchain support.

ELF binaries utilize \emph{program headers} to describe contiguous regions of the file known as \emph{segments}. The kernel loader parses these headers to determine:
\begin{itemize}
\item \textbf{Load addresses:} The virtual memory addresses at which segments are mapped.
\item \textbf{Memory footprint:} The distinction between file size and memory size, allowing for the automatic zero-initialization of uninitialized data regions (BSS).
\item \textbf{Access permissions:} The hardware-level protection attributes (read, write, execute) required for each segment.
\end{itemize}

For early-stage operating systems, \textbf{static linking} is often the preferred strategy. This approach embeds all library code directly into the executable, avoiding the complexity of implementing a dynamic linker and shared object support within the kernel. Furthermore, compiling as Position Independent Executables (PIE) enables the kernel to load programs at arbitrary memory addresses, a prerequisite for advanced security features such as Address Space Layout Randomization (ASLR).

\subsubsection{Memory Layout}

To enforce the given memory layout, a custom linker script is required. This script groups program sections according to their protection requirements, enabling the kernel to utilize hardware-level security mechanisms such as the NX (No-Execute) bit on the x86\_64 platform. A standard userspace memory layout comprises the following sections in order of increasing virtual address:
\begin{enumerate}
\item \textbf{.text}: Executable code; marked read-only and executable.
\item \textbf{.rodata}: Constants and string literals; marked read-only and non-executable.
\item \textbf{.data}: Initialized global variables; marked read-write and non-executable.
\item \textbf{.bss}: Uninitialized global variables; these occupy no storage space in the binary but are allocated in virtual memory at runtime.
\end{enumerate}

For example, on the x86\_64 architecture adhering to the System V ABI, the \textbf{.text} section typically originates at virtual address \texttt{0x400000} to prevent bugs associated with null pointer dereferences. Subsequent sections are aligned to page boundaries (commonly 4KiB). The userspace stack is conventionally located near the upper boundary of the user address space and grows downward, while the heap is initialized immediately following the \textbf{.bss} section and grows upward.

\subsubsection{Runtime Initialization}

The final stage of the compilation pipeline is the definition of the program entry point and the runtime initialization sequence. While application logic begins within the \texttt{main()} function, the kernel does not invoke this function directly. Instead, the linker is configured to use a dedicated entry symbol, conventionally named \texttt{\_start}.

The code located at \texttt{\_start}, known as the \emph{C Runtime Startup} (CRT0), is a routine linked into every userspace application. Its purpose is to establish the execution environment expected by high-level languages:
\begin{enumerate}
\item \textbf{Stack alignment:} Ensuring compliance with ABI alignment requirements (e.g., 16-byte alignment for SIMD instructions).
\item \textbf{Environment initialization:} Retrieving command-line arguments (\texttt{argv}) passed by the kernel.
\item \textbf{Global lifetime management:} Invoking constructors for global/static objects before \texttt{main()} and destructors after \texttt{main()} returns.
\item \textbf{Process termination:} Capturing the return value of \texttt{main()} and invoking the system call to ensure controlled process termination.
\end{enumerate}
This startup sequence ensures that userspace applications execute within a consistent, initialized environment and terminate in a predictable manner.

\subsubsection{OS-Specific Toolchain Adaptation}

While a generic cross-compiler (e.g., targeting a bare-metal environment such as \texttt{x86\_64-elf}) is sufficient for compiling the kernel itself, it presents limitations for userspace development. Generic toolchains are agnostic to the operating system's filesystem layout and standard library locations, requiring the build system to manually manage header paths and library linkage for every compilation unit. To streamline this process, the toolchain can be patched to target the custom operating system natively (e.g., via a target like \texttt{x86\_64-alkos}). This automates environment detection and dependency resolution, significantly reducing reliance on complex build scripts.

\paragraph{System Root (Sysroot)}

The sysroot is a directory hierarchy on the host machine that mirrors the target operating system's root filesystem. It organizes system headers (e.g., \texttt{sysroot/usr/include}) and compiled libraries (e.g., \texttt{sysroot/usr/lib}) according to standard conventions.

By configuring the cross-compiler with the \texttt{\text{-}\text{-}with-sysroot} option, the toolchain anchors its search paths to this directory. This ensures that inclusion directives, such as \texttt{\#include <stdio.h>}, resolve exclusively to the headers of the custom operating system's standard library, thereby isolating the build process from the host environment.

\paragraph{Implicit Linker Configuration}

An OS-specific toolchain further simplifies the linking phase by implicitly managing runtime dependencies. In a raw compilation setup, the developer must explicitly specify the C runtime startup object (\texttt{crt0.o}), the initialization and finalization objects (\texttt{crti.o}, \texttt{crtbegin.o}, \texttt{crtend.o}, \texttt{crtn.o}), and the appropriate standard library linkage for every userspace program.

In contrast, a customized toolchain encapsulates this configuration internally. Upon invocation, the compiler automatically injects the necessary object files and links against the correct version of the system library. This abstraction allows developers to compile userspace applications with minimal command-line arguments.

\subsubsection{Build System Integration}

The compilation of userspace programs can be automated and synchronized with the kernel build process. The build system manages the dependency graph, ensuring that changes to the system library trigger recompilation of user applications. The resulting binaries are then automatically staged for inclusion in the system image, as described in Section~\ref{subsec:rootfs}.

% ==================================================================

\chapter{Analysis of Existing Solutions}

\section{Linux}

The Linux operating system family is one of the most widely used globally, alongside Windows. The Linux kernel relies on a monolithic architecture. This implies that the kernel has a broad range of responsibilities, while user space contains applications that rely entirely on system capabilities. In practice, all code responsible for memory management, scheduling, interrupt handling, and device drivers resides within the kernel address space. This approach presents various advantages and disadvantages.

\noindent Advantages:
\begin{itemize}
    \item \textbf{Zero-copy and memory efficiency} -- As all components reside in a single address space, data does not need to be copied between distinct address spaces. Passing a pointer is sufficient to transfer data between modules.
    \item \textbf{Minimized context switching} -- Within the single kernel address space, invoking functions from different modules is a direct function call, avoiding the overhead of context switching required by microkernels to access driver services.
    \item \textbf{Simplified source management} -- A single binary structure allows for a unified build scheme. Developing numerous separately compiled services can easily lead to inconsistencies in behavior or data models.
    \item \textbf{Ease of implementation} -- This architecture is often easier to implement for small to mid-sized projects.
\end{itemize}

\noindent Disadvantages:
\begin{itemize}
    \item \textbf{Stability} -- A bug in a single driver, even for an unused device, can compromise the stability of the entire system.
    \item \textbf{Security} -- Vulnerabilities in drivers can potentially expose the kernel address space to malicious user-space execution.
    \item \textbf{Scalability challenges} -- For large-scale projects, strict standards are necessary. Without them, the code base can easily become unmaintainable due to complex webs of references and dependencies.
\end{itemize}

To enhance extensibility and avoid the need for recompilation when adding drivers, Linux employs Loadable Kernel Modules (LKMs) \cite{linux_lkm}. These allow the kernel functionality to be extended dynamically via well-defined interfaces during runtime.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tikzpicture}
            \tikzstyle{every node}=[font=\large]
            \draw [fill={rgb,255:red,242; green,242; blue,242}, thick] (0,0) rectangle (14, 8);
            \node [anchor=north west, font=\bfseries\Large] at (0.3, 7.7) {Kernel Address Space};
            \draw [fill=white, thick, rounded corners=3.0] (0.5, 4.5) rectangle (13.5, 6.5);
            \node [font=\Large, align=center] at (7, 5.5) {Base Linux Kernel Components};
            \tikzstyle{mod}=[draw, fill=white, thick, rounded corners=3.0, minimum width=2cm, minimum height=1.3cm, font=\small]
    
            \node[mod] at (1.5, 3.2) {vfat};
            \node[mod] at (3.7, 3.2) {fat};
            \node[mod] at (5.9, 3.2) {realtek};
            \node[mod] at (8.1, 3.2) {pcspkr};
            \node[mod] at (10.3, 3.2) {kvm-intel};
            \node[mod] at (12.5, 3.2) {kvm};
    
            \node[mod] at (1.5, 1.5) {nvme};
            \node[mod] at (3.7, 1.5) {nvidia};
            \node[mod] at (5.9, 1.5) {rfkill};
            \node[mod] at (8.1, 1.5) {nf\_tables};
            \node[mod] at (10.3, 1.5) {snd};
            \node[mod] at (12.5, 1.5) {\Huge \dots};
    
        \end{tikzpicture}
    }
    \caption{Linux Monolithic Architecture with Loadable Kernel Modules}
    \label{fig:linux_lkm}
\end{figure}

\subsection{Timing}
\label{subsec:linux-timing}

Historically, Linux utilized a strictly periodic tick (defined by hardware), configuring hardware timers to generate interrupts at a fixed frequency (e.g., 100 Hz or 1000 Hz). While this design was straightforward, it imposed limitations on sleep granularity and power efficiency. To address these issues, Linux introduced High Resolution Timers (\texttt{hrtimers}), which allow the kernel to schedule interrupts with nanosecond precision based on immediate needs rather than a fixed cadence. Despite the capabilities of high-resolution timers, the concept of a periodic tick is maintained within the kernel for performance reasons and architectural legacy, by simulating the tick with high precision framework.

This hight precision framework has one major drawback - it can get slow for big number of events as it based on priority queue with $O(\log n)$complexities. Relying exclusively on high-precision mechanisms for every timing event would introduce unacceptable overhead in scenarios involving a massive number of active timers. To mitigate this, Linux maintains the \texttt{jiffies} counter, which increments at the frequency of the system tick. This counter drives the Timer Wheel mechanism \cite{linux_timer_wheel}, a highly efficient algorithm designed for managing low-precision timeouts. The timer wheel offers $O(1)$ complexity for insertion and expiration, making it ideal for subsystems that require the management of thousands or even hundreds of thousands of concurrent events where nanosecond precision is unnecessary. A prime example is the networking stack, which must track timeouts for tens of thousands of open TCP connections. Utilizing high-resolution timers for such a volume would be computationally impossible. The \texttt{jiffies}-based timer wheel allows the system to handle these massive quantities of events with minimal CPU overhead. Consequently, standard API calls such as \texttt{msleep} continue to rely on this coarse-grained, high-performance infrastructure.

\subsection{The Process and Thread Model}
\label{subsec:linux-task}

Unlike many other operating systems that distinguish strictly between processes (containers of resources) and threads (units of execution), Linux treats them almost identically. The core data structure is the \texttt{task\_struct} \cite{linux_task}.
\begin{itemize}
    \item A \textbf{Process} is a \texttt{task\_struct} with a unique memory map and file descriptor table.
    \item A \textbf{Thread} is simply a \texttt{task\_struct} created via the \texttt{clone()} system call with flags such as \texttt{CLONE\_VM} and \texttt{CLONE\_FILES}, causing it to share the address space and resources with its parent.
\end{itemize}

\subsection{Scheduler}

Linux implements several scheduling policies \cite{linux_policies}\cite{linux_cfs} operating on task priorities. There are two major classes: \textbf{Real Time}, which operates on priorities 1-99, and \textbf{Fair}, which operates on priority 0. Tasks with higher priorities preempt those with lower priorities. Linux provides six policy classes:

\begin{itemize}
    \item \texttt{SCHED\_DEADLINE} -- Takes precedence over any other policy and provides real-time capabilities.
    \item \texttt{SCHED\_FIFO} -- A real-time policy where a task runs until it yields control or is preempted by a higher-priority task.
    \item \texttt{SCHED\_RR} (Round Robin) -- A real-time policy where each task is assigned a time slice. Upon exhaustion, the task is moved to the end of the queue.
    \item \texttt{SCHED\_OTHER} -- The standard policy for the majority of non-real-time tasks.
    \item \texttt{SCHED\_BATCH} -- Designed for non-interactive tasks that run for longer periods without context switches, tolerating longer scheduling latencies.
    \item \texttt{SCHED\_IDLE} -- Used for very low priority tasks that run only when the system is otherwise idle.
\end{itemize}

From version 2.6.23 \cite{linux_cfs} up to 6.6 \cite{linux_eevdf}, the \texttt{CFS} (Completely Fair Scheduler) was the default scheduler for the \textbf{Fair} class. Starting with version 6.6, the \texttt{EEVDF} (Earliest Eligible Virtual Deadline First) scheduler began replacing CFS.

\subsubsection{Completely Fair Scheduler}
The Linux documentation summarizes the design of the \textbf{CFS} as follows:

\begin{quote}
80\% of CFS's design can be summed up in a single sentence: CFS basically models an “ideal, precise multi-tasking CPU” on real hardware.

“Ideal multi-tasking CPU” is a (non-existent :-)) CPU that has 100\% physical power and which can run each task at precise equal speed, in parallel, each at 1/nr\_running speed. For example: if there are 2 tasks running, then it runs each at 50\% physical power - i.e., actually in parallel.
    
On real hardware, we can run only a single task at once, so we have to introduce the concept of “virtual runtime.” The virtual runtime of a task specifies when its next timeslice would start execution on the ideal multi-tasking CPU described above. In practice, the virtual runtime of a task is its actual runtime normalized to the total number of running tasks.
\cite{linux_cfs}
\end{quote}

The implementation uses a red-black tree sorted by virtual runtime, representing the execution timeline. The next task selected is the leftmost node in the tree (the task with the least spent execution time) and runs until next another taks becomes the leftmost + some granularity to prevent over-scheduling.

\subsubsection{Earliest Eligible Virtual Deadline First Scheduler}

This algorithm is becoming the new standard for Linux scheduling, addressing shortcomings of the previous \textbf{CFS} implementation. The new approach functions similarly to CFS but operates on deadlines rather than accumulated runtime \cite{linux_eevdf}. This modification allows for better prioritization of latency-sensitive tasks.

\subsection{Memory Management}
\label{subsec:linux_mem}

Linux features centralized memory management with various APIs, including \texttt{kmalloc}, \texttt{kzalloc}, \texttt{vmalloc}, and \texttt{kvmalloc} \cite{linux_mem_interfaces}.

\subsubsection{Physical Memory}
As an architecture-independent kernel, Linux abstracts hardware details. Physical memory is partitioned into zones, each serving a distinct purpose \cite{linux_mem_phys}:

\begin{itemize}
    \item \texttt{ZONE\_DMA} -- Memory suitable for DMA (Direct Memory Access) by devices that cannot access the full addressable range.
    \item \texttt{ZONE\_NORMAL} -- Standard memory directly accessible by the kernel.
    \item \texttt{ZONE\_MOVABLE} -- Similar to \texttt{ZONE\_NORMAL}, but the content of pages in this zone can be migrated to different physical frames (preserving virtual address).
    \item \texttt{ZONE\_DEVICE} -- Memory reserved for specific hardware, such as GPUs, mapped to device drivers.
    \item \texttt{ZONE\_HIGHMEM} -- Memory not covered by permanent kernel mappings, used only by some 32-bit architectures.
\end{itemize}

To enhance multi-core performance, Linux employs a two-step strategy \cite{linux_mem_phys}. It utilizes a global buddy allocator \cite{buddy_allocator} alongside Per-CPU Pagesets (PCP). Allocations are first attempted from the local PCP and if that fails, the system falls back to the global allocator with full synchronization.

\subsubsection{Virtual Memory}

Each process possesses its own Page Table, responsible for mapping virtual addresses to physical ones. Physical frames are acquired from the Physical Memory Manager \ref{subsec:linux_mem}. Virtual address space management for each process utilizes red-black trees to efficiently locate free areas. The kernel address space is mapped into every process. Additionally, Linux implements demand paging, a lazy allocation approach where physical pages are assigned only when the process actually accesses the memory, utilizing the page-fault mechanism.

\subsection{System Interface}

Linux is a Unix-like operating system kernel. The primary interface for system communication is provided via system calls, wrapped by the GNU C Library (glibc).

\subsubsection{Standards}

Linux adheres to the \textbf{POSIX} standard, implementing functions such as \texttt{fork()} and \texttt{exec()}, as well as the concept of \texttt{pthread}.

\subsubsection{Fork Mechanism}

The \texttt{fork()} mechanism is rooted in historical design decisions. While copying the entire parent process state appears inefficient and can introduce performance issues, Linux implements various optimizations, such as Copy-On-Write (\texttt{COW}), to mitigate overhead. This is particularly relevant given that \texttt{exec()} is frequently called immediately after \texttt{fork()}, discarding the copied state. The advantages and disadvantages of this mechanism include:

\noindent Advantages:
\begin{itemize}
    \item Simplified implementation of pipes and pipelining.
    \item Automatic state propagation, eliminating the need to manually specify files or streams in function call.
\end{itemize}

\noindent Disadvantages:
\begin{itemize}
    \item Complex implementation requiring multiple optimizations to remain efficient.
    \item Requires the duplication of process structures, which is inherently complex.
    \item Potential performance bottlenecks in edge cases, such as with large page tables.
    \item Requires careful resource management by the programmer to release resources before execution.
    \item Conceptually difficult for beginners to grasp.
    \item Not portable to architectures lacking an MMU.
\end{itemize}

\section{Minix 3}

Minix 3 is a microkernel-based operating system designed with a strong emphasis on high reliability. Unlike monolithic kernel architectures, Minix 3 relocates the majority of operating system components --- including device drivers, file systems, memory management mechanisms, and scheduling services --- into userspace processes. As a result, the kernel's responsibilities are strictly minimized to functions such as message passing, interrupt handling, and low-level scheduling. Currently, the system supports two hardware architectures: x86 and ARM.

This architectural approach provides several notable advantages. By isolating critical system services in user space, Minix 3 significantly improves fault tolerance, as failures in individual components do not lead to the compromise of the entire system. Moreover, these components can be restarted independently, enabling efficient recovery from faults instead of system-wide crashes. The resulting modularity also simplifies system maintenance and evolution, since services may be updated, replaced, or extended without requiring recompilation of the kernel.

\begin{figure}[htbp]
  \centering
  \resizebox{1\textwidth}{!}{%
    \begin{tikzpicture}
    \tikzstyle{every node}=[font=\large]

    % - Background Layers -
    % Kernel Mode Background
    \draw [fill={rgb,255:red,242; green,242; blue,242}] (2.5,10.75) rectangle (17.25,8.25);
    % User Mode Background
    \draw [fill={rgb,255:red,242; green,242; blue,242}] (2.5,18.5) rectangle (17.25,11);
    
    % Horizontal Separators
    \draw (2.5,16) -- (17.25,16);
    \draw (2.5,13.5) -- (17.25,13.5);

    % - User Processes Layer (Layer 4) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,18) rectangle node {\large User Processes} (7,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,18) rectangle node {\large Shell} (9.5,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,18) rectangle node {\large Make} (12,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,18) rectangle node {\large User} (14.5,16.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,18) rectangle node {\LARGE ...} (17,16.5);

    % - Server Processes Layer (Layer 3) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,15.5) rectangle node {\large Server Processes} (7,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,15.5) rectangle node {\large File} (9.5,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,15.5) rectangle node {\large PM} (12,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,15.5) rectangle node {\large Sched} (14.5,14);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,15.5) rectangle node {\LARGE ...} (17,14);

    % - Device Driver Layer (Layer 2) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,13) rectangle node {\large Device Processes} (7,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.25,13) rectangle node {\large Disk} (9.5,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (9.75,13) rectangle node {\large TTY} (12,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.25,13) rectangle node {\large Net} (14.5,11.5);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (14.75,13) rectangle node {\LARGE ...} (17,11.5);

    % - Kernel Layer (Layer 1) -
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (2.75,10.25) rectangle node {\large Kernel} (7.25,8.75);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (7.75,10.25) rectangle node {\large Clock Task} (12,8.75);
    \draw [fill={rgb,255:red,211; green,211; blue,211}, rounded corners=6.0] (12.5,10.25) rectangle node {\large System Task} (17,8.75);
    
    % - Labels and Brackets -
    % User Mode bracket
    \draw [thick] (2.0,11) -- (2.0,18.5);
    \draw [thick] (2.0,11) -- (2.2,11);
    \draw [thick] (2.0,18.5) -- (2.2,18.5);
    \node[left, align=center] at (1.8,14.75) {\large \textbf{User}\\\large \textbf{Mode}};
    
    % Kernel Mode bracket
    \draw [thick] (2.0,8.25) -- (2.0,10.75);
    \draw [thick] (2.0,8.25) -- (2.2,8.25);
    \draw [thick] (2.0,10.75) -- (2.2,10.75);
    \node[left, align=center] at (1.8,9.5) {\large \textbf{Kernel}\\\large \textbf{Mode}};
    
    % - Right Side Layer Labels -
    \node[right] at (17.5, 17.25) {\large \textbf{Layer 4}};
    \node[right] at (17.5, 14.75) {\large \textbf{Layer 3}};
    \node[right] at (17.5, 12.25) {\large \textbf{Layer 2}};
    \node[right] at (17.5, 9.5)   {\large \textbf{Layer 1}};
    
    \end{tikzpicture}
  }%
\caption{The Minix 3 Microkernel Architecture (adapted from \cite{minix_wikipedia})}
\label{fig:minix_architecture}
\end{figure}

\subsection{Scheduler}
Adhering to the microkernel philosophy, Minix 3 separates scheduling policy from the low-level context switching mechanism. While the kernel remains responsible for the mechanics of context switching and interrupt handling, scheduling decisions are delegated to a dedicated userspace server, referred to as the \texttt{Sched} server.

The scheduler employs a multi-level priority round-robin algorithm \cite{minix_sched}. The system maintains sixteen priority queues organized hierarchically. Kernel tasks occupy the highest priority levels, followed by device drivers, system servers, and user applications. The lowest priority queue is reserved exclusively for idle tasks. At any scheduling decision point, the scheduler selects the next runnable process from the highest non-empty priority queue.

Every process is assigned a specific time quantum, representing the maximum CPU time allowed before preemption occurs. Upon the exhaustion of a process's time quantum, the kernel notifies the scheduler server. To penalize CPU-bound tasks and maintain system responsiveness, the scheduler lowers the priority of such processes, moving them to a lower queue.

To prevent indefinite starvation of low-priority processes, a \texttt{balance\_queues} function executes periodically at five-second intervals \cite{minix_sched_bq}. This function re-evaluates process states and promotes tasks that have not received CPU time for an extended period, thereby ensuring that interactive applications remain responsive.

\subsection{Timing}
The timing subsystem in Minix 3 is responsible for maintaining system time, coordinating scheduling intervals, and handling alarms and timers. The core timing functionality is encapsulated within the Clock Task, a kernel-level process that manages time-related operations.

\subsubsection{Hardware Abstraction}
Minix 3 provides an abstraction layer over platform-specific timing hardware. On x86 systems, this includes the legacy Programmable Interval Timer (PIT) \cite{osdev-pit}, while ARM-based platforms rely on board-specific timer implementations \cite{minix_arch_clock}. During system initialization, the kernel configures the selected hardware timer to generate periodic interrupts. If not specified in the kernel environment, the default interrupt frequency is architecture-dependent: 60 Hz on x86 and 1000 Hz on ARM systems.

\subsubsection{Timers and Alarms}
The kernel maintains a \texttt{clock\_timers} queue to manage synchronous timers and alarms \cite{minix_kernel_clock}. Upon receipt of a hardware timer interrupt, the \texttt{timer\_int\_handler} routine is executed. If the expiration time of the next scheduled timer in the \texttt{clock\_timers} queue has been reached, the kernel executes the associated callback function. 

In the case of system alarms, this callback is the \texttt{cause\_alarm} function \cite{minix_do_setalarm}. This function sends a notification message from the Clock Task to the requesting process (e.g., the Process Manager or Scheduler), informing it that the requested time interval has elapsed. This mechanism allows userspace servers to perform periodic tasks, such as the scheduler's queue balancing.

\subsubsection{Real-Time Clock (RTC)}
Persistent wall-clock time is provided by a dedicated userspace driver, \texttt{readclock}. This driver interfaces directly with the CMOS Real-Time Clock (RTC) to retrieve the current date and time. During system initialization, this information is communicated to the Process Manager to establish the system's initial time reference \cite{minix_readclock}.

\subsection{Memory Management}
Memory management in Minix 3 is primarily handled by the userspace Virtual Memory (VM) server. The kernel retains control over the hardware Memory Management Unit (MMU) and address space switching but delegates higher-level memory policies to the VM server.

\subsubsection{VM Server}
The VM server is responsible for memory region allocation, page table administration, and page fault handling. It implements a region-based memory model in which a process address space is divided into contiguous regions with defined access permissions (read, write, and execute). To efficiently manage free and allocated memory regions, the VM server uses \textbf{AVL trees} \cite{minix_vm}.

\subsubsection{Allocation and Paging}
Internal memory allocation for kernel data structures and VM metadata is performed using a slab allocator \cite{slab_allocation}. Minix 3 supports demand paging: when a process attempts to access an unmapped virtual address, a page fault is raised. The kernel intercepts this exception and forwards the fault information to the VM server, which resolves it by mapping an appropriate physical frame or retrieving the required page from secondary storage \cite{shenoy_lecture}.

\subsubsection{Memory Grants}
Due to the strict isolation of address spaces in the microkernel architecture, processes cannot directly access the memory of others. To facilitate the exchange of large data structures without violating protection boundaries, Minix 3 provides a capability-based mechanism known as \texttt{Memory Grants}, accessed via the \texttt{safecopy} API \cite{minix_safecopy}.

A process (the grantor) dynamically generates a grant capability that explicitly permits a specific peer process (the grantee) to read from or write to a designated memory range. The grantee utilizes this grant ID to request a data transfer from the System Task. The kernel validates the grant permissions before performing the copy operation between the disjoint address spaces. This mechanism ensures that drivers and servers can operate on client buffers while preventing unauthorized access to arbitrary memory locations.

\subsection{System Interface}
Although Minix 3 conforms to the POSIX standard, its underlying system interface differs substantially from monolithic operating systems due to its message-based microkernel architecture.

\subsubsection{IPC Primitives}
Inter-process communication (IPC) is built upon four fundamental primitives provided by the kernel. For each operation, the kernel mediates communication by copying message contents directly from the sender's address space to that of the receiver.
\begin{itemize}
    \item \texttt{send(dest, \&message)}: Sends a message to the specified destination. The sender blocks if the destination is not ready to receive.
    \item \texttt{receive(source, \&message)}: Blocks the calling process until a message is received from the specified source.
    \item \texttt{sendrec(src\_dest, \&message)}: A combined operation that sends a message and then blocks until a reply is received from the same process.
    \item \texttt{notify(dest)}: A non-blocking signaling mechanism used to inform a destination of an event without transmitting a data payload.
\end{itemize}

\subsubsection{Communication Policies}
IPC in Minix 3 is governed by these strict policies:
\begin{enumerate}
    \item \textbf{Access Control:} Processes may exchange messages only with explicitly authorized peers.
    \item \textbf{Hierarchical Communication:} Message flow generally follows the system's layered architecture (Layer 4 $\rightarrow$ Layer 3 $\rightarrow$ Layer 2).
    \item \textbf{Userspace Isolation:} User processes are prohibited from communicating directly with one another or with the kernel.
\end{enumerate}

\subsubsection{System Call Implementation}
In Minix 3, system calls are not implemented as direct kernel invocations. Instead, standard library functions (e.g., \texttt{read}, \texttt{fork}) serve as wrappers that construct a message containing the call arguments and transmit it to the appropriate server using the \texttt{sendrec} primitive \cite{minix_ipc}. If the server needs to perform a privileged operation, it forwards the request to the System Task on behalf of the calling process, as servers possess higher privileges than user processes.

\subsubsection{Server Delegation}
System calls are dispatched to specialized servers according to their functionality \cite{minix_callnr}:
\begin{itemize}
    \item \textbf{Process Manager (PM):} Responsible for process lifecycle operations such as \texttt{fork}, \texttt{exec}, \texttt{exit}, and \texttt{wait}.
    \item \textbf{Virtual File System (VFS):} Handles file-related operations including \texttt{open}, \texttt{read}, \texttt{write}, and \texttt{stat}.
\end{itemize}
The kernel acts solely as a transport mechanism and remains agnostic to the semantics of the system call. After processing a request, the server returns a reply message to the calling process, thereby unblocking it and delivering the result.

\chapter{Our Implementation} 
TODO: JAKUB

\section{Bootloader}

Before the architecture-agnostic kernel can commence execution, the underlying hardware must be brought to a known, deterministic state. As discussed in Section \ref{subsec:theory_bootloader}, the initialization protocols differ significantly depending on the specific hardware architecture and the firmware interface.

For the AlkOS kernel on the x86-64 architecture, we rely on the \textbf{Multiboot2} specification \cite{Multiboot2-Spec}. This allows us to utilize a battle-tested external bootloader, such as GRUB, to handle the intricacies of storage drivers, filesystem parsing, and memory map retrieval. GRUB loads our kernel binary into memory and transfers control to our entry point.

However, the state provided by Multiboot2 is insufficient for the immediate execution of our C++ kernel. Upon entry, the system is in \textbf{32-bit Protected Mode}, paging is disabled, interrupts are disabled, and the stack pointer is undefined. Furthermore, the kernel is loaded at a low physical address, whereas AlkOS is designed as a \textbf{higher-half kernel}: its code and data are linked to virtual addresses in the upper portion of the 64-bit address space (\texttt{0xFFFFFFFE00000000}).

To bridge this gap, we implemented a two-stage internal bootloader: \textbf{Loader32} and \textbf{Loader64}. Each stage exists as a separate binary with distinct compilation requirements, as illustrated in Figure \ref{fig:boot_flow}.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=1.8cm,
        stage/.style={draw, fill=gray!15, rounded corners=4pt, minimum width=3.2cm, minimum height=1.2cm, font=\small\bfseries, align=center},
        arrow/.style={-Latex, thick},
        note/.style={font=\scriptsize, align=center, text width=3cm}
    ]
    
    % Stages
    \node[stage] (grub) {GRUB\\Bootloader};
    \node[stage, right=of grub] (loader32) {Loader32\\(32-bit ELF)};
    \node[stage, right=of loader32] (loader64) {Loader64\\(64-bit PIE)};
    \node[stage, right=of loader64] (kernel) {AlkOS Kernel\\(Higher-Half)};
    
    % Arrows with labels
    \draw[arrow] (grub) -- (loader32) node[midway, above, font=\scriptsize] {Protected Mode};
    \draw[arrow] (loader32) -- (loader64) node[midway, above, font=\scriptsize] {Long Mode};
    \draw[arrow] (loader64) -- (kernel) node[midway, above, font=\scriptsize] {Virtual Memory};
    
    % Notes below
    \node[note, below=0.6cm of grub] {Multiboot2\\entry point};
    \node[note, below=0.6cm of loader32] {32-bit instructions\\Enable Long Mode};
    \node[note, below=0.6cm of loader64] {Position-independent\\Load kernel to higher-half};
    \node[note, below=0.6cm of kernel] {Linked at\\0xFFFFFFFE00000000};
    
    \end{tikzpicture}
    }
    \caption{AlkOS Boot Flow: Three-Stage Transition from GRUB to Higher-Half Kernel}
    \label{fig:boot_flow}
\end{figure}

\subsection{Why Two Internal Loaders?}

The separation into Loader32 and Loader64 is not arbitrary, it stems from fundamental constraints of the x86-64 architecture and our kernel design:

\begin{enumerate}
    \item \textbf{Loader32 contains 32-bit instructions}: The transition from Protected Mode to Long Mode requires executing 32-bit code. Instructions such as setting control register bits, loading the GDT, and performing the mode switch \textit{cannot} be encoded in 64-bit instruction format. Thus, Loader32 is compiled as a 32-bit ELF binary (\texttt{elf32-i386}).
    
    \item \textbf{Loader64 is Position-Independent Code (PIC)}: The kernel is linked at a fixed higher-half virtual address (\texttt{0xFFFFFFFE00000000}). However, Loader64 must execute \textit{before} virtual memory is fully configured. GRUB loads Loader64 elf as a Multiboot2 module. This elf must still be loaded in the first place in memory that is big enought to hold it, meaning at an \textit{unknown physical address}. To function correctly regardless of where it is loaded, Loader64 is compiled as a Position-Independent Executable (PIE) using \texttt{-fPIE -pie} flags, employing RIP-relative addressing for all memory accesses.
    
    \item \textbf{The kernel ignores its physical location}: Once virtual memory is established with the higher-half mapping, the kernel code executes using virtual addresses. It is unaware of, and unaffected by, its underlying physical memory location.
\end{enumerate}

\subsection{Loader32: 32-bit to 64-bit Transition}

Loader32 is the true entry point of AlkOS, receiving control directly from GRUB. Its primary responsibility is to transition the CPU from 32-bit Protected Mode to 64-bit Long Mode, enabling the 4-level paging required by the x86-64 architecture \cite{Intel-ControlRegisters}.

\subsubsection{Entry Point}

The Multiboot2 specification defines the machine state upon entry: \texttt{EAX} contains the magic number \texttt{0x36d76289}, and \texttt{EBX} points to the Multiboot information structure. The entry point establishes a stack and delegates to C++ code:

\begin{lstlisting}[style=nasmstyle, caption={Loader32 Entry Point (entry.nasm)}, label={lst:loader32_entry}]
bits 32
; GRUB leaves us in 32-bit Protected Mode
; EAX = Magic number, EBX = Multiboot info pointer

section .text
global Entry
Entry:
    mov esp, stack_top      ; Establish stack
    mov ebp, esp
    and esp, 0xFFFFFFF0     ; Align to 16 bytes (System V ABI)
    
    push ebx                ; Arg2: Multiboot info pointer
    push eax                ; Arg1: Magic number
    call MainLoader32       ; Delegate to C++ code
\end{lstlisting}

\subsubsection{CPU Feature Detection}

Before attempting the Long Mode transition, Loader32 verifies that the CPU supports the required features. This is accomplished through the CPUID instruction \cite{Intel-CPUID}:

\begin{lstlisting}[style=nasmstyle, caption={Long Mode Detection (checks.nasm)}, label={lst:check_longmode}]
EXTENDED_FUNCTIONS_THRESHOLD equ 0x80000000
LONG_MODE_BIT equ 1 << 29

CheckLongMode:
    ; Check if extended CPUID functions are available
    mov eax, EXTENDED_FUNCTIONS_THRESHOLD
    cpuid
    cmp eax, EXTENDED_FUNCTIONS_THRESHOLD + 1
    jl .no_long_mode
    
    ; Query extended processor features (EAX=80000001h)
    mov eax, EXTENDED_FUNCTIONS_THRESHOLD + 1
    cpuid
    test edx, LONG_MODE_BIT   ; Check EDX bit 29
    jz .no_long_mode
    
    mov eax, 0                ; Success
    ret
.no_long_mode:
    mov eax, 1                ; Failure
    ret
\end{lstlisting}

\subsubsection{Memory Management Initialization}

Before enabling paging, Loader32 initializes a temporary bootstrap Physical Memory Manager (PMM) and Virtual Memory Manager (VMM). The memory map obtained from the Multiboot2 information structure is parsed to identify available physical memory regions. Loader32 then creates an \textbf{identity mapping} of the first 10 gigabytes of physical memory using 1GB pages:

\begin{lstlisting}[caption={Identity Mapping Setup (main.cpp)}, label={lst:loader32_mmap}]
MemoryManagers SetupMemoryManagement(MultibootInfo &multiboot_info) {
    auto mmap_tag = multiboot_info.FindTag<TagMmap>();
    const MemoryMap mmap(*mmap_tag);
    
    // Create Physical Memory Manager from Multiboot memory map
    auto pmm = PhysicalMemoryManager::Create(mmap, lowest_safe_addr, ...);
    auto vmm = new VirtualMemoryManager(*pmm);
    
    // Identity map first 10GB using 1GB page granularity
    vmm.Map<PageSizeTag::k1Gb>(
        0,                                   // Virtual address
        0,                                   // Physical address  
        10 * PageSize<PageSizeTag::k1Gb>(),  // 10GB
        kPresentBit | kWriteBit | kGlobalBit
    );
    
    return {*pmm, *vmm};
}
\end{lstlisting}

The identity mapping ensures that code continues to execute correctly after paging is enabled, the instruction pointer remains valid because virtual addresses 0--10GB map directly to the same physical addresses.

\subsubsection{Long Mode Enablement}

The transition to Long Mode follows the sequence defined in the Intel Software Developer's Manual \cite{Intel-ControlRegisters}:

\begin{lstlisting}[style=nasmstyle, caption={Enabling Long Mode and Paging (enables.nasm)}, label={lst:enable_longmode}]
LONG_MODE_BIT equ 1 << 8
EFER_MSR      equ 0xC0000080
PAGING_BIT    equ 1 << 31
PAE_BIT       equ 1 << 5

EnableLongMode:
    ; Set LME (Long Mode Enable) in EFER MSR
    mov ecx, EFER_MSR
    rdmsr
    or eax, LONG_MODE_BIT
    wrmsr
    ret

EnablePaging:
    ; Enable PAE (Physical Address Extension) in CR4
    mov eax, cr4
    or eax, PAE_BIT
    mov cr4, eax
    
    ; Load PML4 table address into CR3
    mov eax, [ebp + 8]    ; pml4_table parameter
    mov cr3, eax
    
    ; Enable paging in CR0
    mov eax, cr0
    or eax, PAGING_BIT
    mov cr0, eax
    ret
\end{lstlisting}

\subsubsection{Loading Loader64 as an ELF Module}

A critical responsibility of Loader32 is loading the next stage. Loader64 is provided to GRUB as a separate Multiboot2 module. Loader32 locates this module, parses it as an ELF64 executable, allocates memory for its segments, and performs dynamic relocation:

\begin{lstlisting}[caption={Loading Loader64 Module (main.cpp)}, label={lst:load_loader64}]
static u64 LoadNextStageModule(MultibootInfo &multiboot_info, 
                               MemoryManagers mem_managers) {
    auto &pmm = mem_managers.pmm;
    
    // Find the loader64 module by command line string
    auto next_module = multiboot_info.FindTag<TagModule>([](TagModule *tag) {
        return strcmp(tag->cmdline, "loader64") == 0;
    });
    
    // Parse ELF and determine required memory
    auto elf = PhysicalPtr<byte>(next_module->mod_start);
    u64 virt_start, virt_end;
    Elf64::GetProgramBounds(elf.ValuePtr(), virt_start, virt_end);
    
    // Allocate contiguous physical memory for the module
    u64 module_size = virt_end - virt_start;
    auto module_dest = pmm.AllocContiguous32(module_size);
    
    // Load and relocate the PIE executable
    auto entry_point = Elf64::Load(elf.ValuePtr(), module_dest);
    Elf64::Relocate(elf.ValuePtr(), module_dest);
    
    return entry_point;
}
\end{lstlisting}

\subsubsection{64-bit GDT and Mode Transition}

Before jumping to 64-bit code, a 64-bit Global Descriptor Table must be loaded. The GDT defines the code and data segments for Long Mode:

\begin{lstlisting}[style=nasmstyle, caption={64-bit GDT Definition (gdt.nasm)}, label={lst:gdt64}]
section .data
align 16
GDT64:
    .Null: dq 0
    .Code: equ $ - GDT64
        dw 0x0000         ; Limit (unused in 64-bit)
        dw 0x0000         ; Base (unused in 64-bit)
        db 0x00
        db 0x9A           ; Access: Present, Code, Readable
        db 0x20           ; Flags: Long mode
        db 0x00
    .Data: equ $ - GDT64
        dw 0x0000
        dw 0x0000
        db 0x00
        db 0x92           ; Access: Present, Data, Writable
        db 0x00
        db 0x00
.End:
    .Pointer:
        dw .End - GDT64 - 1
        dq GDT64
\end{lstlisting}

The final transition performs a far jump to flush the CPU pipeline and begin executing 64-bit code:

\begin{lstlisting}[style=nasmstyle, caption={Transition to 64-bit Mode (transition\_64.nasm)}, label={lst:transition64}]
    ; Save kernel entry address and loader data pointer
    mov eax, [ebp+8]            ; entry_high
    mov [k_ptr+4], eax
    mov eax, [ebp+12]           ; entry_low  
    mov [k_ptr], eax
    mov eax, [ebp+16]           ; loader_data
    mov [loader_data_ptr], eax
    
    ; Load 64-bit GDT
    lgdt [GDT64.Pointer]
    
    ; Set data segments
    mov ax, GDT64.Data
    mov ss, ax
    mov ds, ax
    mov es, ax
    
    ; Far jump to 64-bit code segment
    jmp GDT64.Code:jmp_elf

[bits 64]
jmp_elf:
    ; Now executing in 64-bit Long Mode
    mov rdi, [loader_data_ptr]  ; Pass transition data
    mov rax, [k_ptr]
    jmp rax                     ; Jump to Loader64
\end{lstlisting}

\subsection{Loader64: Position-Independent Higher-Half Setup}

Loader64 executes in 64-bit Long Mode but faces a unique challenge: it is loaded at an unknown physical address by GRUB, yet must set up virtual memory and load the kernel at a fixed higher-half address.

\subsubsection{Position-Independent Compilation}

Loader64 is compiled as a Position-Independent Executable (PIE). The CMake configuration specifies:

\begin{lstlisting}[language=cmake, caption={Loader64 CMake Configuration}, label={lst:loader64_cmake}]
set_target_properties(alkos.boot.loader.64 PROPERTIES 
    POSITION_INDEPENDENT_CODE ON
)
target_link_options(alkos.boot.loader.64 PRIVATE
    -fPIE -pie
    "LINKER:-z,norelro"
)
\end{lstlisting}

This ensures all code uses RIP-relative addressing. The entry point demonstrates this:

\begin{lstlisting}[style=nasmstyle, caption={Loader64 Entry Point with RIP-Relative Addressing (entry.nasm)}, label={lst:loader64_entry}]
section .text
bits 64
Entry:
    ; RIP-relative addressing for position independence
    lea rsp, [rel stack_top]    ; Load stack using RIP-relative
    mov rbp, rsp
    
    mov r10d, edi               ; Save transition data pointer
    
    ; Load GDT using RIP-relative address
    lea rax, [rel GDT64.Pointer]
    lgdt [rax]
    
    ; Set up data segments
    mov ax, [rel GDT64_DATA_SELECTOR]
    mov ss, ax
    mov ds, ax
    mov es, ax
    
    mov edi, r10d
    call MainLoader64
\end{lstlisting}

\subsubsection{State Restoration from Loader32}

Loader32 passes its PMM and VMM state to Loader64 via a \texttt{TransitionData} structure. Loader64 deserializes this state to continue memory management:

\begin{lstlisting}[caption={TransitionData Deserialization (main.cpp)}, label={lst:transition_data}]
static auto InitializeLoaderEnvironment(const TransitionData *transition_data) {
    // Reconstruct PMM from serialized state
    PhysicalMemoryManager *pmm = 
        new PhysicalMemoryManager(transition_data->pmm_state);
    
    // Reconstruct VMM with existing page tables
    VirtualMemoryManager *vmm = 
        new VirtualMemoryManager(*pmm, transition_data->vmm_state);
    
    // Reconstruct Multiboot info accessor
    MultibootInfo mb_info(transition_data->multiboot_info_addr);
    
    return {MemoryManagers{*pmm, *vmm}, mb_info};
}
\end{lstlisting}

\subsubsection{Kernel Loading at Higher-Half Address}

The kernel ELF is linked to execute at virtual address \texttt{0xFFFFFFFE00000000}. Loader64 allocates virtual memory at this address and loads the kernel segments:

\begin{lstlisting}[caption={Higher-Half Kernel Loading (main.cpp)}, label={lst:kernel_load}]
// Kernel virtual address (upper 33 bits set)
static constexpr u64 kKernelVirtualAddressStart = 0xFFFFFFFE00000000ULL;

static u64 LoadKernelIntoMemory(MultibootInfo &multiboot_info,
                                const KernelModuleInfo &kernel_info,
                                MemoryManagers mem_managers) {
    auto &vmm = mem_managers.vmm;
    
    // Allocate virtual memory at the higher-half address
    vmm.Alloc(kKernelVirtualAddressStart, 
              kernel_info.size,
              kPresentBit | kWriteBit | kGlobalBit);
    
    // Load ELF segments into the allocated virtual memory
    byte *module_start = reinterpret_cast<byte *>(kernel_info.tag->mod_start);
    auto entry_point = Elf64::Load(module_start);
    
    return entry_point;
}
\end{lstlisting}

\subsubsection{Direct Memory Mapping}

To allow the kernel to access physical memory directly, Loader64 creates a \textbf{direct memory mapping}, a large region of virtual address space that maps linearly to physical memory:

\begin{lstlisting}[caption={Direct Memory Mapping Setup (main.cpp)}, label={lst:direct_map}]
static constexpr u64 kDirectMemMapAddrStart = 0xFFFF800000000000ULL;
static constexpr u64 kDirectMemMapSizeGb = 512;

static void EstablishDirectMemMapping(MemoryManagers &mms) {
    auto &vmm = mms.vmm;
    
    // Map 512GB of physical memory to higher-half virtual addresses
    vmm.Map<PageSizeTag::k1Gb>(
        kDirectMemMapAddrStart,                        // Virtual base
        0,                                             // Physical base
        kDirectMemMapSizeGb * PageSize<PageSizeTag::k1Gb>(),
        kPresentBit | kWriteBit | kGlobalBit
    );
}
\end{lstlisting}

This mapping allows the kernel to convert any physical address \texttt{P} to the virtual address \texttt{0xFFFF800000000000 + P}.

\subsubsection{Kernel Arguments Preparation}

Before transferring control to the kernel, Loader64 collects essential system information into a \texttt{KernelArguments} structure:

\begin{lstlisting}[caption={Kernel Arguments Preparation (main.cpp)}, label={lst:kernel_args}]
struct PACK alignas(64) KernelArguments {
    /// Kernel Mem Layout
    u64 kernel_start_addr;
    u64 kernel_end_addr;

    /// VMem
    u64 pml_4_table_phys_addr;

    /// Memory Bitmap
    u64 mem_info_bitmap_addr;
    u64 mem_info_total_pages;

    /// Framebuffer
    u64 fb_addr;
    u32 fb_width;
    u32 fb_height;
    u32 fb_pitch;
    u32 fb_bpp;  // Bits per pixel

    // RGB Format
    u8 fb_red_pos;
    u8 fb_red_mask;
    u8 fb_green_pos;
    u8 fb_green_mask;
    u8 fb_blue_pos;
    u8 fb_blue_mask;

    /// Ramdisk
    u64 ramdisk_start;
    u64 ramdisk_end;

    /// ACPI
    u64 acpi_rsdp_phys_addr;

    /// Multiboot
    u64 multiboot_info_addr;
    u64 multiboot_header_start_addr;
    u64 multiboot_header_end_addr;
};
}
\end{lstlisting}

The final transition to the kernel is a simple jump:

\begin{lstlisting}[style=nasmstyle, caption={Kernel Entry (transition.nasm)}, label={lst:enter_kernel}]
; void EnterKernel(u64 kernel_entry, KernelArguments *args)
;                  [rdi]            [rsi]
EnterKernel:
    xchg rdi, rsi       ; Swap: RDI = args, RSI = entry
    jmp rsi             ; Jump to kernel entry point
\end{lstlisting}

\subsection{Memory Layout}

Figure \ref{fig:memory_layout} illustrates the virtual memory layout established by the bootloader before kernel execution begins.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        memblock/.style={draw, minimum width=8cm, minimum height=0.9cm, font=\small\ttfamily},
        addrblock/.style={font=\tiny\ttfamily, anchor=east},
        scale=0.95
    ]
    
    % Virtual address space
    \node[font=\bfseries] at (2, 6.5) {Virtual Address Space};
    
    % Memory regions
    \node[memblock, fill=red!20] (kernel) at (2, 5.5) {Kernel Code \& Data};
    \node[addrblock] at (-2.3, 5.5) {0xFFFFFFFE00000000};
    
    \node[memblock, fill=blue!20] (direct) at (2, 4.3) {Direct Physical Memory Mapping (512GB)};
    \node[addrblock] at (-2.3, 4.3) {0xFFFF800000000000};
    
    \node[memblock, fill=gray!10, minimum height=2cm] (unused) at (2, 2.6) {Unused Virtual Address Space};
    
    \node[memblock, fill=green!20] (identity) at (2, 0.9) {Identity Mapped (10GB)};
    \node[addrblock] at (-2.3, 0.9) {0x0000000000000000};
    
    % Arrows to physical memory
    \node[font=\bfseries] at (9, 6.5) {Physical Memory};
    
    \node[memblock, fill=yellow!20, minimum width=4cm] (phys) at (10, 3) {Physical RAM};
    
    % Mapping arrows
    \draw[-Latex, thick, dashed] (kernel.east) -- ++(1,0) |- (phys.west);
    \draw[-Latex, thick] (direct.east) -- ++(0.5,0) |- (phys.170);
    \draw[-Latex, thick] (identity.east) -- ++(0.5,0) |- (phys.190);
    
    \end{tikzpicture}
    \caption{Virtual Memory Layout After Bootloader Initialization}
    \label{fig:memory_layout}
\end{figure}

\subsection{Architecture Initialization}

After the kernel receives control, the first architecture-specific function executed is \texttt{arch::ArchInit}. This function enables additional CPU features that were not safe to enable during the bootloader phase:

\begin{lstlisting}[caption={Architecture Initialization (init.cpp)}, label={lst:arch_init}]
namespace arch {
void ArchInit(const RawBootArguments &) {
    BlockHardwareInterrupts();
    
    ...
    
    // Enable CPU features (sequence is important)
    EnableOSXSave();    // Enable XSAVE/XRSTOR for context switching
    EnableSSE();        // Enable SSE extensions
    EnableAVX();        // Enable AVX if supported
    EnableNXE();        // Enable No-Execute bit for security
    
    ...
}
}
\end{lstlisting}


\section{Memory Management}
\label{sec:memory}
TODO: KRYCZKA

\section{Interrupts}
\label{sec:interrupts}

Interrupt handling is a critical component of any operating system. Without interrupts, the system would rely on manually polling devices or checking states, which is inefficient and wasteful of resources. Furthermore, interrupts enable the system to be significantly more responsive. For example, when a keyboard key is pressed, the operating system may need to immediately switch execution to the thread responsible for consuming the input, rather than waiting for the current task to finish. More broadly, the OS utilizes the interrupt mechanism to perform context switches. If a task exceeds its allocated execution time, timing devices such as the LAPIC Timer trigger an interrupt, allowing the scheduler to preempt the current task and grant CPU time to others.

\subsection{x86-64 Interrupts}

\subsubsection{Interrupt Handling}
On the x86-64 architecture, every interrupt is assigned a unique number (vector) which maps directly to an entry in the \textbf{Interrupt Descriptor Table} (IDT) \cite{IntelManual-Interrupts}. This table contains instructions for the CPU on how to react to specific interrupts. The entry layout is as follows:

\begin{lstlisting}[caption={IDT Entry Layout}, label={lst:idtEntry}]
  enum class IdtPrivilegeLevel : u8 { kRing0 = 0, kRing1 = 1, kRing2 = 2, kRing3 = 3 };

  struct PACK IdtEntryFlags {
      IdtGateType type : 4;
      u8 zero : 1;
      IdtPrivilegeLevel dpl : 2;
      u8 present : 1;
  };
  struct PACK IdtEntry {
      u16 isr_low;    // The lower 16 bits of the ISR's address
      u16 kernel_cs;  // The GDT segment selector that the CPU
                      //  will load into CS before calling the ISR
      u8 ist;         // The IST in the TSS that the CPU will load into RSP
      IdtEntryFlags attributes;  // Type and attributes
      u16 isr_mid;  // The higher 16 bits of the lower 32 bits of the ISR's address
      u32 isr_high; // The higher 32 bits of the ISR's address
      u32 reserved; // Set to zero
  };
\end{lstlisting}

The most critical component of the IDT entry is the address of the function to be invoked (split into \texttt{isr\_low}, \texttt{isr\_mid}, and \texttt{isr\_high}). Another important field is \texttt{kernel\_cs}, which specifies the code segment \cite{IntelManual-Segments} loaded before executing the handler. In x86-64, there are four privilege levels (Rings). We assume the kernel always operates in Ring 0 (the most privileged level). Therefore, the Ring 0 kernel code segment is always written to this field. Conversely, the \texttt{IdtPrivilegeLevel dpl} field specifies the minimal privilege level required to trigger the interrupt via software, which is essential for implementing system calls (syscalls) invoked from userspace.

\subsubsection{Interrupt Service Routines}

Functions handling interrupts differ significantly from standard functions generated by the compiler. When the CPU calls an interrupt handler, it first switches the stack pointer to the kernel stack if a privilege level change occurs (e.g., Ring 3 to Ring 0). This transition is managed via the Task State Segment (TSS) mechanism (for details, refer to \cite{osdev-tss}). The CPU then changes the code segment as specified in the \textbf{IDT entry} (Listing \ref{lst:idtEntry}). Subsequently, it pushes the state of the interrupted procedure onto the stack.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=0cm,
      start chain=going below,
      stacknode/.style={
          draw, 
          minimum width=5cm, 
          minimum height=1cm, 
          outer sep=0pt, 
          font=\ttfamily
      },
      labelnode/.style={
          minimum height=1cm,
          font=\footnotesize\sffamily,
          anchor=east
      }
  ]
  
  \node (highmem) at (0, 1) {Higher address (High Memory)};
  \draw[->] (highmem) -- (0, 0.2);

  \node [stacknode, on chain, fill=gray!10] (ss) {Stack Segment (SS)};
  \node [stacknode, on chain, fill=gray!10] (rsp) {Stack Pointer (RSP)};
  \node [stacknode, on chain, fill=gray!10] (rflags) {RFLAGS};
  \node [stacknode, on chain, fill=gray!10] (cs) {Code Segment (CS)};
  \node [stacknode, on chain, fill=gray!10] (rip) {Instruction Pointer (RIP)};
  \node [stacknode, on chain, fill=gray!10] (error) {Error Code (optional)};
  \draw[<-, thick, red] (error.east) -- ++(1.5,0) node[right, text=red] {Current RSP};

  \node [below=0.5cm of error] (lowmem) {Lower address (Low Memory)};
  \draw[->] (lowmem.north) -- (error.south);

  \end{tikzpicture}
  \caption{Interrupt Service Routine stack layout on entry}
  \label{fig:stackframe}
\end{figure}

The layout illustrated in Figure \ref{fig:stackframe} is known as the \textbf{Interrupt Frame}. This structure is fundamental to the kernel architecture, serving as the basis for context switching, context conversion, and jumping to userspace (Ring 3). A key distinction is that an \textbf{ISR} must return using the special instruction \textbf{IRETQ} \cite{IntelManual-Interrupts}, which reverses the actions described above, including restoring the privilege level and code segment. 

A significant challenge with standard compiler-generated functions is that they may modify the stack frame (prologue/epilogue) in ways that interfere with the hardware-defined layout. To maintain full control and prevent stack corruption, we implement assembly wrappers. These wrappers perform the necessary architecture-specific actions before invoking the architecture-agnostic interrupt handling code defined in C++.

\begin{lstlisting}[style=nasmstyle, caption={Assembly ISR wrapper}, label={lst:isr_asm}]
%macro context_switch_if_needed 0
  cmp rax, 0
  je .done                         ; Omit context switch if there is no need

  mov r13, rax                     ; save next TCB

  mov rdi, r13
  mov rsi, rsp
  call cdecl_ContextSwitchOnInterrupt

  mov rsp, [r13+Thread.kernel_stack]   ; Change the stack
.done:

  load_user_gs_if_needed

  pop_all_regs                    ; Restore registers.
  add rsp, _all_reg_size          ; Deallocate register save space.
  add rsp, 8                      ; Pop error code.
  iretq
%endmacro

; Macro for hardware or software interrupts.
; Calls a handler with the signature 'void handler(u16 lirq, void* frame)'.
%macro interrupt_wrapper 3 ; %1: Logical IRQ, %2: idt idx %3: C handler function
isr_wrapper_%+%2:
    push 0                          ; Push a dummy error code for unification.
    sub rsp, _all_reg_size          ; Allocate space for saving registers.
    push_all_regs                   ; Save registers.

    load_kernel_gs_if_needed

    cld                         ; Clear direction flag for string operations.
    mov rdi, %1                 ; Arg1: mapped lirq number.
    mov rsi, rsp                ; Arg2: pointer to stack frame.
    call %3                     ; Call the specific ISR handler.

    context_switch_if_needed
%endmacro
\end{lstlisting}

\subsubsection{Synchronization}

Since an interrupt can potentially trigger a context switch, synchronization is crucial. This applies to both kernel-space threads at any moment of their lifetime and userspace programs executing system calls. It would be catastrophic if a timer interrupt forced a context switch while the kernel was in the middle of updating scheduler structures or memory tables. Therefore, even on a single-core system, synchronization must be enforced. This is achieved by disabling hardware interrupts (using \textbf{CLI} and \textbf{STI} instructions on x86-64) during critical sections.

\subsection{Unified Interrupt Frame}
\label{subsec:unifiedFrame}

To simplify interrupt handling, we introduced a unified frame structure based on the hardware Interrupt Frame (Figure \ref{fig:stackframe}). Since the state of a thread must be preserved before a context switch, all general-purpose registers must be saved. To achieve this efficiently, these registers are pushed onto the stack by the assembly wrapper, creating the \textbf{Unified Interrupt Frame}.

The x86-64 architecture introduces an inconsistency in the stack layout depending on the interrupt source. Certain exceptions, such as Page Faults (vector 14) or General Protection Faults (vector 13), automatically push an error code onto the stack by the CPU. Hardware interrupts and other exceptions do not. To use a single, unified C++ structure for all interrupt handling (\texttt{IsrErrorStackFrame}), the assembly entry wrappers must normalize the stack. For vectors that do not produce a hardware error code, the wrapper explicitly pushes a dummy value (typically 0) before saving the general-purpose registers (as seen in Listing \ref{lst:isr_asm}). This ensures that the stack pointer is always aligned correctly and points to a uniform structure when the C++ handler is invoked.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=0cm,
      start chain=going below,
      stacknode/.style={
          draw, 
          minimum width=5cm, 
          minimum height=1cm, 
          outer sep=0pt, 
          font=\ttfamily
      },
      labelnode/.style={
          minimum height=1cm,
          font=\footnotesize\sffamily,
          anchor=east
      }
  ]
  
  \node (highmem) at (0, 1) {Higher address (High Memory)};
  \draw[->] (highmem) -- (0, 0.2);

  \node [stacknode, on chain, fill=gray!10] (ss) {Stack Segment (SS)};
  \node [stacknode, on chain, fill=gray!10] (old_rsp) {Old Stack Pointer (RSP)};
  \node [stacknode, on chain, fill=gray!10] (rflags) {RFLAGS};
  \node [stacknode, on chain, fill=gray!10] (cs) {Code Segment (CS)};
  \node [stacknode, on chain, fill=gray!10] (rip) {Instruction Pointer (RIP)};
  \node [stacknode, on chain, fill=gray!10] (error) {Error Code (or Dummy)};
  \node [stacknode, on chain, fill=gray!10] (genregs) {General Registers (RDI, RSI, ...)};
  \node [stacknode, on chain, fill=gray!10] (rax) {Last Saved Register};
  
  \draw[<-, thick, red] (rax.east) -- ++(1.5,0) node[right, text=red] {RSP (New Stack Pointer)};

  \node [below=0.5cm of rax] (lowmem) {Lower address (Low Memory)};
  \draw[->] (lowmem.north) -- (rax.south);

  \end{tikzpicture}
  \caption{Unified Interrupt Frame (IsrErrorStackFrame)}
  \label{fig:unifiedstackframe}
\end{figure}

\subsection{Context Switch}

As previously mentioned, the OS utilizes the interrupt mechanism to perform task switching. The interrupt mechanism handles most of the necessary context-switching operations automatically:
\begin{itemize}
\item Manages ring permissions (automatically swapping code and stack segments).
\item Automatically swaps the stack from user stack to kernel stack (using the TSS).
\item Restores \textbf{RFLAGS} (which includes the \textbf{interrupt flag} responsible for enabling/disabling hardware interrupts).
\item Jumps back to the code address pointed to by \textbf{RIP}.
\end{itemize}

To switch contexts, we must ensure the current thread's state is preserved in a \textbf{Unified Interrupt Frame} (Figure \ref{fig:unifiedstackframe}) on its kernel stack. Additionally, a valid frame must exist for the target thread. If a thread has run previously, it will have saved its frame naturally during its last preemption. However, for a new thread that has never executed, this frame must be constructed manually. 

We assume that all threads begin execution in kernel space before eventually jumping to user space. Since all interrupt handling occurs within the kernel, the interrupt frame always resides on the kernel stack. Therefore, we can initialize the kernel stack of the new thread with a fabricated frame before the context switch. The initialization procedure is as follows:

\begin{lstlisting}[caption={Thread stack initialization}, label={lst:threadStack}]
void InitializeThreadStack(void **stack, const Sched::Task &task)
{
    /* NOTE: Thread entry always starts in Kernel Code */
    auto stack_top = static_cast<byte *>(*stack) - sizeof(IsrErrorStackFrame);
    auto frame     = reinterpret_cast<IsrErrorStackFrame *>(stack_top);

    memset(stack_top, 0, sizeof(IsrErrorStackFrame));

    /* Initialize IsrErrorStackFrame (Hardware Part + Error Code) */
    frame->isr_stack_frame.rip    = reinterpret_cast<u64>(task.func);
    frame->error_code             = 0; // Dummy error code
    frame->isr_stack_frame.cs     = static_cast<u64>(cpu::GDT::kKernelCodeSelector);
    frame->isr_stack_frame.rflags = kInitialRFlags;
    frame->isr_stack_frame.rsp    = reinterpret_cast<u64>(*stack);
    frame->isr_stack_frame.ss     = static_cast<u64>(cpu::GDT::kKernelDataSelector);

    /* Initialize function arguments (Software Part / Registers) */
    if (task.args_count > 0) {
        frame->registers.rdi = task.args[0];
    }

    // ... other args

    if (task.args_count > 5) {
        frame->registers.r9 = task.args[5];
    }

    /* Save adjusted stack address */
    *stack = reinterpret_cast<void *>(stack_top);
}
\end{lstlisting}

With both stack frames prepared, the context switch logic is straightforward: swap the \textbf{RSP} (current stack pointer) to the kernel stack of the target thread and execute the \textbf{IRETQ} instruction. This instruction restores the state and resumes execution, as demonstrated in the \textbf{context\_switch\_if\_needed} macro (Listing \ref{lst:isr_asm}).

\subsection{Jumping to Userspace}

Transitioning to userspace is performed similarly to a thread context switch. We construct an artificial interrupt frame, but in this case, the code segment and stack segment within the frame are set to the user space selectors, and the stack pointer is set to the user space stack. The implementation is shown below:

\begin{lstlisting}[caption={Userspace Jump C++ code}, label={lst:userSpaceJump}]
extern "C" void cdecl_JumpToUserSpaceEntry(void *addr, IsrStackFrame *frame)
{
    ASSERT_NOT_NULL(addr);
    ASSERT_NOT_NULL(frame);
    ASSERT_NOT_NULL(hardware::GetCoreLocalTcb());

    auto thread          = hardware::GetCoreLocalTcb();
    thread->kernel_stack = thread->kernel_stack_bottom; // reset kernel stack

    frame->rip    = reinterpret_cast<u64>(addr);
    frame->cs     = static_cast<u64>(cpu::GDT::kUserCodeSelector);
    frame->rflags = static_cast<u64>(kInitialRFlags);
    frame->rsp    = reinterpret_cast<u64>(thread->user_stack_bottom);
    frame->ss     = static_cast<u64>(cpu::GDT::kUserDataSelector);

    const u64 t            = TimingModule::Get().GetSystemTime().ReadLifeTimeNs();
    thread->kernel_time_ns = t - thread->timestamp;
    thread->timestamp      = t;

    SetThreadGs(thread);
    __asm__ volatile("swapgs" ::: "memory");
}
\end{lstlisting}

\begin{lstlisting}[style=nasmstyle, caption={Userspace Jump NASM code}, label={lst:userSpaceJumpNasm}]
; c_decl
; void JumpToUserSpace(void (*func)(), void* arg)
;   RDI = func
;   RSI = arg
; Note: Caller is responsible for ensuring proper environment before calling (disabling IRQs)
JumpToUserSpace:
    sub rsp, _jump_userspace_stack_space
    push rsi
    ; aligned properly

    mov rsi, rsp
    add rsi, 8
    call cdecl_JumpToUserSpaceEntry

    xor rax, rax
    mov rax, _user_data_selector
    mov ds, ax
    mov es, ax
    mov fs, ax
    mov gs, ax

    pop rsi
    mov rdi, rsi ; prepare void* arg for func if needed
    iretq
\end{lstlisting}

\subsection{Interrupts Hardware Abstraction}

To maintain architectural independence, the Interrupt Table is abstracted. Upon any interrupt, control is passed to the \textbf{Logical Interrupt Table (LIT)}, which is responsible for executing the necessary actions common to all interrupts. The architecture-specific code establishes the mapping between the hardware interrupts and the logical interrupt table, from that point forward, interrupt management is handled entirely by the LIT. Responsibilities of the LIT include:

\begin{itemize}
\item Managing interrupt handlers, allowing the kernel to modify interrupt responses dynamically.
\item Tracking interrupt nesting levels.
\item Collecting statistics, such as interrupt counts per thread, kernel time, and user space time.
\item Interacting with hardware interrupt drivers such as the \textbf{PIC} or \textbf{APIC}.
\item Masking (blocking) individual interrupts.
\end{itemize}

The LIT handling procedures may return a pointer to the next thread scheduled for execution. If such a pointer is returned, the hardware wrapper performs the context switch upon exit. This mechanism enables the system to react rapidly to state changes or preempt the current thread via a timer interrupt.
\section{Timing}
\label{sec:timing}

Prior to the implementation of the Scheduler, and alongside functional memory management and interrupt handling, it was necessary to establish the infrastructure and drivers for timing mechanisms. This includes devices capable of measuring the system's uptime, referred to as \textbf{Clocks}. Additionally, the system requires devices capable of generating interrupts at specific intervals. These are essential for preempting the currently executing thread if it fails to yield the CPU voluntarily, thereby ensuring fair scheduling. Such devices are referred to as \textbf{Event Clocks}. To unify the timing subsystem, all devices and mechanisms rely on nanoseconds as the fundamental unit of time abstraction.

\subsection{Infrastructure}

The architecture-specific code is responsible for detecting available hardware and registering it within the central timing infrastructure, specifically the \textbf{ClockRegistry} and \textbf{EventClockRegistry}. Subsequently, architecture-agnostic components, such as the \textbf{ACPI} subsystem, may register additional clocks by parsing system description tables. Finally, the architecture-defined functions \texttt{hal::PickSystemClockSource()} and \texttt{hal::PickSystemEventClockSource()} are invoked during the initialization of the timing module to select the optimal sources for each purpose. The infrastructure allows the scheduler to implement either a tick-based or a tickless strategy.

\subsection{Clocks}

Clock Drivers are defined by the following structure:

\begin{lstlisting}[caption={Clock Driver Structure}, label={lst:clockDriver}]
struct alignas(arch::kCacheLineSizeBytes) ClockRegistryEntry : data_structures::RegistryEntry {
  /* Clock numbers */
  u64 frequency_kHz; // Frequency in kHz
  u64 ns_uncertainty_margin_per_sec;  
    // Uncertainty margin in femtoseconds per second
  u64 clock_numerator; // For conversion to nanoseconds, this is the numerator
  u64 clock_denominator; // For conversion to nanoseconds, this is the denominator

  /* Callbacks */
  u64 (*read)(ClockRegistryEntry *);
  bool (*enable_device)(ClockRegistryEntry *);
  bool (*disable_device)(ClockRegistryEntry *);
  void (*stop_counter)(ClockRegistryEntry *);
  void (*resume_counter)(ClockRegistryEntry *);

  /* Own data */
  void *own_data;
};
\end{lstlisting}

For the x86-64 architecture, clock selection is prioritized based on availability and precision in the following order: TSC > HPET > RTC > PIT. Currently, support is implemented only for the TSC and HPET drivers.

\subsubsection{TSC}
The Time Stamp Counter (TSC) is the optimal clock source due to its high precision and core-locality. Accessing the TSC involves reading a CPU register, which requires only a few cycles, unlike external clocks that may require hundreds. However, the TSC has historical limitations. On older processors, the counter's frequency was tied to the core frequency. Consequently, frequency scaling (throttling) caused the time measurement to drift, rendering it unreliable. Modern Intel processors introduced the Invariant TSC, which ensures a constant frequency regardless of the core's power state. Another limitation is that on certain CPUs, the TSC frequency is not explicitly known and must be measured against a known reference. For this purpose, the system utilizes the HPET, which serves as the minimal hardware requirement for reliable calibration (see \cite{IntelManual-TSC}).

\subsubsection{HPET}
The High Precision Event Timer (HPET) also offers high precision and is generally more stable than the TSC on older hardware. However, it is an external device mapped via Memory-Mapped I/O (MMIO). As a result, accessing the HPET is significantly slower than reading the TSC, potentially taking up to 1000 cycles per read. Due to this performance overhead, the HPET is designated as the secondary choice. It is primarily utilized for calibrating other clocks rather than for frequent timekeeping operations.

\subsection{Event Clocks}

Event Clock Drivers are defined by the following structure:

\begin{lstlisting}[caption={Event Clock Driver Structure}, label={lst:eventClockDriver}]
struct PACK EventClockFlags {
    bool IsCoreLocal : 1;
    u32 padding : 31;
};
static_assert(sizeof(EventClockFlags) == sizeof(u32));

enum class EventClockState : u8 {
    kDisabled = 0,  // Clock is disabled
    kPeriodic,      // Clock is in periodic mode
    kOneshot,       // Clock is in oneshot mode
    kOneshotIdle,   // Clock is in oneshot mode but no event is scheduled
    klast,
};

struct alignas(arch::kCacheLineSizeBytes) EventClockRegistryEntry : data_structures::RegistryEntry {
    /* Clock numbers */
    u64 min_next_event_time_ns;  // Minimum time for the next event in nanoseconds

    /* Clock specific data */
    EventClockFlags flags;     // Features of the event clock, e.g., core-local
    CoreMask supported_cores;  // Cores that support this event clock

    /* infra data */
    u64 next_event_time_ns;  // Time for the next event in nanoseconds
    EventClockState state;   // Current state of the clock

    /* Driver data */
    void *own_data;  // Pointer to the clock's own data, used for callback

    /* callbacks */
    struct callbacks {
        // Callback to set next event time
        u32 (*next_event)(EventClockRegistryEntry *, u64);  
        // Callback to set clock state
        u32 (*set_oneshot)(EventClockRegistryEntry *);
        // Callback to set clock state
        u32 (*set_periodic)(EventClockRegistryEntry *);     
        void (*on_entry)(EventClockRegistryEntry *);        // optional
        void (*on_exit)(EventClockRegistryEntry *);         // optional
    } cbs;
};
\end{lstlisting}

For x86-64, the selection of event clocks is based on the following priority order: LAPIC Timer > HPET > PIT. Currently, only the LAPIC Timer is supported as it provides the core-local interrupt capabilities required for efficient scheduling.

\subsubsection{LAPIC Timer}
Similar to the TSC, the LAPIC Timer is local to the processor core, ensuring extremely low-latency access to its registers. A significant advantage of this architecture is that each core possesses an independent timer, which eliminates the need for shared resource management or synchronization between cores. However, like the TSC, the LAPIC Timer operates at a frequency derived from the CPU bus or core frequency, which is not standard across various CPUs. Consequently, it requires calibration against a reference clock, for which the HPET is utilized.

\section{File System}
\label{section:fs}

The file system is a fundamental part of any operating system, providing mechanisms to store, access, and manage data on storage devices. We have designed a unified abstraction layer known as the Virtual File System (VFS). The VFS abstracts the underlying implementation details of specific file systems, providing the kernel and user space with an API for operations such as reading, writing, creating, moving, and deleting files and directories.

\subsection{VFS}

The VFS enables the operating system to interact with various file system types (e.g., FAT12, FAT16, FAT32) through a uniform interface, eliminating the need for the kernel to understand the specific implementation details. This modularity is achieved through a combination of compile-time enforced interfaces (Concepts) and a runtime dispatch mechanism.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
      node distance=1.5cm,
      block/.style={
        rectangle, draw, fill=gray!10,
        text width=5cm, text centered,
        rounded corners, minimum height=1cm
      },
      component/.style={
        rectangle, draw, fill=gray!10,
        text width=5cm, text centered,
        minimum height=1cm
      },
      interface/.style={
        rectangle, draw, dashed, fill=gray!10,
        text width=5cm, text centered,
        minimum height=1cm
      },
      storage/.style={
        cylinder, draw, fill=gray!10,
        shape aspect=0.5,
        text width=4.5cm, text centered,
        minimum height=1cm
      },
      arrow/.style={-Latex, thick}
  ]

  \node (user_process) [block] {User Process};
  \node (fd_manager) [block, below=of user_process] {File Descriptor Manager};
  \node (vfs_module) [block, below=of fd_manager] {VFS Module};
  \node (fs_interface) [interface, below=of vfs_module] {Filesystem Interface};
  \node (fs_driver) [component, below=of fs_interface] {File System Driver \\ (FAT12 / FAT16 / FAT32)};
  \node (vfs_io) [interface, below=of fs_driver] {VFS I/O Interface};
  \node (storage_device) [storage, below=of vfs_io] {Storage Device \\ (RAM, Disk, Network)};

  \draw [arrow] (user_process) -- (fd_manager) node [midway, right] {Syscalls};
  \draw [arrow] (fd_manager) -- (vfs_module) node [midway, right] {File Operations};
  \draw [arrow] (vfs_module) -- (fs_interface) node [midway, right] {Dispatch};
  \draw [arrow] (fs_interface) -- (fs_driver) node [midway, right] {Implements};
  \draw [arrow] (fs_driver) -- (vfs_io) node [midway, right] {Uses};
  \draw [arrow] (vfs_io) -- (storage_device) node [midway, right] {I/O Operations};

  \end{tikzpicture}
  \caption{High-level VFS Architecture}
  \label{fig:vfs_architecture}
\end{figure}

As illustrated in Figure \ref{fig:vfs_architecture}, the VFS architecture is composed of four primary layers:

\begin{itemize}
\item \textbf{VFS Module}: The central orchestrator for path resolution and operation delegation.
\item \textbf{Filesystem Interface}: A struct containing function pointers that abstract specific driver operations.
\item \textbf{Filesystem Driver}: The concrete implementation of a specific file system format.
\item \textbf{VFS I/O Interface}: An abstraction for block-level data access.
\end{itemize}

The following subsections describe each of these components in detail.

\subsubsection{VFS Module}

The VFS module serves as the entry point for all file operations and is responsible for managing file system mounts. It exposes the internal kernel API for operations such as opening, reading, and writing files. The module's primary responsibility is to translate these requests into calls to the appropriate filesystem instance based on the provided file path and the currently registered mount points.

When a VFS operation (e.g., \texttt{CreateFile}) is invoked, the module executes the following sequence:
\begin{enumerate}
    \item \textbf{Find Mount Point}: The system utilizes a \textbf{crit-bit tree} \cite{critbit} to efficiently locate the longest prefix match for the given path among all registered mount points. This step identifies the specific filesystem driver responsible for handling the operation.
    \item \textbf{Check Permissions}: The module validates the operation against the mount point options (e.g., ensuring write operations are not attempted on read-only mounts).
    \item \textbf{Path Translation}: The absolute system path is translated into a path relative to the mount point root.
    \item \textbf{Delegation}: The operation is delegated to the corresponding method of the specific filesystem driver.
\end{enumerate}

\begin{lstlisting}[caption={VfsModule CreateFile Implementation}, label={lst:vfsModuleCreateFile}]
Result<> internal::VfsModule::CreateFile(const Path &path)
{
    auto mount_result = FindMountPoint(path);
    RET_UNEXPECTED_IF_ERR(mount_result);

    MountPoint *mount = mount_result.value();

    RET_UNEXPECTED_IF(mount->options.read_only, VfsError::kReadOnly);

    Path relative_path = GetRelativePath_(path, mount->path);
    return mount->fs.CreateFile(relative_path);
}
\end{lstlisting}

\subsubsection{Filesystem Interface}

The \texttt{vfs::Filesystem} struct acts as a uniform interface that all concrete filesystem drivers must expose. It contains function pointers for various file and directory manipulations, alongside metadata about the filesystem. Each function pointer accepts a \texttt{void* ctx} argument, allowing the generic VFS layer to pass the concrete driver instance.

\begin{lstlisting}[caption={vfs::Filesystem Structure}, label={lst:vfsFilesystem}]
struct Filesystem {
    struct Operations {
        // File operations
        Result<> (*create_file)(void *ctx, const Path &path);
        Result<size_t> (*read_file)(
            void *ctx, const Path &path, void *buffer, size_t size, size_t offset
        );
        Result<size_t> (*write_file)(
            void *ctx, const Path &path, const void *buffer, size_t size, size_t offset
        );
        Result<> (*delete_file)(void *ctx, const Path &path);
        // ... (additional operations omitted for brevity)
    };

    struct Info {
        Type type;
        const char *name;  // e.g., "FAT32", "FAT16"
    };
    
    Filesystem() = delete;
    explicit Filesystem(void *context, const Operations &operations, const Info &info)
    : context_(context), ops_(operations), info_(info) {}

private:
    void *context_;  // Pointer to the concrete driver instance
    Operations ops_;
    Info info_;
};
\end{lstlisting}

\subsubsection{Filesystem Driver}

Each filesystem driver implements the logic required for a specific filesystem type (e.g., FAT12, FAT16, FAT32). The driver interprets the raw data structures on the storage device and performs the requested operations. For instance, the FAT32 driver handles the manipulation of the File Allocation Table, directory entries, and cluster chains according to the FAT32 specification (for details, see \cite{fat-spec}).

To implement these drivers efficiently, we employ the Curiously Recurring Template Pattern (CRTP) \cite{crtp}. Each driver class inherits from a templated base class that provides common functionality, while the derived class implements format-specific details. This approach enables static polymorphism, reducing the runtime overhead typically associated with virtual function calls.

\begin{lstlisting}[caption={FAT Driver CRTP Base Class}, label={lst:fatCrtp}]
template <template <typename> typename T, typename IO>
class Fat
{
    using ImplT  = T<IO>;
    using Traits = FatTraits<T, IO>;

protected:
    // ... common FAT structures, validation, and operations

public:
    NODISCARD Filesystem GetFilesystem()
    {
        return Filesystem(
            this, // 'this' pointer is passed as the context
            Filesystem::Operations{
                .create_file = &Fat::CreateFileCallback_,
                // ...
            },
            Filesystem::Info{
                .type = ImplT::kFsType,
                .name = ImplT::kFsName,
            }
        );
    }

private:
    FAST_CALL Result<> CreateFileCallback_(void *ctx, const Path &path)
    {
        return static_cast<Fat *>(ctx)->CreateFile(path);
    }
    // ...
};
\end{lstlisting}

Each \texttt{Fat} derivative implements its specific logic (e.g., Getting FAT entries differs between FAT12 and FAT32/16) and provides a static \texttt{IsValid(IO \&io)} method to probe whether a given I/O device contains a valid instance of that filesystem.

\begin{lstlisting}[caption={Fat12 GetFATEntry Implementation}, label={lst:fat12GetFatEntry}]
    NODISCARD FORCE_INLINE_F ClusterNumT GetFATEntry_(ClusterNumT cluster) const
    {
        ASSERT_LT(
            cluster, BaseT::cluster_count_ + BaseT::kFirstClusterNumber,
            "Cluster number out of range"
        );
        const size_t fat_offset = cluster + (cluster / 2);
        const size_t sector_number =
            BaseT::fat_region_.start + (fat_offset / boot_sector_.fat.bytes_per_sector);
        const size_t sector_offset = fat_offset % boot_sector_.fat.bytes_per_sector;

        // Load 2 sectors if entry spans two sectors
        size_t count =
            (sector_offset == static_cast<size_t>(boot_sector_.fat.bytes_per_sector - 1)) ? 2 : 1;
        auto range = BaseT::io_.ReadRange({sector_number, count});
        if ((cluster % 2) == 0) {  // Even cluster
            return internal::get<ClusterNumT>(range, sector_offset) & kClusterMask;
        } else {
            return internal::get<ClusterNumT>(range, sector_offset) >> 4;
        }
    }
\end{lstlisting}

\subsubsection{VFS Interfaces and Concepts}

To enforce architectural compliance at compile time, we utilize C++20 Concepts to define strict contracts for both filesystem drivers and low-level storage operations.

The \texttt{VFSInterface} concept (Listing \ref{lst:vfsInterfaceConcept}) mandates that any compliant driver class must provide a constructor accepting an I/O backend, a static method to validate the filesystem signature on the storage medium, and a method to retrieve the runtime function table.

\begin{lstlisting}[caption={VFSInterface Concept Definition}, label={lst:vfsInterfaceConcept}]
template <template <typename> typename T, typename IO>
concept VFSInterface = VFSIO<IO> and requires(T<IO> fs, IO io) {
    T<IO>(io);
    { T<IO>::IsValid(io) } -> std::same_as<bool>;
    { fs.GetFilesystem() } -> std::same_as<Filesystem>;
};
\end{lstlisting}

The VFS I/O interface abstracts low-level data access. The \texttt{VFSIO} concept (Listing \ref{lst:vfsioConcept}) establishes a contract for block-based input and output. By adhering to this concept, filesystem drivers remain decoupled from the underlying hardware, enabling seamless interaction with diverse storage backends - such as RAM disks, physical partitions, or network storage - without requiring implementation changes.

\begin{lstlisting}[caption={VFSIO Concept Definition}, label={lst:vfsioConcept}]
template <typename IO>
concept VFSIO =
    requires(IO io, size_t offset, io::SectorRange range, std::span<const byte> data, size_t size) {
        { io.ReadRange(range) } -> std::same_as<std::span<byte>>;
        { io.ReadSector(offset) } -> std::same_as<std::span<byte>>;
        { io.WriteRange(range, data) } -> std::same_as<void>;
        { io.WriteSector(offset, data) } -> std::same_as<void>;
        { io.GetSectorSize() } -> std::same_as<size_t>;
    };
\end{lstlisting}

\subsection{File Descriptors}

The file descriptor system provides a hierarchical structure to handle file I/O operations initiated by user processes. This design tracks open files, maintains current read/write offsets, and enforces access rights.

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{res/fd-diagram.svg}
  \caption{File Descriptor Design}
  \label{fig:fd_architecture}
\end{figure}

As depicted in Figure \ref{fig:fd_architecture}, the system comprises three main tables:

\subsubsection{FdTable (Per-Process)}
Each process maintains its own \texttt{FdTable}, which is an array mapping integer file descriptors to \texttt{RefPtr<OpenFileEntry>} objects.

\begin{lstlisting}[caption={FdTable Allocation}, label={lst:fdTableAllocate}]
FdResult<fd_t> FdTable::Allocate(data_structures::RefPtr<OpenFileEntry> global_entry)
{
    std::lock_guard lock(lock_);
    for (size_t i = 0; i < kMaxFdsPerProcess; ++i) {
        if (entries_[i] == nullptr) {
            entries_[i] = std::move(global_entry);
            ++count_;
            return static_cast<fd_t>(i);
        }
    }
    return std::unexpected(FdError::kFdTableFull);
}
\end{lstlisting}

\subsubsection{OpenFileTable (Global)}

The \texttt{OpenFileTable} is a system-wide pool of \texttt{OpenFileEntry} objects. Each entry represents a unique "open" instance of a resource (file or pipe), maintaining dynamic state such as the current read/write offset, access flags (e.g., Read/Write), and a handle to the underlying resource. Multiple file descriptors from different processes can point to the same \texttt{OpenFileEntry}, allowing them to share the cursor position.

\begin{lstlisting}[caption={OpenFileTable Creation}, label={lst:openFileTableOpenFile}]
FdResult<data_structures::RefPtr<OpenFileEntry>> OpenFileTable::OpenFile(File *file, OpenMode flags)
{
    RET_UNEXPECTED_IF(file == nullptr, FdError::kInvalidArgument);

    std::lock_guard lock(lock_);

    const size_t idx = entries_.Allocate();
    RET_UNEXPECTED_IF(idx == std::numeric_limits<size_t>::max(), FdError::kIoError);

    OpenFileEntry *entry = entries_.Get(idx);
    ASSERT_NOT_NULL(entry);

    new (entry) OpenFileEntry();
    entry->pool_idx_ = idx;

    entry->handle    = FileHandle::Wrap(file);
    entry->flags     = static_cast<u32>(flags);
    entry->offset    = 0;
    entry->is_append = HasMode(flags, OpenMode::kAppend);
    ++count_;

    return data_structures::RefPtr(entry);
}
\end{lstlisting}

The \texttt{FileHandle} utilizes a \texttt{NonOwningTaggedPtr} to hold either a pointer to a \texttt{File} object or a \texttt{Pipe} instance, enabling the \texttt{OpenFileEntry} to manage both cases seamlessly.

\subsubsection{FileTable (Global)}
The \texttt{FileTable} manages unique \texttt{File} objects system-wide. Each \texttt{File} object acts as the in-memory representation of a physical file on the filesystem, storing static attributes such as the path and file size. This ensures that a file on the VFS has a single representation in memory, regardless of how many times it has been opened.

\begin{lstlisting}[caption={FileTable GetOrCreate}, label={lst:fileTableGetOrCreate}]
FdResult<data_structures::RefPtr<File>> FileTable::GetOrCreate(const vfs::Path &path)
{
    RET_UNEXPECTED_IF(path.IsEmpty(), FdError::kInvalidArgument);

    File *existing = Find(path);
    if (existing != nullptr) {
        return data_structures::RefPtr(existing, false);
    }

    const size_t idx = files_.Allocate();
    RET_UNEXPECTED_IF(idx == std::numeric_limits<size_t>::max(), FdError::kIoError);

    File *file = files_.Get(idx);
    ASSERT_NOT_NULL(file);

    new (file) File();
    file->pool_idx_ = idx;

    file->size = 0;
    file->mode = 0;
    file->path = path;
    ++count_;

    return data_structures::RefPtr(file);
}
\end{lstlisting}

\subsubsection{FdManager}
The \texttt{FdManager} integrates these three tables and exposes file descriptor operations (e.g., \texttt{Open}, \texttt{Close}, \texttt{Read}). When a process requests to open a file, the \texttt{FdManager} coordinates the interaction: it validates existence via the VFS module, retrieves the \texttt{File} object, creates a new entry in the \texttt{OpenFileTable}, and finally assigns a file descriptor in the process's \texttt{FdTable}.

\begin{lstlisting}[caption={FdManager Open Implementation}, label={lst:fdManagerOpen}]
FdResult<fd_t> FdManager::Open(const vfs::Path &path, OpenMode flags)
{
    // Check if file exists in VFS
    auto exists_result = VfsModule::Get().FileExists(path);
    RET_UNEXPECTED_IF(!exists_result, FdError::kIoError);
    RET_UNEXPECTED_IF(!*exists_result, FdError::kNotFound));

    auto file_result = file_table_.GetOrCreate(path);
    RET_UNEXPECTED_IF_ERR(file_result);

    auto open_result = open_file_table_.OpenFile(file_result->Get(), flags);
    RET_UNEXPECTED_IF_ERR(open_result);

    FdTable *fd_table = GetCurrentProcessFdTable();
    RET_UNEXPECTED_IF(fd_table == nullptr, FdError::kIoError);

    auto fd_result = fd_table->Allocate(std::move(*open_result));
    RET_UNEXPECTED_IF_ERR(fd_result);

    return *fd_result;
}
\end{lstlisting}

\subsubsection{Resource Lifecycle Management}

To ensure efficient memory usage and prevent resource leaks, the life cycle of \texttt{OpenFileEntry} and \texttt{File} objects is managed through intrusive reference counting. Both structures inherit from a \texttt{RefCounted} base class and are managed via smart pointers (\texttt{RefPtr} and \texttt{TaggedPointer}).

When a process closes a file descriptor (or terminates), the reference to the corresponding \texttt{OpenFileEntry} in the process's \texttt{FdTable} is released. If the reference count drops to zero - indicating that no other file descriptors (across any process) refer to this open stream - the \texttt{OpenFileEntry} is automatically deallocated and removed from the global \texttt{OpenFileTable}.

Consequently, the destruction of an \texttt{OpenFileEntry} releases its hold on the underlying \texttt{File} object. Similarly, if the reference count of the \texttt{File} object reaches zero, it implies that no active streams are reading from or writing to that specific file. The system then automatically reclaims the associated memory from the \texttt{FileTable}. This cascading deallocation mechanism ensures that kernel memory is only consumed for files that are actively in use by at least one process.

\section{Scheduling}

The scheduling module is the final core component of the operating system, relying heavily on and utilizing the previously described modules. Although the current implementation targets a single-core architecture for simplicity, the design is extensible to multi-core systems, as discussed later in this document.

\subsection{Process Structure}

The process structure is defined as follows:

\begin{lstlisting}[caption={Process Structure}, label={lst:processStruct}]
struct PACK Pid {
    u16 id;
    u64 count : 48;

    bool operator==(const Pid &other) const = default;
};

struct PACK ProcessFlags {
    bool KernelSpaceOnly : 1;
    bool PreserveFloats : 1;
};
static_assert(sizeof(ProcessFlags) == 1);

enum class ProcessState : u64 {
    kReady = 0,
    kWaitingForJoin,
    kTerminated,
    kLast,
};
static_assert(sizeof(ProcessState) == sizeof(u64));

struct Process : hal::Process {
    static constexpr size_t kMaxNameLength = vfs::kMaxComponentSize;

    /* Management */
    char name[kMaxNameLength];
    Pid pid;
    ProcessFlags flags;
    Thread *threads;
    u64 live_threads;
    u64 threads_to_clean;
    ProcessState state;
    WaitQueue<Thread, 3> *wait_queue;
    int status;

    /* Process resources */
    Mem::VPtr<Mem::AddressSpace> address_space;

    /* File descriptor table */
    Mem::VPtr<Fs::FdTable> fd_table;

    /* Standard I/O pipes (owned by process) */
    IO::Pipe<Fs::kStdioBufferSize> stdin_pipe;
    IO::Pipe<Fs::kStdioBufferSize> stdout_pipe;
    IO::Pipe<Fs::kStdioBufferSize> stderr_pipe;
};
\end{lstlisting}

\subsubsection{PID}
\label{subsubsec:pid}

As mentioned in the limitations section \ref{subsec:limitations}, the system imposes a hard limit on the maximum number of existing processes. This constraint allows for optimized process lookup via direct indexing. The \textbf{PID} (Process Identifier) is composed of a reservable identifier and an atomic counter, ensuring that each PID remains unique throughout the entire lifetime of the system.

\subsubsection{Process Flags}

\textbf{KernelSpaceOnly} -- enables the creation of kernel-only threads and processes. These processes do not require a separate user-space address space or user-space stack. This minimization of resource consumption improves performance; additionally, context switching between kernel-only threads does not necessitate switching the address space.

\textbf{PreserveFloats} -- indicates that all threads within the process save and restore floating-point registers during context switches by default. Disabling this flag allows the system to omit these potentially expensive operations for threads that do not utilize floating-point arithmetic.

\subsubsection{Process State}

The process state has less significance than the Thread State (described later) and is primarily utilized for resource management when waiting for a process to terminate. It ensures that process resources are not deallocated more than once.

\subsubsection{Address Space}

The address space is a fundamental component of the process structure. It defines all information regarding virtual memory, specifically establishing the translation between the virtual and physical layers. Each process operates within its own virtual address space, facilitating simpler memory management regarding fragmentation, performance, and security. Refer to Memory Management \ref{sec:memory} for further details.

\subsubsection{File Descriptor Table}

This table contains all file descriptors opened by threads executing within the process. Refer to the File System section \ref{section:fs} for more information.

\subsubsection{Pipes}

Pipes handle the buffered I/O of the process, primarily used for standard input and output communication with the user.

\subsubsection{Name}

The name of the process typically corresponds to the name of the executable. Unlike the unique PID, the process name is not required to be unique.

\subsubsection{Threads}

This field points to the first element of a doubly linked list of threads running in this environment. It is used primarily for operations such as termination (kill/exit).

\subsubsection{Live Threads and Threads to Clean}

These counters are used for synchronization with the \textbf{ProcessRipper}. Refer to Kernel Workers \ref{subsubsec:kworkers} for more details.
 
\subsubsection{Status}

The exit status of the process, which is passed to any thread waiting for the process to complete.

\subsubsection{Wait Queue}

The \textbf{Wait Queue} stores threads that are waiting for this process to finish. All threads in this queue are woken up upon process termination.

\subsection{Kernel Address Space}

The entire operating system shares a single Kernel Address Space. Every kernel process operates within this space. Additionally, the kernel address space is mapped into every user-space process's address space. This design choice enhances performance and simplifies architecture, as user threads frequently invoke system calls. Switching address spaces on every system call would incur significant performance overhead and complicate the logic required for handling syscalls.

\subsection{Thread Structure}

The thread structure is defined as follows:

\begin{lstlisting}[caption={Thread Structure}, label={lst:threadStruct}]
enum class UserPriority : u8 { kLow = 0, kMediumLow, kMedium, kMediumHigh, kHigh, kLast };

struct PACK Tid {
    u16 id;
    u64 count : 48;

    bool operator==(const Tid &other) const = default;
};

struct PACK ThreadFlags {
    SchedulingPolicy policy : 8;
    u8 priority : 8;
    UserPriority user_priority : 3;
    bool preserve_floats : 1;
    bool detached : 1;
    u64 padding : 43;
};
static_assert(sizeof(ThreadFlags) == 8);

enum class ThreadState : u64 {
    kReady = 0,
    kRunning,
    kSleeping,
    kBlockedOnWaitQueue,
    kWaitingForJoin,
    kTerminated,
    kLast,
};
static_assert(sizeof(ThreadState) == sizeof(u64));

static constexpr int kSchedulingIntrusiveLevel  = 0;
static constexpr int kSleepingIntrusiveLevel    = 1;
static constexpr int kProcessListIntrusiveLevel = 2;
static constexpr int kWaitQueueIntrusiveLevel   = 3;

struct Thread : data_structures::IntrusiveRbNode<Thread, u64, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveRbNode<Thread, u64, kSleepingIntrusiveLevel>,
                data_structures::IntrusiveListNode<Thread, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveListNode<Thread, kSleepingIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kSchedulingIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kWaitQueueIntrusiveLevel>,
                data_structures::IntrusiveDoubleListNode<Thread, kProcessListIntrusiveLevel> {
    /* Management */
    Tid tid;
    Pid owner;
    ThreadFlags flags;
    ThreadState state;
    void *retval;
    WaitQueue<Thread, kWaitQueueIntrusiveLevel> *wait_queue;

    /* Thread resources */
    void *kernel_stack;
    void *kernel_stack_bottom;
    void *user_stack;
    void *user_stack_bottom;

    /* Statistics */
    u64 kernel_time_ns;
    u64 user_time_ns;
    u64 timestamp;
    u64 timestamp_execution_start_ns;
    u64 num_interrupts;
    u64 num_syscalls;
    u64 num_context_switches;

    /* Arch */
    hal::Thread arch_data;

    NODISCARD u64 CalculateCpuTime();
};
\end{lstlisting}

\subsubsection{TID}

Similar to the PID \ref{subsubsec:pid}, the system limits the number of available threads. This property is utilized to create unique \textbf{TIDs} (Thread Identifiers), allowing for rapid lookups.

\subsubsection{Owner's PID}

The PID of the process that owns the thread.

\subsubsection{Thread Flags}

A collection of bitfields that determine how the scheduler manages the thread:

\textbf{SchedulingPolicy policy} -- defines the scheduling policy under which the thread currently operates. Refer to Policies \ref{subsec:policies} for more details.

\textbf{u8 priority} -- defines the internal priority of the thread within its assigned policy. This value is managed by the kernel.

\textbf{UserPriority user\_priority} -- may be specified by User Space to inform the scheduler of the relative importance of threads.

\textbf{bool preserve\_floats} -- specifies whether the floating-point state is preserved during context switches.

\textbf{bool detached} -- specifies whether the scheduler should automatically clean up the thread upon termination, without waiting for a join operation.

\subsubsection{Thread State}

Describes the current status of the thread. This state tracking allows the kernel to monitor each thread individually and verify system integrity, as only specific state transitions are permitted. The thread state transitions graph is defined as follows:

\newgeometry{top=1cm, bottom=1.5cm, left=1cm, right=1cm} 

\begin{figure}[p]
    \centering
    \includesvg[height=0.95\textheight, width=\textwidth, keepaspectratio]{res/thread-states.svg}
    \caption{Thread State Transitions Graph}
    \label{fig:thread-states}
\end{figure}

\restoregeometry

\subsubsection{Stacks}

Two distinct stacks are required for each thread: one for user space and one for kernel space. A separate kernel stack is essential for security and stability. Furthermore, sharing a single kernel stack is inefficient because context switches may occur within kernel code, such as during thread joining or other synchronization events.

\subsubsection{Statistics}

Statistics tracking allows for the analysis of execution specifics for each thread. This data is used to dynamically adjust the scheduling module and to debug or test the scheduler.

\subsubsection{Wait Queue}

Allows other threads to block and wait for a specific thread to finish execution (Thread joining).

\subsection{Kernel Workers}
\label{subsubsec:kworkers}

Before initializing the scheduling module, the operating system creates three essential kernel workers:

\textbf{Trace Dumper} -- responsible for dumping kernel traces to the terminal or storage files, ensuring debuggability and stability. This task cannot be performed directly by the kernel code during critical execution paths, as writing to devices or files is too slow for system calls or interrupt handlers. The buffers are dumped periodically every 20ms.

\textbf{Thread Ripper} -- responsible for the deallocation of thread resources, including descriptors and stacks.

\textbf{Process Ripper} -- responsible for the deallocation of process resources, including the address space.

\subsection{Intrusive Data Structures}

Since threads frequently migrate between different linked lists and data structures, allocating and freeing list nodes for every operation introduces significant overhead. To address this, the system employs intrusive data structures. Instead of allocating a node that contains a pointer to the object, the node structure is embedded directly within the object itself. Operations on the lists are performed by simply modifying the fields of the object.

\begin{lstlisting}[caption={Intrusive Nodes}, label={lst:intrusiveNodes}]
template <class T, int kIntrusiveLevel>
struct IntrusiveListNode {
    T *next;
};

template <class T, int kIntrusiveLevel>
struct IntrusiveDoubleListNode {
    T *next;
    T *prev;
};

template <class T, class KeyT, int kIntrusiveLevel>
struct IntrusiveRbNode {
    enum class Color : u8 {
        kBlack = 0,
        kRed   = 1,
    };

    T *parent;
    union {
        struct {
            T *left;
            T *right;
        };
        T *child[2];
    };

    Color color;
    KeyT key;
};
\end{lstlisting}

The Thread data structure \ref{lst:threadStruct} demonstrates the usage of such nodes. This approach offers multiple benefits, including:
\begin{itemize}
    \item No dynamic allocation or deallocation during insert or pop operations.
    \item Improved memory access patterns, the object is accessed directly rather than via a pointer from a separate node.
    \item Verification of whether an object belongs to a list in \textbf{O(1)} time, given only the object itself.
    \item Removal of the object from the list in \textbf{O(1)} time.
    \item Removal of the object from the list without requiring access to the list head structure.
\end{itemize}

\subsection{Meta-Scheduler}

The scheduler architecture follows the \textbf{meta-scheduler} design pattern, which is common in modern operating systems. This approach relies on abstracting scheduling logic into Policies. Policies are primarily responsible for selecting the next task from the set of tasks they manage and reacting to thread behavior (e.g., punishing or rewarding threads with CPU time based on workload characteristics). The scheduler itself is responsible for all other operations, including:
\begin{itemize}
    \item Managing thread states.
    \item Transitioning threads to sleep.
    \item Waking up threads.
    \item Managing idle time.
    \item Configuring next timing events via the timing infrastructure.
    \item Blocking threads on wait queues.
    \item Releasing threads from wait queues.
\end{itemize}
A hierarchy exists between policies, ensuring that threads from higher-priority policies are selected before threads from lower-priority ones.

\subsection{Timing Model}

The system implements a tickless kernel architecture. Instead of relying on a periodic interrupt (the \textbf{Kernel Tick}) at a fixed frequency, the scheduler calculates precisely when the next timing event must occur. This approach improves efficiency and precision.

\subsection{Policies}

The following scheduling policies have been implemented:

\textbf{Round Robin Scheduling Policy} -- simple policy that iterates through a linked list from front to back. Threads are ordered based on their arrival time in the policy.

\textbf{Priority Queue Scheduling Policy} -- policy that orders threads based on kernel-assigned priorities. Priorities are capped at a range of 0-64 to enable the use of an \textbf{O(1)} priority queue, known as a \textbf{Bitmap Priority Queue}. This structure maintains an array of linked lists representing individual priorities. Additionally, a bitmask indicates the presence of tasks at specific priority levels. Lookup operations utilize bitwise instructions (counting leading/trailing zeros) to efficiently find the minimum or maximum priority value (\textbf{O(1)}).

\textbf{Multi-Level Feedback Queue (MLFQ) Scheduling Policy} -- policy that segregates threads into distinct queues according to their workload characteristics. The policy penalizes CPU-bound threads by demoting them to lower priorities, whereas I/O-bound threads are elevated to higher priorities, thereby favoring interactive performance. To prevent the starvation of low-priority tasks, a periodic reset mechanism promotes all threads to the highest priority queue every 100ms. Within each queue, threads are managed using \textbf{Red-Black trees}, sorted by a key that combines user-defined priority and aggregated CPU burst times (the amount of time a thread spends executing on the CPU before it either completes, requires I/O operations, or is interrupted by the operating system). This ensures that each thread in the given queue receives a fair share of CPU time.

\noindent These policies establish a hierarchy defined by the following enumeration:

\begin{lstlisting}[caption={Scheduling Policy Hierarchy}, label={lst:policyHierarchy}]
// PO > P1 > .. > P4
enum SchedulingPolicy {
    kUberTask_PQ_P0 = 0,
    kDrivers_PQ_P1,
    kUrgentTasks_PQ_P2,
    kNormalTasks_MLFQ_P3,
    kBackgroundTasks_RR_P4,
    kLast,
};
\end{lstlisting}

Each policy is designated for a specific group of tasks:

\textbf{Uber Tasks} -- Tasks that must be executed as quickly as possible, such as emergency recovery actions or critical kernel state preservation (e.g., saving state before shutdown or terminating processes during out-of-memory conditions).

\textbf{Drivers} -- Driver tasks that require immediate execution to prevent device blocking and ensure smooth system operation (e.g., audio drivers or network interfaces).

\textbf{Urgent Tasks} -- Tasks that are less critical than drivers but more important than standard user tasks. These may include privileged user-space tasks that consume data from drivers.

\textbf{Normal Tasks} -- The most common group of tasks, comprising standard user-space programs such as graphical interfaces and data processing applications. Basic kernel workers, including those listed in the kernel workers section \ref{subsubsec:kworkers}, also belong to this category.

\textbf{Background Tasks} -- Tasks that are not time-critical and should only be executed when the system is otherwise idle. Examples include update checks or non-urgent cleanup operations.

\subsubsection{Policies Abstraction}
\label{subsec:policies}

The policy abstraction is defined as follows:

\begin{lstlisting}[caption={Policy Abstraction}, label={lst:policyStruct}]
struct Policy {
    struct {
        Thread *(*pick_next_task)(void *);

        void (*add_task)(void *, Thread *);
        void (*remove_task)(void *, Thread *);

        u64 (*get_preempt_time)(void *, Thread *);
        bool (*is_first_higher_priority)(void *, Thread *, Thread *);
        bool (*validate_flags)(void *, const ThreadFlags *);

        void (*on_thread_yield)(void *, Thread *);
        void (*on_periodic_update)(void *, u64 current_time_ns);
    } cbs;
    void *self;
};
\end{lstlisting}

As shown, the primary operation is \textbf{pick\_next\_task}, alongside standard \textbf{add\_task} and \textbf{remove\_task} operations. Additionally, helper functions are provided for the scheduler and for statistics gathering.

\subsection{Sleeping}

To provide high-precision sleeping, the system relies on one-shot, precise timing events. Upon receiving a timer interrupt, the scheduler determines the timestamp for the next event based on the system state, which includes the current thread preemption time and the wake-up times of sleeping threads. Consequently, the scheduler must inspect the sleeping queue to process expired events. To ensure high performance and stable time complexity, an Intrusive Red-Black Tree was selected as the underlying data structure for the \textbf{Sleeping Queue}. The critical operations are \textbf{Insert}, \textbf{ExtractMin}, and \textbf{Remove}, all of which must have predictable execution times. The requirement for efficient arbitrary removal excluded binary heaps, while the need for stability excluded heaps with amortized complexity bounds. The Red-Black Tree satisfied all requirements and was already implemented for other system components, making it the optimal choice.

% ==================================================================

\chapter{Results}
TODO: JAKUB

\section{User's Manual}

This section describes the usage of the AlkOS build environment and documents the public interfaces available to userspace applications. It outlines the build and execution workflow, the integration of user programs, and the system call interface provided by the operating system.

\subsection{Building and Running the OS}

AlkOS provides a command-line utility script, \texttt{alkos\_cli.bash}, located in the \texttt{scripts/} directory. This script automates the setup of the development environment, the compilation of the kernel, and the execution of the operating system within an emulator. The build infrastructure is based on CMake and requires a Linux host system.

Automated dependency installation is officially supported for Arch Linux and Ubuntu distributions. Nevertheless, the build process is expected to function on other Linux distributions, provided that the required dependencies are installed manually. Reference installation dependencies for supported environments can be found in the \texttt{scripts/env/} directory.

The build and execution workflow consists of three sequential stages:

\begin{enumerate}
    \item \textbf{Environment Initialization:}
    Prior to compilation, a custom cross-compilation toolchain and all required system dependencies must be installed. This can be achieved by executing:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --install all
    \end{verbatim}
    Alternatively, to install only the cross-compiler toolchain, the following command may be used:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --install toolchain
    \end{verbatim}

    \item \textbf{Configuration:}
    During the configuration phase, build configuration and kernel feature flags are generated and stored in the \texttt{config/} directory. The default configuration can be created using:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --configure
    \end{verbatim}
    For customized configurations, the following script is provided:
    \begin{verbatim}
    ./scripts/config/configure.bash <platform> <build_type> [options...]
    \end{verbatim}
    Here, \texttt{<platform>} denotes the target architecture (e.g., \texttt{x86\_64}), \texttt{<build\_type>} specifies either \texttt{Debug} or \texttt{Release}. A complete list of options can be obtained using the \texttt{\text{-}\text{-}help} flag.

    \item \textbf{Compilation and Execution:}
    In the final stage, the kernel and all registered userspace applications are compiled, the root filesystem image and bootable ISO are generated, and the operating system is launched using the QEMU emulator. This process can be initiated with the command:
    \begin{verbatim}
    ./scripts/alkos_cli.bash --run
    \end{verbatim}
\end{enumerate}

In the event of errors occurring at any stage of the process, the scripts may be executed with the \texttt{\text{-}\text{-}verbose} flag, which enables detailed logging output to facilitate debugging.

\subsection{Developing Userspace Applications}

AlkOS supports userspace applications written in C and C++. These applications are integrated into the operating system at build time and included directly in the generated root filesystem.

\subsubsection{Project Structure}

Userspace programs reside in the \texttt{userspace/programs/} directory. Each application is placed in its own subdirectory and must include all relevant source files along with a \texttt{CMakeLists.txt} configuration file. A minimal example of a user application is shown in Listing \ref{lst:minimalUserAppStructure}.

\begin{lstlisting}[caption={Minimal User Application Structure}, label={lst:minimalUserAppStructure}]
// userspace/programs/my_app/main.cpp
#include <stdio.h>

extern "C" int main() {
    printf("Hello from AlkOS User Space!\n");
    return 0;
}
\end{lstlisting}

\subsubsection{Build System Integration}

To include a userspace application in the build process, it must be registered with the CMake-based build system. AlkOS provides the helper macro \texttt{alkos\_register\_userspace\_app}, which automatically configures compiler flags and links the application against the custom C and C++ standard libraries (\texttt{libc} and \texttt{libc++}). An example registration is shown in Listing \ref{lst:userAppCMakeRegistration}.

{
    \renewcommand{\lstlistingname}{CMake Snippet}
    \begin{lstlisting}[style=cmakestyle, caption={User Application Registration}, label={lst:userAppCMakeRegistration}]
    # userspace/programs/my_app/CMakeLists.txt
    alkos_find_sources(MY_APP_SOURCES)
    alkos_register_userspace_app(my_app "${MY_APP_SOURCES}")
    \end{lstlisting}
}

After registration, the application is automatically compiled, linked, and placed in the \texttt{/bin} directory of the root filesystem during the next build.

\subsection{C Standard Library}

Instead of relying on an external standard library implementation, AlkOS provides a custom C standard library tailored to its kernel interface.

\subsubsection{Standard C Support}

The library includes implementations of commonly used standard headers such as \texttt{<stdio.h>}, \texttt{<stdlib.h>}, and \texttt{<string.h>}. This enables the compilation of conventional C programs with minimal adaptation. Standard file operations, including \texttt{fopen}, \texttt{fread}, and \texttt{fwrite}, are fully supported and internally routed through the VFS.

\subsubsection{AlkOS-Specific Extensions}

Certain low-level system interactions are beyond the scope of the standard C library. To expose kernel-specific functionality, AlkOS provides additional interfaces via the \texttt{<alkos/calls.h>} header. These interfaces enable access to hardware abstraction, threading primitives, and system control mechanisms. The most important extensions include:

\begin{itemize}
    \item \textbf{Input and Output Subsystem (\texttt{alkos/sys/video.h}, \texttt{alkos/sys/input.h}):}
    The \texttt{GetVideoBufferInfo} function maps the framebuffer into the process address space, allowing direct graphical output. Keyboard input is accessed via \texttt{GetKeyState}, which enables polling of individual keys.

    \item \textbf{Process Management, Threading, and Timing (\texttt{alkos/sys/thread.h}, \texttt{alkos/sys/time.h}):}
    Thread lifecycle management is provided through \texttt{ThreadCreate}, \texttt{ThreadJoin}, and \texttt{ThreadDetach}. High-precision timing services are available via \texttt{NanoSleep} and \texttt{NanoSleepUntil}. Process-level control is facilitated by \texttt{Exec} and \texttt{ProcExit}.

    \item \textbf{Power Management (\texttt{alkos/sys/power.h}, \texttt{alkos/sys/proc.h}):}
    System power states can be controlled using the \texttt{Shutdown} and \texttt{Reboot} interfaces, which invoke the underlying ACPI mechanisms.

    \item \textbf{Extended Filesystem Operations (\texttt{alkos/sys/fs/fs.h}):}
    While standard I/O functions support file access, directory enumeration and metadata queries require OS-specific calls. The \texttt{ReadDirectory} function enables directory traversal, and \texttt{FileInfo} provides information about filesystem objects.
\end{itemize}

\subsection{Syscalls}

The system call interface represents the controlled boundary between userspace applications executing in Ring 3 and the kernel executing in Ring 0. On the \texttt{x86\_64} architecture, AlkOS utilizes the \texttt{int 0x80} software interrupt mechanism to invoke system calls.

\subsubsection{Calling Convention}

The calling convention is largely inspired by the System V AMD64 ABI, with modifications to accommodate system call semantics. The register usage is summarized in Table \ref{tab:syscall_abi}.

\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Register} & \textbf{Purpose} \\ \hline
\texttt{RAX} & System call number (input) / return value (output) \\ \hline
\texttt{RDI} & Argument 1 \\ \hline
\texttt{RSI} & Argument 2 \\ \hline
\texttt{RDX} & Argument 3 \\ \hline
\texttt{R10} & Argument 4 \\ \hline
\texttt{R8} & Argument 5 \\ \hline
\texttt{R9} & Argument 6 \\ \hline
\end{tabular}
\caption{System Call Register Mapping on x86\_64}
\label{tab:syscall_abi}
\end{table}

\subsubsection{Provided System Services}

The available system calls are grouped into logical categories:

\paragraph{Process and Thread Management}
\begin{itemize}
    \item \texttt{exec(path)}: Loads and executes a program.
    \item \texttt{proc\_exit(status)} and \texttt{proc\_abort()}: Terminates the calling process.
    \item \texttt{kill(pid)} and \texttt{wait(pid)}: Manage inter-process control and synchronization.
    \item \texttt{thread\_create}, \texttt{thread\_exit}, \texttt{thread\_join}, \texttt{thread\_detach}: Thread management primitives.
    \item \texttt{get\_heap\_start()}: Returns the base address of the process heap.
\end{itemize}

\paragraph{File I/O and Filesystem Management}
\begin{itemize}
    \item \texttt{open}, \texttt{close}, \texttt{read}, \texttt{write}, \texttt{seek}: File descriptor operations.
    \item \texttt{dup(fd)} and \texttt{dup\_to(fd, newfd)}: File descriptor duplication.
    \item \texttt{read\_directory} and \texttt{file\_info}: Filesystem inspection.
    \item \texttt{create\_directory}, \texttt{delete\_file}, \texttt{move\_file}: Filesystem modification operations.
\end{itemize}

\paragraph{Graphics and Input}
\begin{itemize}
    \item \texttt{create\_graphic\_session(info)}: Initializes a graphical context.
    \item \texttt{blit()}: Requests composition of the application backbuffer.
    \item \texttt{get\_key\_state(vk)}: Retrieves the state of a virtual key.
\end{itemize}

\paragraph{Time and Synchronization}
\begin{itemize}
    \item \texttt{nanosleep(ns)} and \texttt{nanosleep\_until(sys\_ns)}: Suspends execution for a defined duration.
    \item \texttt{get\_clock\_value}: Returns the current value of a specified clock.
    \item \texttt{get\_timezone}: Retrieves the system time zone configuration.
\end{itemize}

\paragraph{System Control and Debugging}
\begin{itemize}
    \item \texttt{power(action)}: Controls system power states.
    \item \texttt{panic(msg)}: Terminates execution and records a diagnostic message.
\end{itemize}


\section{Example Programs}
TODO ADAM

% TODO: extension
% \section{Performance Analysis}
% \subsection{KMalloc Performance}
% \subsection{KFree Performance}
% \subsection{Context Switch Performance}
% \subsection{Syscall Performance}
% \subsection{Scheduler Tests}


\chapter{Future Work}
TODO: KRYCZKA, ADAM, JAKUB

\chapter{Conclusion}
TODO: JAKUB

% ----------- BIBLIOGRAPHY ---------

\printbibliography[heading=bibintoc]

\pagenumbering{gobble}
\thispagestyle{empty}



% --------- LIST OF SYMBOLS AND ABBREVIATIONS ------
\chapter*{List of symbols and abbreviations}

\begin{tabular}{cl}
ABI & Application Binary Interface \\
API & Application Programming Interface \\
APIC & Advanced Programmable Interrupt Controller \\
CRTP & Curiously Recurring Template Pattern \\
IDT & Interrupt Descriptor Table \\
IPC & Inter-Process Communication \\
ISR & Interrupt Service Routine \\
LAPIC & Local Advanced Programmable Interrupt Controller \\
LIT & Logical Interrupt Table \\
MLFQ & Multi-Level Feedback Queue \\
MMIO & Memory-Mapped I/O \\
MMU & Memory Management Unit \\
OS & Operating System \\
PIC & Programmable Interrupt Controller \\
PID & Process IDentifier \\
PIT & Programmable Interval Timer \\
RTC & Real-Time Clock \\
TID & Thread IDentifier \\
TSC & TimeStamp Counter \\
TSS & Task State Segment \\
VFS & Virtual File System \\
CFS & Completly Fair Scheduler \\
EEVDF & Earliest Eligible Virtual Deadline First \\
DMA & Direct Memory Access \\
PCP & Per-CPU Pagessets \\
COW & Copy On Write \\
SMP & Symmetric MultiProcessing \\
ACPI & Advanced Configuration and Power Interface \\
PCI & Peripheral Component Interconnect \\
USB & Universal Serial Bus \\
IPC & Inter Process Communication \\
FAT & File Allocation Table \\
FHS & Filesystem Hierarchy Standard \\
ELF & Executable and Linkable Format \\
ASLR & Address Space Layout Randomization \\
SIMD & Single Instruction, Multiple Data \\
\end{tabular}
\\
\thispagestyle{empty}


% ----------  LIST OF FIGURES ------------
\listoffigures
\thispagestyle{empty}

% -----------  LIST OF TABLES ------------
\renewcommand{\listtablename}{List of Tables}
\listoftables
\thispagestyle{empty}

\end{document}

% ==================================================================
% TODOS:
% - wytlumaczyc na starcie co zakladamy etc i dlaczego np segmenty
